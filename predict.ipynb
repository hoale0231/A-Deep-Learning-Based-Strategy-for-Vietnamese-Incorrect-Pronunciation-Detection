{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: latest is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConformerRNNModel(\n",
       "  (encoder): ConformerEncoder(\n",
       "    (conv_subsample): Conv2dSubampling(\n",
       "      (sequential): Sequential(\n",
       "        (0): Conv2d(1, 144, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (input_projection): Sequential(\n",
       "      (0): Linear(\n",
       "        (linear): Linear(in_features=2736, out_features=144, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ConformerBlock(\n",
       "        (sequential): Sequential(\n",
       "          (0): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualConnectionModule(\n",
       "            (module): MultiHeadedSelfAttentionModule(\n",
       "              (positional_encoding): PositionalEncoding()\n",
       "              (layer_norm): LayerNorm()\n",
       "              (attention): RelativeMultiHeadAttention(\n",
       "                (query_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (key_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (value_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (pos_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (out_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualConnectionModule(\n",
       "            (module): ConformerConvModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Transpose()\n",
       "                (2): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (3): GLU()\n",
       "                (4): DepthwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144, bias=False)\n",
       "                )\n",
       "                (5): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (6): Swish()\n",
       "                (7): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (8): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): ConformerBlock(\n",
       "        (sequential): Sequential(\n",
       "          (0): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualConnectionModule(\n",
       "            (module): MultiHeadedSelfAttentionModule(\n",
       "              (positional_encoding): PositionalEncoding()\n",
       "              (layer_norm): LayerNorm()\n",
       "              (attention): RelativeMultiHeadAttention(\n",
       "                (query_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (key_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (value_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (pos_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (out_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualConnectionModule(\n",
       "            (module): ConformerConvModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Transpose()\n",
       "                (2): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (3): GLU()\n",
       "                (4): DepthwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144, bias=False)\n",
       "                )\n",
       "                (5): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (6): Swish()\n",
       "                (7): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (8): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): ConformerBlock(\n",
       "        (sequential): Sequential(\n",
       "          (0): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualConnectionModule(\n",
       "            (module): MultiHeadedSelfAttentionModule(\n",
       "              (positional_encoding): PositionalEncoding()\n",
       "              (layer_norm): LayerNorm()\n",
       "              (attention): RelativeMultiHeadAttention(\n",
       "                (query_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (key_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (value_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (pos_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (out_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualConnectionModule(\n",
       "            (module): ConformerConvModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Transpose()\n",
       "                (2): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (3): GLU()\n",
       "                (4): DepthwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144, bias=False)\n",
       "                )\n",
       "                (5): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (6): Swish()\n",
       "                (7): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (8): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): ConformerBlock(\n",
       "        (sequential): Sequential(\n",
       "          (0): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualConnectionModule(\n",
       "            (module): MultiHeadedSelfAttentionModule(\n",
       "              (positional_encoding): PositionalEncoding()\n",
       "              (layer_norm): LayerNorm()\n",
       "              (attention): RelativeMultiHeadAttention(\n",
       "                (query_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (key_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (value_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "                (pos_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (out_proj): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualConnectionModule(\n",
       "            (module): ConformerConvModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Transpose()\n",
       "                (2): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (3): GLU()\n",
       "                (4): DepthwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144, bias=False)\n",
       "                )\n",
       "                (5): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (6): Swish()\n",
       "                (7): PointwiseConv1d(\n",
       "                  (conv): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "                (8): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualConnectionModule(\n",
       "            (module): FeedForwardModule(\n",
       "              (sequential): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(\n",
       "                  (linear): Linear(in_features=144, out_features=576, bias=True)\n",
       "                )\n",
       "                (2): Swish()\n",
       "                (3): Dropout(p=0.1, inplace=False)\n",
       "                (4): Linear(\n",
       "                  (linear): Linear(in_features=576, out_features=144, bias=True)\n",
       "                )\n",
       "                (5): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Transpose()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(\n",
       "        (linear): Linear(in_features=144, out_features=131, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RNNDecoder(\n",
       "    (embedding): Embedding(131, 144)\n",
       "    (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (rnn): GRU(144, 144, batch_first=True, dropout=0.1)\n",
       "    (attention): MultiHeadAttention(\n",
       "      (query_proj): Linear(\n",
       "        (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "      )\n",
       "      (key_proj): Linear(\n",
       "        (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "      )\n",
       "      (value_proj): Linear(\n",
       "        (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "      )\n",
       "      (scaled_dot_attn): ScaledDotProductAttention()\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=288, out_features=144, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): View()\n",
       "      (3): Linear(in_features=144, out_features=131, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (word_decoder): WordDecoder(\n",
       "    (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (self_attention): MultiHeadedSelfAttentionModule(\n",
       "      (positional_encoding): PositionalEncoding()\n",
       "      (layer_norm): LayerNorm()\n",
       "      (attention): RelativeMultiHeadAttention(\n",
       "        (query_proj): Linear(\n",
       "          (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (key_proj): Linear(\n",
       "          (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (value_proj): Linear(\n",
       "          (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (pos_proj): Linear(\n",
       "          (linear): Linear(in_features=144, out_features=144, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(\n",
       "          (linear): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (rnn): GRU(144, 144, batch_first=True, dropout=0.1)\n",
       "    (ff): Linear(in_features=288, out_features=144, bias=True)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=144, out_features=72, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=72, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deploy.model import ConformerRNNModel\n",
    "from mpvn.configs import DictConfig\n",
    "from mpvn.vocabs.grad import GradVocabulary\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "\n",
    "configs = DictConfig()\n",
    "vocab = GradVocabulary(f\"Data/token.txt\")\n",
    "phone_map = json.load(open(\"Data/phone_map.json\"))\n",
    "\n",
    "model = ConformerRNNModel.load_from_checkpoint(\n",
    "    'Checkpoint/finetuned-epoch=24-valid_loss=0.14-valid_per=0.47-valid_acc=0.93-valid_f1=0.68.ckpt',\n",
    "    configs=configs,\n",
    "    num_classes=len(vocab),\n",
    "    vocab=vocab,\n",
    "    phone_map=phone_map\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = 'Data/label/Audio/2022-12-12-AFRO-trym/2022-12-12-AFRO-trym_42.wav'\n",
    "transcript = 'quy nhơn'\n",
    "\n",
    "model.predict(audio_path, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b1a50cc58c14d82ff378b4491fa094aec266868d23c7ca7976fe8f9a4aaf20a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

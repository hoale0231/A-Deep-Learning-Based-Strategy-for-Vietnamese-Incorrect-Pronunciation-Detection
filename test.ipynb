{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: latest is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mpvn.model import *\n",
    "from mpvn.metric import *\n",
    "import pytorch_lightning as pl\n",
    "from mpvn.configs import DictConfig\n",
    "from mpvn.data.grad.lit_data_module import LightningGradDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "configs = DictConfig()\n",
    "\n",
    "data_module = LightningGradDataModule(configs)\n",
    "vocab, word_vocab = data_module.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"checkpoint\",\n",
    "    filename=\"mpvn-{epoch:02d}-{valid_loss:.2f}-{valid_per:.2f}\",\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"valid_loss\", \n",
    "    min_delta=0.00, \n",
    "    patience=5, \n",
    "    verbose=False, \n",
    "    mode=\"min\"\n",
    ")\n",
    "logger = TensorBoardLogger(\"tensorboard\", name=\"Pronunciation for Vietnamese\")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='cpu',\n",
    "                      logger=logger,\n",
    "                      max_epochs=configs.max_epochs,\n",
    "                      callbacks=[checkpoint_callback, early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = ConformerRNNModel(\n",
    "    configs=configs,\n",
    "    num_classes=len(vocab),\n",
    "    num_words=len(word_vocab),\n",
    "    vocab=vocab,\n",
    "    per_metric=WordErrorRate(vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 0/760 [00:00<?, ?it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([90, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "\n",
      "1 sample result\n",
      "EP: torch.Size([90]) a5a5a5a5a5a5a5a5a5a5a5əː6əː6a5a5a5a5əː6a5a5a5a5a5a5a5əː6əː6a5a5a5əː6əː6əː6a5a5əː6a5a5a5a5a5a5a5a5a5a5əː6əː6əː6əː6a5a5a5əː6əː6a5a5a5a5əː6əː6əː6a5əː6əː6əː6əː6əː6a5əː6əː6əː6əː6əː6əː6əː6a5əː6a5a5a5a5a5a5a5a5a5a5a5a5\n",
      "DP    : torch.Size([46]) yɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜiɛ6yɜyɜyɜyɜyɜyɜiɛ6yɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜyɜiɛ6yɜyɜyɜ\n",
      "Target: torch.Size([46]) zi6c-vu6-kiɛ4m-tʃaː-syɜc-xwɛ4-ɗi6ɲ-ki2-laː2-mo6t̪-iɜ-kiɛɜn-haj<e>\n",
      "Per: 1.3235294117647058\n",
      "Attention: torch.Size([4, 13, 46])\n",
      "torch.Size([13, 46])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 1/760 [00:00<06:12,  2.04it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 32, 288])\n",
      "torch.Size([1, 32, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([77, 1, 155])\n",
      "torch.Size([32, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:   0%|          | 2/760 [00:00<03:23,  3.73it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 22, 288])\n",
      "torch.Size([1, 22, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([55, 1, 155])\n",
      "torch.Size([22, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:   0%|          | 3/760 [00:00<02:54,  4.34it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 54, 288])\n",
      "torch.Size([1, 54, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([54, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   1%|          | 4/760 [00:00<02:17,  5.49it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 51, 288])\n",
      "torch.Size([1, 51, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([88, 1, 155])\n",
      "torch.Size([51, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   1%|          | 5/760 [00:00<01:54,  6.57it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 37, 288])\n",
      "torch.Size([1, 37, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([37, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:   1%|          | 6/760 [00:00<01:56,  6.49it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([76, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   1%|          | 7/760 [00:00<01:44,  7.20it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 28, 288])\n",
      "torch.Size([1, 28, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([62, 1, 155])\n",
      "torch.Size([28, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:   1%|          | 8/760 [00:01<01:43,  7.26it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 47, 288])\n",
      "torch.Size([1, 47, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([101, 1, 155])\n",
      "torch.Size([47, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   1%|          | 9/760 [00:01<01:35,  7.86it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([83, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   1%|▏         | 10/760 [00:01<01:37,  7.71it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 27, 288])\n",
      "torch.Size([1, 27, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([85, 1, 155])\n",
      "torch.Size([27, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:   1%|▏         | 11/760 [00:01<01:30,  8.28it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([119, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   2%|▏         | 12/760 [00:01<01:33,  7.96it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 45, 288])\n",
      "torch.Size([1, 45, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([45, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   2%|▏         | 13/760 [00:01<01:28,  8.46it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 57, 288])\n",
      "torch.Size([1, 57, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([113, 1, 155])\n",
      "torch.Size([57, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:   2%|▏         | 14/760 [00:01<01:30,  8.25it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 33, 288])\n",
      "torch.Size([1, 33, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([33, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:   2%|▏         | 15/760 [00:01<01:25,  8.70it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 20, 288])\n",
      "torch.Size([1, 20, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([60, 1, 155])\n",
      "torch.Size([20, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:   2%|▏         | 16/760 [00:01<01:27,  8.51it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 45, 288])\n",
      "torch.Size([1, 45, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([45, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   2%|▏         | 17/760 [00:01<01:23,  8.88it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([61, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   2%|▏         | 18/760 [00:01<01:19,  9.29it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 54, 288])\n",
      "torch.Size([1, 54, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([107, 1, 155])\n",
      "torch.Size([54, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:   2%|▎         | 19/760 [00:02<01:22,  8.94it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([54, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:   3%|▎         | 20/760 [00:02<01:20,  9.21it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([89, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   3%|▎         | 21/760 [00:02<01:22,  8.93it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([122, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   3%|▎         | 22/760 [00:02<01:23,  8.79it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 31, 288])\n",
      "torch.Size([1, 31, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([83, 1, 155])\n",
      "torch.Size([31, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:   3%|▎         | 23/760 [00:02<01:22,  8.92it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 38, 288])\n",
      "torch.Size([1, 38, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([109, 1, 155])\n",
      "torch.Size([38, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   3%|▎         | 24/760 [00:02<01:23,  8.79it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 39, 288])\n",
      "torch.Size([1, 39, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([91, 1, 155])\n",
      "torch.Size([39, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   3%|▎         | 25/760 [00:02<01:21,  9.02it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 55, 288])\n",
      "torch.Size([1, 55, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([100, 1, 155])\n",
      "torch.Size([55, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   3%|▎         | 26/760 [00:02<01:22,  8.91it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 39, 288])\n",
      "torch.Size([1, 39, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([83, 1, 155])\n",
      "torch.Size([39, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   4%|▎         | 27/760 [00:02<01:20,  9.06it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([63, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:   4%|▎         | 28/760 [00:03<01:21,  8.94it/s]torch.Size([1, 3])\n",
      "d1\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 155])\n",
      "d2\n",
      "torch.Size([1, 3, 288])\n",
      "torch.Size([1, 3, 2])\n",
      "\n",
      "\n",
      "torch.Size([33, 1, 155])\n",
      "torch.Size([10, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([3, 2]) \n",
      " torch.Size([1, 3])\n",
      "Validation DataLoader 0:   4%|▍         | 29/760 [00:03<01:19,  9.21it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 57, 288])\n",
      "torch.Size([1, 57, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([125, 1, 155])\n",
      "torch.Size([57, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:   4%|▍         | 30/760 [00:03<01:20,  9.02it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([104, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   4%|▍         | 31/760 [00:03<01:21,  8.94it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([104, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   4%|▍         | 32/760 [00:03<01:21,  8.97it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 27, 288])\n",
      "torch.Size([1, 27, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([65, 1, 155])\n",
      "torch.Size([27, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   4%|▍         | 33/760 [00:03<01:20,  9.05it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 39, 288])\n",
      "torch.Size([1, 39, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([77, 1, 155])\n",
      "torch.Size([39, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:   4%|▍         | 34/760 [00:03<01:21,  8.89it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([62, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   5%|▍         | 35/760 [00:03<01:19,  9.09it/s]torch.Size([1, 3])\n",
      "d1\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 155])\n",
      "d2\n",
      "torch.Size([1, 3, 288])\n",
      "torch.Size([1, 3, 2])\n",
      "\n",
      "\n",
      "torch.Size([51, 1, 155])\n",
      "torch.Size([10, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([3, 2]) \n",
      " torch.Size([1, 3])\n",
      "Validation DataLoader 0:   5%|▍         | 36/760 [00:03<01:18,  9.28it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 26, 288])\n",
      "torch.Size([1, 26, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([59, 1, 155])\n",
      "torch.Size([26, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   5%|▍         | 37/760 [00:04<01:19,  9.11it/s]torch.Size([1, 4])\n",
      "d1\n",
      "torch.Size([1, 16, 288])\n",
      "torch.Size([1, 16, 155])\n",
      "d2\n",
      "torch.Size([1, 4, 288])\n",
      "torch.Size([1, 4, 2])\n",
      "\n",
      "\n",
      "torch.Size([43, 1, 155])\n",
      "torch.Size([16, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([4, 2]) \n",
      " torch.Size([1, 4])\n",
      "Validation DataLoader 0:   5%|▌         | 38/760 [00:04<01:17,  9.31it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   5%|▌         | 39/760 [00:04<01:16,  9.46it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 56, 288])\n",
      "torch.Size([1, 56, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([114, 1, 155])\n",
      "torch.Size([56, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:   5%|▌         | 40/760 [00:04<01:17,  9.28it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 23, 288])\n",
      "torch.Size([1, 23, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([70, 1, 155])\n",
      "torch.Size([23, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   5%|▌         | 41/760 [00:04<01:16,  9.45it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   6%|▌         | 42/760 [00:04<01:14,  9.59it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 27, 288])\n",
      "torch.Size([1, 27, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([66, 1, 155])\n",
      "torch.Size([27, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   6%|▌         | 43/760 [00:04<01:16,  9.41it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 54, 288])\n",
      "torch.Size([1, 54, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([119, 1, 155])\n",
      "torch.Size([54, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   6%|▌         | 44/760 [00:04<01:15,  9.54it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([50, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:   6%|▌         | 45/760 [00:04<01:15,  9.44it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([121, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   6%|▌         | 46/760 [00:04<01:16,  9.33it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([96, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   6%|▌         | 47/760 [00:05<01:16,  9.31it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 42, 288])\n",
      "torch.Size([1, 42, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([82, 1, 155])\n",
      "torch.Size([42, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   6%|▋         | 48/760 [00:05<01:15,  9.44it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([110, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   6%|▋         | 49/760 [00:05<01:14,  9.58it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 42, 288])\n",
      "torch.Size([1, 42, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([42, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   7%|▋         | 50/760 [00:05<01:15,  9.43it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([126, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   7%|▋         | 51/760 [00:05<01:14,  9.49it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 22, 288])\n",
      "torch.Size([1, 22, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([66, 1, 155])\n",
      "torch.Size([22, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:   7%|▋         | 52/760 [00:05<01:15,  9.43it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 28, 288])\n",
      "torch.Size([1, 28, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([28, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:   7%|▋         | 53/760 [00:05<01:14,  9.53it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 18, 288])\n",
      "torch.Size([1, 18, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([55, 1, 155])\n",
      "torch.Size([18, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:   7%|▋         | 54/760 [00:05<01:12,  9.67it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([111, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   7%|▋         | 55/760 [00:05<01:14,  9.52it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 19, 288])\n",
      "torch.Size([1, 19, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([47, 1, 155])\n",
      "torch.Size([19, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:   7%|▋         | 56/760 [00:05<01:12,  9.66it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 39, 288])\n",
      "torch.Size([1, 39, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([108, 1, 155])\n",
      "torch.Size([39, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   8%|▊         | 57/760 [00:05<01:11,  9.76it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 26, 288])\n",
      "torch.Size([1, 26, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([55, 1, 155])\n",
      "torch.Size([26, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:   8%|▊         | 58/760 [00:05<01:10,  9.90it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 45, 288])\n",
      "torch.Size([1, 45, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([72, 1, 155])\n",
      "torch.Size([45, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   8%|▊         | 59/760 [00:05<01:09, 10.02it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 19, 288])\n",
      "torch.Size([1, 19, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([72, 1, 155])\n",
      "torch.Size([19, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:   8%|▊         | 60/760 [00:05<01:09, 10.13it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([86, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   8%|▊         | 61/760 [00:06<01:10,  9.86it/s]torch.Size([1, 16])\n",
      "d1\n",
      "torch.Size([1, 60, 288])\n",
      "torch.Size([1, 60, 155])\n",
      "d2\n",
      "torch.Size([1, 16, 288])\n",
      "torch.Size([1, 16, 2])\n",
      "\n",
      "\n",
      "torch.Size([158, 1, 155])\n",
      "torch.Size([60, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([16, 2]) \n",
      " torch.Size([1, 16])\n",
      "Validation DataLoader 0:   8%|▊         | 62/760 [00:06<01:11,  9.75it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 54, 288])\n",
      "torch.Size([1, 54, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([151, 1, 155])\n",
      "torch.Size([54, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:   8%|▊         | 63/760 [00:06<01:12,  9.68it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([114, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   8%|▊         | 64/760 [00:06<01:11,  9.77it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([81, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   9%|▊         | 65/760 [00:06<01:11,  9.69it/s]torch.Size([1, 4])\n",
      "d1\n",
      "torch.Size([1, 16, 288])\n",
      "torch.Size([1, 16, 155])\n",
      "d2\n",
      "torch.Size([1, 4, 288])\n",
      "torch.Size([1, 4, 2])\n",
      "\n",
      "\n",
      "torch.Size([46, 1, 155])\n",
      "torch.Size([16, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([4, 2]) \n",
      " torch.Size([1, 4])\n",
      "Validation DataLoader 0:   9%|▊         | 66/760 [00:06<01:10,  9.79it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 29, 288])\n",
      "torch.Size([1, 29, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([29, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:   9%|▉         | 67/760 [00:06<01:11,  9.68it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 52, 288])\n",
      "torch.Size([1, 52, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([111, 1, 155])\n",
      "torch.Size([52, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:   9%|▉         | 68/760 [00:06<01:10,  9.77it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([83, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:   9%|▉         | 69/760 [00:07<01:10,  9.85it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 55, 288])\n",
      "torch.Size([1, 55, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([96, 1, 155])\n",
      "torch.Size([55, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:   9%|▉         | 70/760 [00:07<01:10,  9.75it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 41, 288])\n",
      "torch.Size([1, 41, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([41, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:   9%|▉         | 71/760 [00:07<01:11,  9.69it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 37, 288])\n",
      "torch.Size([1, 37, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([90, 1, 155])\n",
      "torch.Size([37, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:   9%|▉         | 72/760 [00:07<01:10,  9.78it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 51, 288])\n",
      "torch.Size([1, 51, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([113, 1, 155])\n",
      "torch.Size([51, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:  10%|▉         | 73/760 [00:07<01:09,  9.87it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 28, 288])\n",
      "torch.Size([1, 28, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([28, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  10%|▉         | 74/760 [00:07<01:10,  9.74it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([116, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  10%|▉         | 75/760 [00:07<01:09,  9.80it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 41, 288])\n",
      "torch.Size([1, 41, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([85, 1, 155])\n",
      "torch.Size([41, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  10%|█         | 76/760 [00:07<01:10,  9.74it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([123, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:  10%|█         | 77/760 [00:07<01:09,  9.81it/s]torch.Size([1, 4])\n",
      "d1\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 155])\n",
      "d2\n",
      "torch.Size([1, 4, 288])\n",
      "torch.Size([1, 4, 2])\n",
      "\n",
      "\n",
      "torch.Size([49, 1, 155])\n",
      "torch.Size([15, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([4, 2]) \n",
      " torch.Size([1, 4])\n",
      "Validation DataLoader 0:  10%|█         | 78/760 [00:07<01:09,  9.77it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([108, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  10%|█         | 79/760 [00:08<01:09,  9.84it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 34, 288])\n",
      "torch.Size([1, 34, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([113, 1, 155])\n",
      "torch.Size([34, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:  11%|█         | 80/760 [00:08<01:08,  9.91it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 45, 288])\n",
      "torch.Size([1, 45, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([100, 1, 155])\n",
      "torch.Size([45, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  11%|█         | 81/760 [00:08<01:07, 10.00it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 31, 288])\n",
      "torch.Size([1, 31, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([88, 1, 155])\n",
      "torch.Size([31, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:  11%|█         | 82/760 [00:08<01:09,  9.77it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 42, 288])\n",
      "torch.Size([1, 42, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([71, 1, 155])\n",
      "torch.Size([42, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  11%|█         | 83/760 [00:08<01:08,  9.85it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([60, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:  11%|█         | 84/760 [00:08<01:09,  9.77it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 28, 288])\n",
      "torch.Size([1, 28, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([72, 1, 155])\n",
      "torch.Size([28, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:  11%|█         | 85/760 [00:08<01:08,  9.86it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([114, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  11%|█▏        | 86/760 [00:08<01:08,  9.79it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 49, 288])\n",
      "torch.Size([1, 49, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([95, 1, 155])\n",
      "torch.Size([49, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  11%|█▏        | 87/760 [00:08<01:08,  9.85it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 35, 288])\n",
      "torch.Size([1, 35, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([108, 1, 155])\n",
      "torch.Size([35, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:  12%|█▏        | 88/760 [00:09<01:08,  9.77it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([129, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  12%|█▏        | 89/760 [00:09<01:09,  9.70it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([122, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  12%|█▏        | 90/760 [00:09<01:08,  9.76it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 25, 288])\n",
      "torch.Size([1, 25, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([71, 1, 155])\n",
      "torch.Size([25, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:  12%|█▏        | 91/760 [00:09<01:08,  9.72it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 55, 288])\n",
      "torch.Size([1, 55, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([95, 1, 155])\n",
      "torch.Size([55, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:  12%|█▏        | 92/760 [00:09<01:08,  9.79it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([104, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  12%|█▏        | 93/760 [00:09<01:08,  9.71it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([85, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  12%|█▏        | 94/760 [00:09<01:08,  9.78it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 19, 288])\n",
      "torch.Size([1, 19, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([76, 1, 155])\n",
      "torch.Size([19, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:  12%|█▎        | 95/760 [00:09<01:07,  9.80it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 37, 288])\n",
      "torch.Size([1, 37, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([82, 1, 155])\n",
      "torch.Size([37, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:  13%|█▎        | 96/760 [00:09<01:07,  9.79it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([91, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  13%|█▎        | 97/760 [00:09<01:08,  9.74it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 32, 288])\n",
      "torch.Size([1, 32, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([32, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  13%|█▎        | 98/760 [00:09<01:07,  9.81it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 52, 288])\n",
      "torch.Size([1, 52, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([111, 1, 155])\n",
      "torch.Size([52, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:  13%|█▎        | 99/760 [00:10<01:07,  9.74it/s]torch.Size([1, 7])\n",
      "d1\n",
      "torch.Size([1, 23, 288])\n",
      "torch.Size([1, 23, 155])\n",
      "d2\n",
      "torch.Size([1, 7, 288])\n",
      "torch.Size([1, 7, 2])\n",
      "\n",
      "\n",
      "torch.Size([77, 1, 155])\n",
      "torch.Size([23, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([7, 2]) \n",
      " torch.Size([1, 7])\n",
      "Validation DataLoader 0:  13%|█▎        | 100/760 [00:10<01:07,  9.81it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([90, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  13%|█▎        | 101/760 [00:10<01:07,  9.77it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([105, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  13%|█▎        | 102/760 [00:10<01:07,  9.82it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 42, 288])\n",
      "torch.Size([1, 42, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([105, 1, 155])\n",
      "torch.Size([42, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  14%|█▎        | 103/760 [00:10<01:07,  9.74it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 47, 288])\n",
      "torch.Size([1, 47, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([105, 1, 155])\n",
      "torch.Size([47, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  14%|█▎        | 104/760 [00:10<01:07,  9.76it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 32, 288])\n",
      "torch.Size([1, 32, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([65, 1, 155])\n",
      "torch.Size([32, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:  14%|█▍        | 105/760 [00:10<01:06,  9.80it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([98, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  14%|█▍        | 106/760 [00:10<01:07,  9.72it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([94, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  14%|█▍        | 107/760 [00:11<01:07,  9.67it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 56, 288])\n",
      "torch.Size([1, 56, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([108, 1, 155])\n",
      "torch.Size([56, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:  14%|█▍        | 108/760 [00:11<01:07,  9.67it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 44, 288])\n",
      "torch.Size([1, 44, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([160, 1, 155])\n",
      "torch.Size([44, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  14%|█▍        | 109/760 [00:11<01:07,  9.63it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 32, 288])\n",
      "torch.Size([1, 32, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([32, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:  14%|█▍        | 110/760 [00:11<01:07,  9.69it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 31, 288])\n",
      "torch.Size([1, 31, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([75, 1, 155])\n",
      "torch.Size([31, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  15%|█▍        | 111/760 [00:11<01:06,  9.75it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 36, 288])\n",
      "torch.Size([1, 36, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([84, 1, 155])\n",
      "torch.Size([36, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:  15%|█▍        | 112/760 [00:11<01:06,  9.68it/s]torch.Size([1, 15])\n",
      "d1\n",
      "torch.Size([1, 54, 288])\n",
      "torch.Size([1, 54, 155])\n",
      "d2\n",
      "torch.Size([1, 15, 288])\n",
      "torch.Size([1, 15, 2])\n",
      "\n",
      "\n",
      "torch.Size([151, 1, 155])\n",
      "torch.Size([54, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([15, 2]) \n",
      " torch.Size([1, 15])\n",
      "Validation DataLoader 0:  15%|█▍        | 113/760 [00:11<01:07,  9.63it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([116, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  15%|█▌        | 114/760 [00:11<01:07,  9.63it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 28, 288])\n",
      "torch.Size([1, 28, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([74, 1, 155])\n",
      "torch.Size([28, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  15%|█▌        | 115/760 [00:11<01:06,  9.63it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([79, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  15%|█▌        | 116/760 [00:12<01:07,  9.58it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 23, 288])\n",
      "torch.Size([1, 23, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([62, 1, 155])\n",
      "torch.Size([23, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:  15%|█▌        | 117/760 [00:12<01:06,  9.64it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  16%|█▌        | 118/760 [00:12<01:06,  9.59it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 41, 288])\n",
      "torch.Size([1, 41, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([116, 1, 155])\n",
      "torch.Size([41, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  16%|█▌        | 119/760 [00:12<01:07,  9.55it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 30, 288])\n",
      "torch.Size([1, 30, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([87, 1, 155])\n",
      "torch.Size([30, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  16%|█▌        | 120/760 [00:12<01:06,  9.58it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 52, 288])\n",
      "torch.Size([1, 52, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([69, 1, 155])\n",
      "torch.Size([52, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:  16%|█▌        | 121/760 [00:12<01:06,  9.55it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 29, 288])\n",
      "torch.Size([1, 29, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([85, 1, 155])\n",
      "torch.Size([29, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  16%|█▌        | 122/760 [00:12<01:06,  9.60it/s]torch.Size([1, 8])\n",
      "d1\n",
      "torch.Size([1, 32, 288])\n",
      "torch.Size([1, 32, 155])\n",
      "d2\n",
      "torch.Size([1, 8, 288])\n",
      "torch.Size([1, 8, 2])\n",
      "\n",
      "\n",
      "torch.Size([57, 1, 155])\n",
      "torch.Size([32, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([8, 2]) \n",
      " torch.Size([1, 8])\n",
      "Validation DataLoader 0:  16%|█▌        | 123/760 [00:12<01:06,  9.56it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 46, 288])\n",
      "torch.Size([1, 46, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([76, 1, 155])\n",
      "torch.Size([46, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  16%|█▋        | 124/760 [00:12<01:06,  9.60it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 18, 288])\n",
      "torch.Size([1, 18, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([49, 1, 155])\n",
      "torch.Size([18, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:  16%|█▋        | 125/760 [00:13<01:06,  9.57it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([85, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  17%|█▋        | 126/760 [00:13<01:06,  9.56it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 48, 288])\n",
      "torch.Size([1, 48, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([112, 1, 155])\n",
      "torch.Size([48, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  17%|█▋        | 127/760 [00:13<01:06,  9.50it/s]torch.Size([1, 12])\n",
      "d1\n",
      "torch.Size([1, 45, 288])\n",
      "torch.Size([1, 45, 155])\n",
      "d2\n",
      "torch.Size([1, 12, 288])\n",
      "torch.Size([1, 12, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([45, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([12, 2]) \n",
      " torch.Size([1, 12])\n",
      "Validation DataLoader 0:  17%|█▋        | 128/760 [00:13<01:06,  9.55it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 51, 288])\n",
      "torch.Size([1, 51, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([101, 1, 155])\n",
      "torch.Size([51, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:  17%|█▋        | 129/760 [00:13<01:06,  9.52it/s]torch.Size([1, 14])\n",
      "d1\n",
      "torch.Size([1, 50, 288])\n",
      "torch.Size([1, 50, 155])\n",
      "d2\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 2])\n",
      "\n",
      "\n",
      "torch.Size([93, 1, 155])\n",
      "torch.Size([50, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([14, 2]) \n",
      " torch.Size([1, 14])\n",
      "Validation DataLoader 0:  17%|█▋        | 130/760 [00:13<01:05,  9.55it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([61, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:  17%|█▋        | 131/760 [00:13<01:05,  9.55it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 40, 288])\n",
      "torch.Size([1, 40, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([88, 1, 155])\n",
      "torch.Size([40, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  17%|█▋        | 132/760 [00:13<01:05,  9.52it/s]torch.Size([1, 4])\n",
      "d1\n",
      "torch.Size([1, 14, 288])\n",
      "torch.Size([1, 14, 155])\n",
      "d2\n",
      "torch.Size([1, 4, 288])\n",
      "torch.Size([1, 4, 2])\n",
      "\n",
      "\n",
      "torch.Size([48, 1, 155])\n",
      "torch.Size([14, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([4, 2]) \n",
      " torch.Size([1, 4])\n",
      "Validation DataLoader 0:  18%|█▊        | 133/760 [00:13<01:05,  9.58it/s]torch.Size([1, 13])\n",
      "d1\n",
      "torch.Size([1, 47, 288])\n",
      "torch.Size([1, 47, 155])\n",
      "d2\n",
      "torch.Size([1, 13, 288])\n",
      "torch.Size([1, 13, 2])\n",
      "\n",
      "\n",
      "torch.Size([122, 1, 155])\n",
      "torch.Size([47, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([13, 2]) \n",
      " torch.Size([1, 13])\n",
      "Validation DataLoader 0:  18%|█▊        | 134/760 [00:14<01:05,  9.52it/s]torch.Size([1, 6])\n",
      "d1\n",
      "torch.Size([1, 24, 288])\n",
      "torch.Size([1, 24, 155])\n",
      "d2\n",
      "torch.Size([1, 6, 288])\n",
      "torch.Size([1, 6, 2])\n",
      "\n",
      "\n",
      "torch.Size([83, 1, 155])\n",
      "torch.Size([24, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([6, 2]) \n",
      " torch.Size([1, 6])\n",
      "Validation DataLoader 0:  18%|█▊        | 135/760 [00:14<01:05,  9.55it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 42, 288])\n",
      "torch.Size([1, 42, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([88, 1, 155])\n",
      "torch.Size([42, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  18%|█▊        | 136/760 [00:14<01:05,  9.51it/s]torch.Size([1, 11])\n",
      "d1\n",
      "torch.Size([1, 43, 288])\n",
      "torch.Size([1, 43, 155])\n",
      "d2\n",
      "torch.Size([1, 11, 288])\n",
      "torch.Size([1, 11, 2])\n",
      "\n",
      "\n",
      "torch.Size([65, 1, 155])\n",
      "torch.Size([43, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([11, 2]) \n",
      " torch.Size([1, 11])\n",
      "Validation DataLoader 0:  18%|█▊        | 137/760 [00:14<01:05,  9.51it/s]torch.Size([1, 9])\n",
      "d1\n",
      "torch.Size([1, 34, 288])\n",
      "torch.Size([1, 34, 155])\n",
      "d2\n",
      "torch.Size([1, 9, 288])\n",
      "torch.Size([1, 9, 2])\n",
      "\n",
      "\n",
      "torch.Size([79, 1, 155])\n",
      "torch.Size([34, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([9, 2]) \n",
      " torch.Size([1, 9])\n",
      "Validation DataLoader 0:  18%|█▊        | 138/760 [00:14<01:05,  9.47it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 20, 288])\n",
      "torch.Size([1, 20, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([53, 1, 155])\n",
      "torch.Size([20, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:  18%|█▊        | 139/760 [00:14<01:05,  9.52it/s]torch.Size([1, 5])\n",
      "d1\n",
      "torch.Size([1, 19, 288])\n",
      "torch.Size([1, 19, 155])\n",
      "d2\n",
      "torch.Size([1, 5, 288])\n",
      "torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "torch.Size([115, 1, 155])\n",
      "torch.Size([19, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([5, 2]) \n",
      " torch.Size([1, 5])\n",
      "Validation DataLoader 0:  18%|█▊        | 140/760 [00:14<01:05,  9.48it/s]torch.Size([1, 10])\n",
      "d1\n",
      "torch.Size([1, 38, 288])\n",
      "torch.Size([1, 38, 155])\n",
      "d2\n",
      "torch.Size([1, 10, 288])\n",
      "torch.Size([1, 10, 2])\n",
      "\n",
      "\n",
      "torch.Size([97, 1, 155])\n",
      "torch.Size([38, 155])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1])\n",
      "\n",
      " torch.Size([10, 2]) \n",
      " torch.Size([1, 10])\n",
      "Validation DataLoader 0:  19%|█▊        | 141/760 [00:14<01:05,  9.52it/s]torch.Size([1, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicii/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.validate(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.conv_subsample.sequential.0.weight',\n",
       "              tensor([[[[ 0.2757,  0.0442,  0.1599],\n",
       "                        [-0.0467, -0.0237, -0.1374],\n",
       "                        [ 0.0466, -0.0505,  0.0152]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0040,  0.0281, -0.3300],\n",
       "                        [ 0.1879, -0.2522,  0.2929],\n",
       "                        [-0.1747, -0.1976,  0.1268]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0081, -0.0070,  0.3300],\n",
       "                        [ 0.0410, -0.1383,  0.1801],\n",
       "                        [ 0.2723,  0.1885, -0.0276]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.2627, -0.1031,  0.3285],\n",
       "                        [-0.3328,  0.1644,  0.1304],\n",
       "                        [-0.3268, -0.0623, -0.1417]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1631, -0.1962,  0.1556],\n",
       "                        [ 0.1559, -0.1948, -0.1843],\n",
       "                        [-0.2572,  0.0770,  0.1672]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0187,  0.2177, -0.3067],\n",
       "                        [-0.0058, -0.0803,  0.0455],\n",
       "                        [ 0.0059,  0.1258, -0.3198]]]])),\n",
       "             ('encoder.conv_subsample.sequential.0.bias',\n",
       "              tensor([ 0.1214, -0.2474, -0.2976,  0.1893, -0.2040, -0.1828, -0.0809,  0.2750,\n",
       "                       0.2701, -0.2639, -0.3300, -0.2095, -0.2884, -0.2468, -0.2161, -0.2883,\n",
       "                       0.0549, -0.3311,  0.1888,  0.1456, -0.1978,  0.0656, -0.1751, -0.1752,\n",
       "                      -0.2025,  0.1878,  0.0442,  0.0554,  0.2655, -0.1270,  0.2157,  0.2286,\n",
       "                       0.0745,  0.2219,  0.2551, -0.0099, -0.2891, -0.3076, -0.0964, -0.0334,\n",
       "                       0.1105, -0.3332,  0.1091,  0.2312, -0.0676, -0.3202, -0.2277, -0.1327,\n",
       "                       0.0246, -0.1045,  0.0840, -0.1740, -0.2968,  0.1930,  0.1568, -0.3132,\n",
       "                      -0.0925,  0.0979,  0.0970,  0.1986, -0.3115, -0.2068, -0.1384,  0.0017,\n",
       "                      -0.1821, -0.0802, -0.0952,  0.1418,  0.2585, -0.2703, -0.0120,  0.2268,\n",
       "                       0.1719,  0.1550, -0.2240,  0.0817,  0.2227, -0.0073, -0.0226, -0.1151,\n",
       "                       0.0250,  0.2342, -0.1329,  0.3088,  0.2063, -0.1323,  0.1358, -0.0866,\n",
       "                       0.1028, -0.2753, -0.2763, -0.3124,  0.1912,  0.1727, -0.1329, -0.2939,\n",
       "                      -0.2225, -0.1760,  0.0616,  0.0951, -0.1691,  0.1206,  0.2191, -0.1415,\n",
       "                      -0.3094,  0.1024, -0.0708, -0.0643,  0.0902, -0.3249,  0.1147,  0.1409,\n",
       "                       0.1543,  0.2592, -0.2970, -0.2461, -0.1571,  0.0860, -0.3233,  0.1349,\n",
       "                      -0.0996,  0.2092, -0.0952,  0.2637, -0.2417, -0.0200, -0.1381,  0.0434,\n",
       "                       0.2016, -0.1022, -0.2916,  0.1684, -0.0917,  0.2228,  0.0574, -0.3016,\n",
       "                      -0.2728, -0.2921,  0.0786, -0.2475,  0.1352, -0.1226,  0.0420, -0.3313])),\n",
       "             ('encoder.conv_subsample.sequential.2.weight',\n",
       "              tensor([[[[ 0.0257, -0.0098, -0.0237],\n",
       "                        [ 0.0138,  0.0187,  0.0049],\n",
       "                        [-0.0040,  0.0235,  0.0275]],\n",
       "              \n",
       "                       [[-0.0137,  0.0114,  0.0078],\n",
       "                        [-0.0262,  0.0199,  0.0004],\n",
       "                        [ 0.0244,  0.0156, -0.0071]],\n",
       "              \n",
       "                       [[ 0.0275, -0.0001,  0.0200],\n",
       "                        [ 0.0040, -0.0237,  0.0223],\n",
       "                        [ 0.0099, -0.0023,  0.0126]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0008, -0.0264,  0.0127],\n",
       "                        [-0.0211, -0.0198, -0.0203],\n",
       "                        [-0.0008,  0.0101,  0.0217]],\n",
       "              \n",
       "                       [[ 0.0035, -0.0102, -0.0008],\n",
       "                        [-0.0269,  0.0269, -0.0029],\n",
       "                        [-0.0241, -0.0094, -0.0190]],\n",
       "              \n",
       "                       [[ 0.0154, -0.0039,  0.0210],\n",
       "                        [ 0.0052, -0.0224,  0.0245],\n",
       "                        [-0.0135, -0.0036, -0.0157]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0075, -0.0208, -0.0205],\n",
       "                        [ 0.0250, -0.0065, -0.0064],\n",
       "                        [-0.0191,  0.0004,  0.0229]],\n",
       "              \n",
       "                       [[ 0.0037, -0.0229,  0.0013],\n",
       "                        [-0.0136,  0.0243,  0.0020],\n",
       "                        [-0.0131, -0.0060,  0.0071]],\n",
       "              \n",
       "                       [[ 0.0246,  0.0022, -0.0185],\n",
       "                        [ 0.0084,  0.0071,  0.0207],\n",
       "                        [ 0.0061, -0.0125,  0.0086]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0237, -0.0225,  0.0134],\n",
       "                        [-0.0277, -0.0189, -0.0090],\n",
       "                        [ 0.0237,  0.0271,  0.0137]],\n",
       "              \n",
       "                       [[-0.0275, -0.0111,  0.0136],\n",
       "                        [-0.0170,  0.0080,  0.0240],\n",
       "                        [ 0.0033,  0.0012, -0.0262]],\n",
       "              \n",
       "                       [[-0.0057,  0.0222, -0.0121],\n",
       "                        [-0.0175,  0.0165,  0.0190],\n",
       "                        [-0.0085, -0.0257,  0.0156]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0231, -0.0037,  0.0136],\n",
       "                        [ 0.0257, -0.0100, -0.0067],\n",
       "                        [ 0.0099, -0.0242,  0.0174]],\n",
       "              \n",
       "                       [[-0.0053,  0.0219, -0.0091],\n",
       "                        [ 0.0205,  0.0118, -0.0234],\n",
       "                        [-0.0197,  0.0100,  0.0055]],\n",
       "              \n",
       "                       [[ 0.0095,  0.0181, -0.0249],\n",
       "                        [-0.0236, -0.0073,  0.0069],\n",
       "                        [ 0.0067, -0.0197, -0.0231]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0179,  0.0276, -0.0172],\n",
       "                        [ 0.0139,  0.0145, -0.0222],\n",
       "                        [-0.0004,  0.0094, -0.0195]],\n",
       "              \n",
       "                       [[ 0.0011,  0.0041,  0.0202],\n",
       "                        [-0.0099, -0.0089, -0.0256],\n",
       "                        [-0.0041,  0.0042, -0.0142]],\n",
       "              \n",
       "                       [[-0.0052,  0.0169,  0.0180],\n",
       "                        [ 0.0132,  0.0188,  0.0144],\n",
       "                        [-0.0203, -0.0088,  0.0197]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0160,  0.0214,  0.0112],\n",
       "                        [ 0.0043,  0.0061,  0.0106],\n",
       "                        [-0.0060,  0.0200, -0.0046]],\n",
       "              \n",
       "                       [[-0.0228, -0.0009,  0.0234],\n",
       "                        [-0.0230, -0.0089,  0.0091],\n",
       "                        [ 0.0066,  0.0262, -0.0101]],\n",
       "              \n",
       "                       [[ 0.0061,  0.0253, -0.0037],\n",
       "                        [ 0.0247,  0.0112, -0.0010],\n",
       "                        [ 0.0157,  0.0198, -0.0160]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0233, -0.0070, -0.0127],\n",
       "                        [ 0.0135,  0.0090,  0.0037],\n",
       "                        [-0.0087, -0.0263,  0.0082]],\n",
       "              \n",
       "                       [[ 0.0122,  0.0104,  0.0146],\n",
       "                        [-0.0131,  0.0261, -0.0271],\n",
       "                        [-0.0080,  0.0179,  0.0171]],\n",
       "              \n",
       "                       [[ 0.0174,  0.0278,  0.0065],\n",
       "                        [ 0.0048,  0.0126, -0.0113],\n",
       "                        [-0.0209,  0.0035, -0.0232]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0191, -0.0086, -0.0182],\n",
       "                        [-0.0204,  0.0059, -0.0093],\n",
       "                        [ 0.0009, -0.0093, -0.0275]],\n",
       "              \n",
       "                       [[-0.0231, -0.0200,  0.0063],\n",
       "                        [ 0.0177, -0.0254,  0.0234],\n",
       "                        [ 0.0118,  0.0035,  0.0239]],\n",
       "              \n",
       "                       [[ 0.0068,  0.0271,  0.0011],\n",
       "                        [ 0.0172,  0.0075, -0.0003],\n",
       "                        [ 0.0137, -0.0019,  0.0072]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0238,  0.0172, -0.0159],\n",
       "                        [-0.0250, -0.0227,  0.0169],\n",
       "                        [-0.0152,  0.0252, -0.0078]],\n",
       "              \n",
       "                       [[-0.0148,  0.0156, -0.0075],\n",
       "                        [-0.0134, -0.0088, -0.0015],\n",
       "                        [-0.0115,  0.0090,  0.0056]],\n",
       "              \n",
       "                       [[ 0.0053,  0.0277,  0.0060],\n",
       "                        [ 0.0070,  0.0056, -0.0050],\n",
       "                        [-0.0077, -0.0232, -0.0037]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0149, -0.0114, -0.0185],\n",
       "                        [-0.0014,  0.0062,  0.0209],\n",
       "                        [-0.0269,  0.0016,  0.0258]],\n",
       "              \n",
       "                       [[-0.0033,  0.0245,  0.0022],\n",
       "                        [ 0.0264, -0.0186, -0.0108],\n",
       "                        [ 0.0110,  0.0124,  0.0191]],\n",
       "              \n",
       "                       [[-0.0009,  0.0234, -0.0187],\n",
       "                        [ 0.0152,  0.0112, -0.0171],\n",
       "                        [-0.0074, -0.0155,  0.0173]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0128,  0.0188,  0.0262],\n",
       "                        [-0.0113, -0.0144,  0.0041],\n",
       "                        [ 0.0108,  0.0037, -0.0169]],\n",
       "              \n",
       "                       [[ 0.0255, -0.0203, -0.0185],\n",
       "                        [-0.0264, -0.0116,  0.0248],\n",
       "                        [-0.0095,  0.0126, -0.0107]],\n",
       "              \n",
       "                       [[ 0.0215,  0.0130, -0.0088],\n",
       "                        [-0.0241,  0.0273,  0.0108],\n",
       "                        [ 0.0229, -0.0138,  0.0094]]]])),\n",
       "             ('encoder.conv_subsample.sequential.2.bias',\n",
       "              tensor([ 0.0024, -0.0029,  0.0088, -0.0060, -0.0102, -0.0273,  0.0217, -0.0151,\n",
       "                       0.0090,  0.0194, -0.0027, -0.0265, -0.0269,  0.0103, -0.0105,  0.0070,\n",
       "                       0.0057, -0.0184, -0.0130,  0.0218, -0.0209,  0.0094, -0.0183, -0.0124,\n",
       "                      -0.0158, -0.0136,  0.0165,  0.0013,  0.0217, -0.0117,  0.0261,  0.0245,\n",
       "                       0.0219,  0.0248, -0.0097, -0.0254,  0.0266,  0.0213,  0.0226,  0.0269,\n",
       "                      -0.0089, -0.0013,  0.0170, -0.0240,  0.0223, -0.0224, -0.0193, -0.0243,\n",
       "                       0.0042,  0.0241,  0.0116, -0.0261,  0.0215, -0.0197,  0.0139, -0.0243,\n",
       "                      -0.0179,  0.0100, -0.0029, -0.0137, -0.0062,  0.0166, -0.0058, -0.0073,\n",
       "                      -0.0098,  0.0270, -0.0211, -0.0218, -0.0011,  0.0121, -0.0080, -0.0171,\n",
       "                       0.0211,  0.0088,  0.0200,  0.0158, -0.0154,  0.0245,  0.0071,  0.0132,\n",
       "                      -0.0099, -0.0241,  0.0261,  0.0250,  0.0015, -0.0165,  0.0040, -0.0051,\n",
       "                       0.0164, -0.0080, -0.0047,  0.0060, -0.0136,  0.0175,  0.0013, -0.0033,\n",
       "                       0.0231, -0.0233, -0.0137, -0.0019,  0.0004,  0.0234, -0.0113, -0.0264,\n",
       "                      -0.0230,  0.0171, -0.0103, -0.0098,  0.0100, -0.0125,  0.0212,  0.0258,\n",
       "                      -0.0156,  0.0019,  0.0087,  0.0100,  0.0105,  0.0081, -0.0115, -0.0018,\n",
       "                       0.0085, -0.0220, -0.0195,  0.0042,  0.0161,  0.0092, -0.0250,  0.0165,\n",
       "                      -0.0087, -0.0245,  0.0147,  0.0094, -0.0122, -0.0263,  0.0059,  0.0072,\n",
       "                      -0.0064,  0.0107,  0.0026, -0.0034,  0.0047,  0.0154, -0.0189, -0.0176])),\n",
       "             ('encoder.input_projection.0.linear.weight',\n",
       "              tensor([[-0.0120, -0.0328,  0.0105,  ..., -0.0449,  0.0261,  0.0091],\n",
       "                      [ 0.0253, -0.0163, -0.0319,  ...,  0.0340,  0.0208,  0.0421],\n",
       "                      [ 0.0183,  0.0038,  0.0129,  ...,  0.0395,  0.0202,  0.0155],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0382, -0.0102,  ...,  0.0035,  0.0438, -0.0273],\n",
       "                      [ 0.0181,  0.0239, -0.0337,  ..., -0.0109,  0.0168,  0.0253],\n",
       "                      [-0.0315, -0.0446,  0.0126,  ...,  0.0392, -0.0107,  0.0328]])),\n",
       "             ('encoder.input_projection.0.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.1.linear.weight',\n",
       "              tensor([[ 0.0575,  0.0564, -0.0887,  ..., -0.0491,  0.0821, -0.0329],\n",
       "                      [ 0.0624,  0.0815,  0.0664,  ...,  0.0512,  0.0543,  0.0429],\n",
       "                      [ 0.0770, -0.0601, -0.0773,  ..., -0.0077,  0.0839, -0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0517,  0.0223,  ...,  0.0841, -0.0759, -0.0853],\n",
       "                      [-0.0292, -0.0561,  0.0306,  ...,  0.0112, -0.0181, -0.0826],\n",
       "                      [-0.0879, -0.0386,  0.0420,  ..., -0.0751, -0.0406,  0.0810]])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.4.linear.weight',\n",
       "              tensor([[-0.0627, -0.0235, -0.0714,  ..., -0.0593,  0.0577, -0.0776],\n",
       "                      [-0.0249, -0.0752,  0.0650,  ...,  0.0030, -0.0655, -0.0687],\n",
       "                      [ 0.0120,  0.0795,  0.0781,  ..., -0.0570,  0.0704,  0.0302],\n",
       "                      ...,\n",
       "                      [ 0.0464, -0.0650, -0.0813,  ...,  0.0336,  0.0739, -0.0576],\n",
       "                      [ 0.0081,  0.0801, -0.0332,  ..., -0.0442, -0.0335,  0.0027],\n",
       "                      [ 0.0153,  0.0896,  0.0282,  ..., -0.0101,  0.0383, -0.0269]])),\n",
       "             ('encoder.layers.0.sequential.0.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.1.module.positional_encoding.pe',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                         0.0000e+00,  1.0000e+00],\n",
       "                       [ 8.4147e-01,  5.4030e-01,  7.7069e-01,  ...,  1.0000e+00,\n",
       "                         1.1365e-04,  1.0000e+00],\n",
       "                       [ 9.0930e-01, -4.1615e-01,  9.8218e-01,  ...,  1.0000e+00,\n",
       "                         2.2729e-04,  1.0000e+00],\n",
       "                       ...,\n",
       "                       [ 4.3692e-01,  8.9950e-01,  1.2617e-01,  ...,  2.7600e-01,\n",
       "                         9.0701e-01,  4.2111e-01],\n",
       "                       [ 9.9297e-01,  1.1834e-01,  8.4491e-01,  ...,  2.7588e-01,\n",
       "                         9.0706e-01,  4.2101e-01],\n",
       "                       [ 6.3609e-01, -7.7162e-01,  9.5065e-01,  ...,  2.7576e-01,\n",
       "                         9.0710e-01,  4.2091e-01]]])),\n",
       "             ('encoder.layers.0.sequential.1.module.layer_norm.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.1.module.layer_norm.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.u_bias',\n",
       "              tensor([[ 0.2155,  0.2171, -0.0976, -0.0692, -0.1005, -0.2172,  0.2250,  0.3838,\n",
       "                       -0.1934, -0.0485, -0.1812,  0.2235,  0.3860, -0.0404, -0.3565,  0.3740,\n",
       "                       -0.2522,  0.0826,  0.0363, -0.3296,  0.3737, -0.0930,  0.3546, -0.1342,\n",
       "                       -0.1937,  0.1558, -0.3736, -0.2684, -0.2607,  0.1121, -0.0275,  0.3640,\n",
       "                        0.0473,  0.1773, -0.3152, -0.0585],\n",
       "                      [-0.0622, -0.3195,  0.1050,  0.3309,  0.0247, -0.1418, -0.1643,  0.3457,\n",
       "                        0.0050, -0.0862,  0.1151,  0.2267,  0.3095,  0.3515, -0.0908,  0.0189,\n",
       "                       -0.0336, -0.3258, -0.0969,  0.2289, -0.2920, -0.1105, -0.2662,  0.1593,\n",
       "                       -0.3016, -0.2627, -0.2665,  0.0448, -0.1173,  0.1606,  0.2847, -0.2450,\n",
       "                       -0.3712,  0.0011,  0.3197, -0.2817],\n",
       "                      [-0.0004,  0.0923, -0.1657,  0.1028,  0.0389, -0.0959, -0.3198,  0.3302,\n",
       "                       -0.3708,  0.1025,  0.2880,  0.1511,  0.2158, -0.1768, -0.0359,  0.2056,\n",
       "                        0.3084,  0.2030,  0.2453,  0.0700,  0.2877, -0.0677, -0.3539, -0.0900,\n",
       "                       -0.3719, -0.1393,  0.0972,  0.1369,  0.2967, -0.3280, -0.3819, -0.3143,\n",
       "                       -0.3243,  0.2048,  0.2568, -0.2115],\n",
       "                      [-0.0219, -0.0359, -0.3178, -0.2637,  0.2200, -0.2754,  0.3136, -0.3121,\n",
       "                       -0.0861, -0.2307,  0.2064,  0.0072, -0.3034,  0.2087,  0.1830,  0.3086,\n",
       "                        0.2335,  0.0708, -0.0730,  0.1713, -0.0701, -0.0645,  0.0197,  0.3637,\n",
       "                       -0.1120,  0.0586, -0.0478, -0.3505, -0.2846, -0.0523, -0.2037,  0.2738,\n",
       "                        0.1438, -0.3051,  0.1338,  0.3154]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.v_bias',\n",
       "              tensor([[-0.0265, -0.2518, -0.0786, -0.2954, -0.1202, -0.1654,  0.3027, -0.3279,\n",
       "                       -0.0477,  0.3705, -0.2606,  0.3025,  0.2035, -0.3500, -0.0842,  0.0660,\n",
       "                       -0.2085,  0.0542, -0.2329, -0.3289,  0.1712,  0.0039, -0.3849, -0.3207,\n",
       "                       -0.0975,  0.0026,  0.0076,  0.3250,  0.3250, -0.3011, -0.3398,  0.2139,\n",
       "                       -0.1759, -0.1163,  0.0733, -0.1948],\n",
       "                      [-0.3768, -0.2219, -0.0070, -0.0905,  0.3515, -0.1975, -0.0065, -0.2060,\n",
       "                       -0.3666, -0.1684, -0.2280,  0.1291,  0.3146, -0.3861, -0.3588, -0.2431,\n",
       "                        0.0812, -0.0060,  0.3857, -0.2232,  0.0261, -0.0686,  0.1179,  0.3261,\n",
       "                       -0.1073,  0.2662, -0.2475, -0.0289,  0.3111, -0.1585, -0.2684,  0.3325,\n",
       "                        0.0163,  0.3166, -0.3742,  0.3849],\n",
       "                      [ 0.3687,  0.3797, -0.0722,  0.3463, -0.2193,  0.0827,  0.1995,  0.0825,\n",
       "                        0.1509,  0.1916,  0.0820, -0.2107,  0.0196,  0.0382, -0.3157,  0.0387,\n",
       "                        0.1447, -0.0937, -0.3114,  0.1013, -0.3315, -0.1991,  0.2094, -0.1475,\n",
       "                        0.0732, -0.1216, -0.2774, -0.1709,  0.2376,  0.2947, -0.0702, -0.3244,\n",
       "                        0.1221, -0.3624, -0.3591, -0.1595],\n",
       "                      [ 0.2266, -0.0692, -0.1035, -0.1108, -0.3697,  0.0798, -0.1354, -0.3713,\n",
       "                       -0.0187,  0.2767, -0.1459,  0.0960, -0.2619, -0.1597, -0.1657,  0.1318,\n",
       "                        0.0663, -0.2493,  0.2757,  0.2057,  0.1658,  0.2616,  0.2968, -0.3039,\n",
       "                       -0.0967, -0.3042,  0.0183, -0.2401,  0.2960,  0.1578, -0.3341, -0.1704,\n",
       "                       -0.3335, -0.3563, -0.3247,  0.1567]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.query_proj.linear.weight',\n",
       "              tensor([[ 0.0126,  0.0087,  0.0265,  ...,  0.0476, -0.1388, -0.0629],\n",
       "                      [-0.0618,  0.0359,  0.0273,  ...,  0.0983,  0.0289,  0.0490],\n",
       "                      [ 0.0650, -0.1119,  0.1144,  ..., -0.1238,  0.0678,  0.1076],\n",
       "                      ...,\n",
       "                      [ 0.0434, -0.1249,  0.0366,  ..., -0.1173, -0.0217,  0.1246],\n",
       "                      [ 0.0061, -0.0968, -0.0622,  ...,  0.0296, -0.0018, -0.0441],\n",
       "                      [-0.1265, -0.0346,  0.1397,  ..., -0.1422, -0.0003,  0.1376]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.key_proj.linear.weight',\n",
       "              tensor([[ 0.0903,  0.0906,  0.0772,  ..., -0.0726, -0.0844, -0.0213],\n",
       "                      [ 0.0969, -0.0242,  0.0681,  ...,  0.1307,  0.0752,  0.1178],\n",
       "                      [ 0.0733, -0.1433,  0.1029,  ...,  0.0131,  0.0932, -0.0149],\n",
       "                      ...,\n",
       "                      [-0.0358,  0.0277,  0.1338,  ..., -0.1339, -0.0095,  0.0670],\n",
       "                      [-0.0821,  0.1440,  0.1422,  ...,  0.0086, -0.0136,  0.1224],\n",
       "                      [ 0.1435,  0.1345,  0.1323,  ..., -0.0548,  0.0106,  0.0955]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.value_proj.linear.weight',\n",
       "              tensor([[ 0.1055, -0.0056, -0.0347,  ...,  0.1127,  0.1423, -0.1089],\n",
       "                      [ 0.0246, -0.1168, -0.1130,  ..., -0.1287, -0.0751,  0.1088],\n",
       "                      [ 0.0558,  0.1389,  0.1103,  ...,  0.0604, -0.0449, -0.0853],\n",
       "                      ...,\n",
       "                      [ 0.0650, -0.0313, -0.0166,  ..., -0.0919,  0.0913, -0.0353],\n",
       "                      [-0.0013,  0.0411,  0.0407,  ...,  0.1381,  0.0758, -0.0466],\n",
       "                      [ 0.0709, -0.0462,  0.0098,  ..., -0.1292, -0.0509, -0.0678]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.pos_proj.linear.weight',\n",
       "              tensor([[-0.0895,  0.1307,  0.0550,  ..., -0.0205,  0.0759, -0.0843],\n",
       "                      [-0.1047,  0.1292, -0.0284,  ..., -0.0509, -0.0743, -0.1222],\n",
       "                      [-0.0607,  0.1158, -0.0388,  ...,  0.0086,  0.1343,  0.1325],\n",
       "                      ...,\n",
       "                      [-0.0506, -0.1126,  0.0603,  ..., -0.1233, -0.0696,  0.1369],\n",
       "                      [-0.0852, -0.1344,  0.0846,  ...,  0.0402, -0.1248, -0.0333],\n",
       "                      [-0.1085, -0.1395, -0.0302,  ..., -0.1199,  0.1422, -0.1249]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.out_proj.linear.weight',\n",
       "              tensor([[-0.0271, -0.1077, -0.0503,  ...,  0.0260,  0.1339, -0.0997],\n",
       "                      [-0.0674, -0.1189,  0.0086,  ...,  0.1417, -0.1142,  0.1118],\n",
       "                      [ 0.0352,  0.0939,  0.0535,  ...,  0.0771, -0.0006,  0.1079],\n",
       "                      ...,\n",
       "                      [-0.0240,  0.0953, -0.0136,  ...,  0.1024,  0.0182, -0.1143],\n",
       "                      [ 0.0686,  0.0341,  0.1193,  ...,  0.1340,  0.1063, -0.0204],\n",
       "                      [ 0.0317, -0.1267, -0.1410,  ...,  0.1185,  0.0213, -0.0126]])),\n",
       "             ('encoder.layers.0.sequential.1.module.attention.out_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.2.conv.weight',\n",
       "              tensor([[[ 0.0054],\n",
       "                       [-0.0711],\n",
       "                       [ 0.0798],\n",
       "                       ...,\n",
       "                       [ 0.0735],\n",
       "                       [ 0.0203],\n",
       "                       [-0.0470]],\n",
       "              \n",
       "                      [[ 0.0232],\n",
       "                       [ 0.0501],\n",
       "                       [-0.0420],\n",
       "                       ...,\n",
       "                       [ 0.0013],\n",
       "                       [-0.0150],\n",
       "                       [ 0.0688]],\n",
       "              \n",
       "                      [[-0.0088],\n",
       "                       [ 0.0436],\n",
       "                       [ 0.0317],\n",
       "                       ...,\n",
       "                       [ 0.0010],\n",
       "                       [-0.0568],\n",
       "                       [ 0.0043]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0095],\n",
       "                       [-0.0118],\n",
       "                       [-0.0795],\n",
       "                       ...,\n",
       "                       [ 0.0029],\n",
       "                       [ 0.0759],\n",
       "                       [ 0.0420]],\n",
       "              \n",
       "                      [[-0.0227],\n",
       "                       [-0.0299],\n",
       "                       [ 0.0715],\n",
       "                       ...,\n",
       "                       [-0.0564],\n",
       "                       [ 0.0580],\n",
       "                       [-0.0617]],\n",
       "              \n",
       "                      [[-0.0681],\n",
       "                       [-0.0513],\n",
       "                       [ 0.0103],\n",
       "                       ...,\n",
       "                       [-0.0541],\n",
       "                       [-0.0826],\n",
       "                       [-0.0223]]])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.2.conv.bias',\n",
       "              tensor([ 5.2065e-02,  2.8517e-02,  8.0448e-03,  7.9715e-02,  1.9320e-02,\n",
       "                      -5.5805e-02, -2.4161e-03, -1.8153e-02,  1.0915e-02,  5.5568e-02,\n",
       "                      -3.3351e-03, -1.3074e-02, -3.4172e-02, -7.3631e-02, -1.7275e-02,\n",
       "                       6.7635e-02,  1.4888e-02, -8.1075e-03,  5.0714e-02, -1.4262e-02,\n",
       "                      -6.8393e-02,  3.4801e-03, -6.3303e-02, -2.5390e-02,  1.3439e-02,\n",
       "                      -8.1545e-02, -5.9603e-02,  1.5547e-02, -2.2414e-02,  3.1281e-02,\n",
       "                       2.2000e-02,  4.3597e-02, -1.4653e-02, -6.0737e-02,  1.8232e-02,\n",
       "                       8.3267e-02, -6.8875e-02,  4.7508e-02,  4.3397e-03, -4.9591e-02,\n",
       "                      -6.8315e-02,  1.1605e-02,  5.1838e-02,  6.6646e-02, -6.3106e-02,\n",
       "                      -6.8818e-02, -2.9246e-02,  4.9134e-02,  6.1133e-02,  3.2273e-02,\n",
       "                       7.3185e-02,  1.4334e-02, -4.2427e-02, -5.0180e-02, -6.0846e-02,\n",
       "                      -6.9636e-02, -1.8512e-02, -2.5755e-03, -6.3220e-02, -5.2404e-03,\n",
       "                      -1.5938e-02, -4.6803e-02, -1.8314e-02,  3.0726e-02,  3.0023e-02,\n",
       "                      -5.0900e-02,  6.0144e-03, -5.2841e-03,  3.7938e-02,  1.0552e-04,\n",
       "                       8.6454e-03, -3.6070e-02,  7.0978e-02, -1.5450e-02, -7.2795e-02,\n",
       "                      -6.3519e-02,  7.6877e-02, -4.1895e-02,  1.4266e-02,  5.1488e-05,\n",
       "                      -5.8560e-02,  6.6862e-02,  5.5277e-02,  7.0281e-03, -6.4153e-02,\n",
       "                      -2.4613e-02,  2.2064e-02,  6.0945e-02, -2.7353e-02, -6.3429e-02,\n",
       "                       5.4072e-02,  3.1023e-02, -7.0718e-02,  6.6393e-02,  5.6178e-02,\n",
       "                       1.4003e-02,  3.5719e-02, -1.7951e-02, -3.2928e-02, -7.2476e-02,\n",
       "                       4.1163e-02, -7.7056e-02, -2.9448e-02,  6.2668e-02,  6.7858e-02,\n",
       "                      -3.8007e-02, -3.9711e-02,  2.4051e-02, -2.4853e-02, -2.4010e-02,\n",
       "                      -2.3925e-02, -3.6330e-02,  6.9535e-02, -6.9799e-02,  2.7994e-02,\n",
       "                      -1.6700e-02, -2.4996e-04,  4.2984e-02, -2.2870e-02,  7.6534e-02,\n",
       "                       5.9470e-03,  1.9751e-02, -5.4670e-03, -2.2424e-02,  6.9581e-02,\n",
       "                       7.1100e-02,  2.9272e-02, -4.8464e-02,  3.6526e-02, -4.6552e-02,\n",
       "                       4.0874e-02, -1.2144e-02,  7.1271e-02,  1.9088e-02,  8.9098e-03,\n",
       "                       5.8075e-02, -1.4465e-02,  2.9085e-02, -8.0952e-02, -2.6849e-02,\n",
       "                       5.7322e-02,  3.6598e-02,  1.6407e-02,  6.2399e-02,  7.4082e-02,\n",
       "                       1.8064e-02,  7.4905e-02,  3.6974e-02, -2.0935e-02,  5.4718e-02,\n",
       "                      -4.5335e-02, -7.1794e-02,  6.2626e-02, -7.9710e-02, -6.7630e-03,\n",
       "                      -7.4464e-02, -4.5403e-02, -7.9019e-02, -4.7692e-02, -3.4119e-02,\n",
       "                       6.4084e-03, -2.8755e-02, -5.0058e-02, -1.2214e-02, -7.8348e-02,\n",
       "                       5.6179e-03,  3.1613e-02,  5.2616e-03,  3.4439e-02, -5.5094e-02,\n",
       "                      -2.1442e-02, -6.3880e-02, -6.7123e-02, -4.3233e-02, -2.6435e-02,\n",
       "                      -6.7269e-02, -6.6734e-02,  1.9274e-02,  1.4037e-02, -2.5039e-02,\n",
       "                      -1.9973e-02, -5.5874e-02, -8.8639e-03,  3.7828e-03, -3.7761e-02,\n",
       "                      -2.5057e-03,  1.5059e-02,  2.7876e-02,  6.5333e-03, -4.7170e-02,\n",
       "                      -4.7861e-02, -1.6132e-02, -3.3263e-02, -3.2132e-03, -4.5621e-02,\n",
       "                       2.2913e-02,  6.5376e-02,  4.0826e-02,  8.3619e-03,  2.8171e-02,\n",
       "                       4.4095e-02, -7.7568e-02, -9.4320e-03, -7.7819e-02,  4.6476e-02,\n",
       "                      -4.0519e-02,  6.1301e-02, -4.3583e-02, -4.7193e-02,  5.3047e-02,\n",
       "                       6.4686e-02,  4.3406e-03,  2.7696e-02, -5.1041e-02,  7.1739e-02,\n",
       "                       4.2160e-03, -1.8670e-02, -4.0077e-02,  6.8928e-02, -3.7160e-02,\n",
       "                      -2.8184e-02,  6.7654e-02,  1.0665e-02, -5.8072e-02, -3.6257e-02,\n",
       "                      -4.7540e-02, -6.9819e-02, -7.9002e-02, -6.5259e-02,  1.9774e-02,\n",
       "                      -6.8234e-02,  2.5024e-02,  5.1902e-02, -4.3537e-02, -7.4991e-02,\n",
       "                      -8.1431e-02, -1.7437e-02,  7.5273e-02,  5.5192e-02, -4.4586e-02,\n",
       "                      -7.3743e-02,  1.8590e-02, -7.4560e-02,  4.8880e-02,  7.8631e-03,\n",
       "                       1.9967e-02, -8.1984e-02, -1.8655e-02, -6.1732e-02, -5.4893e-03,\n",
       "                       8.1866e-02, -5.7616e-02,  2.3935e-02,  5.9049e-02,  2.9975e-02,\n",
       "                       2.6142e-03, -4.8756e-02,  1.3773e-02, -4.2475e-02,  2.1648e-02,\n",
       "                      -2.7090e-02,  2.0415e-02, -2.1287e-02, -3.8439e-02,  6.6125e-02,\n",
       "                      -3.9671e-02,  4.3113e-02,  5.1154e-02,  1.5546e-02, -2.7784e-02,\n",
       "                       7.2015e-02,  3.7927e-02, -3.1053e-02,  6.8003e-02,  6.8185e-02,\n",
       "                       5.7879e-02, -2.7022e-02, -1.6771e-02,  7.8414e-02, -5.9018e-02,\n",
       "                      -1.4706e-02,  5.0927e-03, -6.2750e-02,  3.7774e-03,  3.7162e-02,\n",
       "                      -5.2193e-02, -4.0785e-02,  3.0713e-02])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.4.conv.weight',\n",
       "              tensor([[[-0.1217,  0.0182,  0.0845,  ..., -0.0657, -0.0290,  0.1664]],\n",
       "              \n",
       "                      [[ 0.1764, -0.0693, -0.1120,  ...,  0.1772, -0.0301,  0.0691]],\n",
       "              \n",
       "                      [[-0.1337,  0.0713, -0.0131,  ...,  0.0824, -0.0967,  0.0775]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0611,  0.0613, -0.0781,  ..., -0.0897,  0.0981, -0.1044]],\n",
       "              \n",
       "                      [[-0.0694, -0.1176,  0.1090,  ..., -0.1622, -0.1445,  0.0424]],\n",
       "              \n",
       "                      [[-0.1722, -0.1220,  0.0339,  ..., -0.1408,  0.0603, -0.0406]]])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.5.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.5.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.5.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.5.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.7.conv.weight',\n",
       "              tensor([[[ 0.0474],\n",
       "                       [-0.0418],\n",
       "                       [-0.0494],\n",
       "                       ...,\n",
       "                       [ 0.0007],\n",
       "                       [ 0.0001],\n",
       "                       [-0.0821]],\n",
       "              \n",
       "                      [[-0.0219],\n",
       "                       [ 0.0215],\n",
       "                       [ 0.0182],\n",
       "                       ...,\n",
       "                       [-0.0473],\n",
       "                       [ 0.0332],\n",
       "                       [ 0.0132]],\n",
       "              \n",
       "                      [[-0.0030],\n",
       "                       [-0.0744],\n",
       "                       [ 0.0761],\n",
       "                       ...,\n",
       "                       [-0.0819],\n",
       "                       [-0.0064],\n",
       "                       [-0.0553]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0332],\n",
       "                       [ 0.0768],\n",
       "                       [ 0.0435],\n",
       "                       ...,\n",
       "                       [ 0.0792],\n",
       "                       [ 0.0397],\n",
       "                       [ 0.0425]],\n",
       "              \n",
       "                      [[ 0.0654],\n",
       "                       [ 0.0310],\n",
       "                       [ 0.0447],\n",
       "                       ...,\n",
       "                       [-0.0241],\n",
       "                       [ 0.0512],\n",
       "                       [-0.0771]],\n",
       "              \n",
       "                      [[-0.0302],\n",
       "                       [ 0.0316],\n",
       "                       [-0.0182],\n",
       "                       ...,\n",
       "                       [ 0.0797],\n",
       "                       [ 0.0263],\n",
       "                       [ 0.0783]]])),\n",
       "             ('encoder.layers.0.sequential.2.module.sequential.7.conv.bias',\n",
       "              tensor([-0.0365,  0.0522, -0.0025, -0.0307,  0.0527,  0.0571, -0.0476,  0.0435,\n",
       "                      -0.0439,  0.0610,  0.0473, -0.0053,  0.0056,  0.0524, -0.0661, -0.0280,\n",
       "                       0.0238, -0.0831, -0.0710, -0.0035,  0.0451,  0.0249,  0.0249,  0.0618,\n",
       "                      -0.0515, -0.0479, -0.0742, -0.0219,  0.0450,  0.0030,  0.0490, -0.0114,\n",
       "                      -0.0624, -0.0150, -0.0819,  0.0573, -0.0821,  0.0016,  0.0272, -0.0434,\n",
       "                      -0.0750,  0.0236, -0.0142,  0.0042,  0.0592, -0.0557,  0.0546, -0.0556,\n",
       "                       0.0209,  0.0628, -0.0251,  0.0369,  0.0326,  0.0670, -0.0678,  0.0106,\n",
       "                       0.0531, -0.0413, -0.0797,  0.0829, -0.0793,  0.0115,  0.0269, -0.0056,\n",
       "                      -0.0312,  0.0677,  0.0746,  0.0685,  0.0622,  0.0710, -0.0726,  0.0451,\n",
       "                      -0.0498,  0.0368, -0.0687, -0.0819, -0.0561,  0.0665,  0.0378,  0.0436,\n",
       "                       0.0099, -0.0773, -0.0307,  0.0086, -0.0341, -0.0219, -0.0508, -0.0684,\n",
       "                       0.0305, -0.0570, -0.0496, -0.0433, -0.0669, -0.0070,  0.0562,  0.0778,\n",
       "                      -0.0051, -0.0201, -0.0631,  0.0218,  0.0212, -0.0298, -0.0664,  0.0299,\n",
       "                       0.0603,  0.0461, -0.0432,  0.0700, -0.0492, -0.0128,  0.0506, -0.0390,\n",
       "                       0.0473,  0.0806,  0.0162,  0.0329, -0.0619, -0.0669, -0.0127,  0.0092,\n",
       "                      -0.0095, -0.0350, -0.0068,  0.0291,  0.0393, -0.0408,  0.0523, -0.0826,\n",
       "                       0.0026, -0.0399,  0.0448,  0.0464, -0.0399,  0.0039, -0.0133, -0.0308,\n",
       "                       0.0613, -0.0180,  0.0543,  0.0111, -0.0656, -0.0272,  0.0175,  0.0396])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.1.linear.weight',\n",
       "              tensor([[ 0.0419,  0.0266,  0.0804,  ..., -0.0601,  0.0788, -0.0912],\n",
       "                      [ 0.0372,  0.0184,  0.0856,  ..., -0.0628, -0.0237,  0.0470],\n",
       "                      [-0.0177,  0.0435, -0.0765,  ..., -0.0728, -0.0715,  0.0608],\n",
       "                      ...,\n",
       "                      [-0.0487,  0.0258, -0.0646,  ...,  0.0144,  0.0738, -0.0592],\n",
       "                      [ 0.0248,  0.0742,  0.0792,  ...,  0.0234, -0.0870,  0.0011],\n",
       "                      [-0.0074, -0.0193,  0.0637,  ..., -0.0348,  0.0178, -0.0545]])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.4.linear.weight',\n",
       "              tensor([[ 0.0751, -0.0826,  0.0705,  ...,  0.0115, -0.0298,  0.0185],\n",
       "                      [-0.0754,  0.0416,  0.0405,  ..., -0.0770,  0.0494, -0.0165],\n",
       "                      [-0.0098,  0.0867, -0.0457,  ..., -0.0680, -0.0790, -0.0571],\n",
       "                      ...,\n",
       "                      [ 0.0551,  0.0685, -0.0085,  ..., -0.0463,  0.0505, -0.0895],\n",
       "                      [ 0.0886,  0.0511, -0.0137,  ...,  0.0835,  0.0546,  0.0897],\n",
       "                      [ 0.0400,  0.0491, -0.0413,  ..., -0.0256, -0.0573,  0.0773]])),\n",
       "             ('encoder.layers.0.sequential.3.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.0.sequential.4.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.0.sequential.4.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0812,  0.0023,  0.0496,  ...,  0.0871,  0.0491,  0.0074],\n",
       "                      [-0.0753,  0.0528,  0.0651,  ..., -0.0378, -0.0437,  0.0493],\n",
       "                      [ 0.0155, -0.0457,  0.0262,  ..., -0.0540, -0.0019, -0.0309],\n",
       "                      ...,\n",
       "                      [ 0.0561,  0.0657,  0.0455,  ..., -0.0840,  0.0145,  0.0276],\n",
       "                      [ 0.0433,  0.0660,  0.0325,  ...,  0.0574,  0.0597, -0.0115],\n",
       "                      [-0.0799, -0.0880,  0.0186,  ...,  0.0092, -0.0052, -0.0787]])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.4.linear.weight',\n",
       "              tensor([[ 0.0368,  0.0660, -0.0194,  ...,  0.0600,  0.0713,  0.0761],\n",
       "                      [-0.0302, -0.0547, -0.0805,  ...,  0.0099, -0.0117, -0.0299],\n",
       "                      [ 0.0689,  0.0065, -0.0377,  ..., -0.0874,  0.0363, -0.0350],\n",
       "                      ...,\n",
       "                      [ 0.0759,  0.0617, -0.0394,  ...,  0.0895,  0.0012,  0.0326],\n",
       "                      [ 0.0651, -0.0870, -0.0094,  ...,  0.0430, -0.0430,  0.0440],\n",
       "                      [ 0.0230, -0.0742,  0.0180,  ..., -0.0377,  0.0133, -0.0169]])),\n",
       "             ('encoder.layers.1.sequential.0.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.1.module.positional_encoding.pe',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                         0.0000e+00,  1.0000e+00],\n",
       "                       [ 8.4147e-01,  5.4030e-01,  7.7069e-01,  ...,  1.0000e+00,\n",
       "                         1.1365e-04,  1.0000e+00],\n",
       "                       [ 9.0930e-01, -4.1615e-01,  9.8218e-01,  ...,  1.0000e+00,\n",
       "                         2.2729e-04,  1.0000e+00],\n",
       "                       ...,\n",
       "                       [ 4.3692e-01,  8.9950e-01,  1.2617e-01,  ...,  2.7600e-01,\n",
       "                         9.0701e-01,  4.2111e-01],\n",
       "                       [ 9.9297e-01,  1.1834e-01,  8.4491e-01,  ...,  2.7588e-01,\n",
       "                         9.0706e-01,  4.2101e-01],\n",
       "                       [ 6.3609e-01, -7.7162e-01,  9.5065e-01,  ...,  2.7576e-01,\n",
       "                         9.0710e-01,  4.2091e-01]]])),\n",
       "             ('encoder.layers.1.sequential.1.module.layer_norm.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.1.module.layer_norm.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.u_bias',\n",
       "              tensor([[ 0.1266,  0.2032,  0.3128,  0.3319, -0.1762,  0.2703, -0.2383, -0.1636,\n",
       "                       -0.1013,  0.2444,  0.2882, -0.1760, -0.3350,  0.1272,  0.2347, -0.2326,\n",
       "                       -0.0791,  0.1794,  0.1615,  0.3026, -0.1745, -0.0433, -0.3001,  0.2756,\n",
       "                       -0.0547,  0.0296, -0.0217,  0.3221,  0.0744,  0.2661,  0.1704, -0.0768,\n",
       "                       -0.2033,  0.1854,  0.2222,  0.1575],\n",
       "                      [-0.2872,  0.0989, -0.2592,  0.1275,  0.0577,  0.3494,  0.1978, -0.1059,\n",
       "                       -0.2485, -0.2042,  0.3337,  0.1012, -0.1168,  0.1150, -0.0653,  0.3458,\n",
       "                        0.2906, -0.3249, -0.3085, -0.2689,  0.3560,  0.0514, -0.2794,  0.0879,\n",
       "                       -0.1791,  0.2523,  0.3555, -0.2740,  0.1990,  0.3086,  0.3637, -0.2206,\n",
       "                        0.1853,  0.1921,  0.2137, -0.3341],\n",
       "                      [-0.1224,  0.0582,  0.2860,  0.3691, -0.1319, -0.3674,  0.3093, -0.0013,\n",
       "                       -0.3344,  0.3360,  0.0211, -0.0266, -0.2151,  0.1623, -0.0560, -0.2112,\n",
       "                        0.1691, -0.1887, -0.2656,  0.1642, -0.1276, -0.1815,  0.1054,  0.3673,\n",
       "                       -0.1761, -0.0449, -0.1539,  0.0299, -0.0846, -0.3051, -0.1796,  0.2399,\n",
       "                        0.0741, -0.1844,  0.3851, -0.1068],\n",
       "                      [ 0.3375,  0.2716,  0.1041,  0.2511, -0.3550, -0.2580, -0.2998, -0.3636,\n",
       "                       -0.2620,  0.3812,  0.0951,  0.0057,  0.0813,  0.2665,  0.1630, -0.2307,\n",
       "                       -0.1279, -0.2788,  0.2736, -0.1662, -0.1838, -0.1817,  0.3629, -0.1321,\n",
       "                        0.0701,  0.2429,  0.0851, -0.1051, -0.0703, -0.0131, -0.3388, -0.1905,\n",
       "                        0.1922, -0.2979,  0.0612, -0.2566]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.v_bias',\n",
       "              tensor([[-0.0439, -0.0988, -0.0290,  0.2886,  0.3815, -0.1449,  0.1214, -0.3530,\n",
       "                       -0.0874, -0.0831, -0.1818,  0.1294, -0.2198,  0.1468,  0.1933, -0.1428,\n",
       "                       -0.2051, -0.0123,  0.0331, -0.0156,  0.1400,  0.0198, -0.3176,  0.3469,\n",
       "                        0.0373,  0.0114, -0.1545, -0.2829,  0.1465, -0.2308,  0.0145, -0.2205,\n",
       "                        0.3080, -0.3839,  0.0507, -0.1640],\n",
       "                      [ 0.2085, -0.2297,  0.0848, -0.3463,  0.1698, -0.3203,  0.3088,  0.1666,\n",
       "                       -0.1986,  0.1507,  0.3739,  0.1347, -0.2264, -0.3289,  0.3769,  0.3849,\n",
       "                        0.2545, -0.2475,  0.2846,  0.2894, -0.0012, -0.2222,  0.2927, -0.1132,\n",
       "                        0.0703,  0.3247, -0.2809,  0.1076, -0.2439, -0.3084, -0.1290,  0.2357,\n",
       "                        0.1063, -0.3481,  0.1082,  0.2301],\n",
       "                      [ 0.2924,  0.0556, -0.3223,  0.2419, -0.2436,  0.2876, -0.0559,  0.1394,\n",
       "                        0.2877, -0.0134,  0.2764, -0.2000, -0.1901,  0.0327,  0.2254,  0.1669,\n",
       "                        0.2309, -0.3110, -0.3101, -0.3740,  0.1584,  0.2659,  0.3205, -0.2240,\n",
       "                        0.2818,  0.0970,  0.0437, -0.1346, -0.0218,  0.0506, -0.1466, -0.0640,\n",
       "                       -0.3247,  0.3540,  0.3255,  0.2309],\n",
       "                      [ 0.3213, -0.0607, -0.2914,  0.3857, -0.1412,  0.2996,  0.1273,  0.0447,\n",
       "                       -0.0122, -0.2736,  0.1282,  0.2450,  0.3605,  0.2975, -0.2780, -0.1767,\n",
       "                        0.3163, -0.0831,  0.1122, -0.0682, -0.2802,  0.3127, -0.3277,  0.0391,\n",
       "                        0.1176,  0.0475, -0.0473,  0.0976,  0.2898, -0.3390,  0.2692,  0.2704,\n",
       "                       -0.3666, -0.2246,  0.0775,  0.1250]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.query_proj.linear.weight',\n",
       "              tensor([[ 0.1313, -0.0678,  0.0339,  ..., -0.0816, -0.0134,  0.1167],\n",
       "                      [ 0.0402,  0.0253, -0.1413,  ..., -0.0843, -0.1375, -0.0037],\n",
       "                      [ 0.1303,  0.0715,  0.0526,  ...,  0.0804, -0.0393,  0.0858],\n",
       "                      ...,\n",
       "                      [ 0.1355,  0.1320, -0.1429,  ..., -0.1414, -0.1099,  0.1339],\n",
       "                      [ 0.0828,  0.1259,  0.0832,  ..., -0.1170, -0.1042,  0.0207],\n",
       "                      [ 0.1253, -0.1377, -0.0353,  ...,  0.0500,  0.1419, -0.0637]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.key_proj.linear.weight',\n",
       "              tensor([[ 0.0670, -0.0984,  0.0065,  ..., -0.0114, -0.0125, -0.0288],\n",
       "                      [-0.0930,  0.0635, -0.1176,  ..., -0.1252,  0.1300, -0.0525],\n",
       "                      [ 0.0288, -0.0519, -0.0790,  ..., -0.1163, -0.0531, -0.0574],\n",
       "                      ...,\n",
       "                      [ 0.0979, -0.1431, -0.0521,  ...,  0.0151,  0.0264, -0.1350],\n",
       "                      [ 0.0310, -0.0465,  0.0777,  ...,  0.1340,  0.0791, -0.1093],\n",
       "                      [-0.0646, -0.0755,  0.1076,  ..., -0.0660,  0.0051,  0.0541]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.value_proj.linear.weight',\n",
       "              tensor([[ 0.1100,  0.0942,  0.0670,  ...,  0.0954,  0.0751, -0.1214],\n",
       "                      [ 0.0919, -0.1315,  0.0524,  ...,  0.0999, -0.0984, -0.0324],\n",
       "                      [ 0.0005, -0.1182, -0.0722,  ...,  0.0226,  0.0035,  0.0280],\n",
       "                      ...,\n",
       "                      [-0.0112,  0.0678,  0.0142,  ...,  0.0962,  0.1040, -0.1249],\n",
       "                      [-0.0461, -0.0022, -0.1268,  ...,  0.1278, -0.1253,  0.1380],\n",
       "                      [-0.1243,  0.0447,  0.1310,  ...,  0.1166,  0.1055,  0.1287]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.pos_proj.linear.weight',\n",
       "              tensor([[ 0.0176,  0.1018, -0.1099,  ..., -0.1031,  0.1150,  0.0410],\n",
       "                      [-0.0757, -0.1021, -0.0064,  ...,  0.0375, -0.0364,  0.1368],\n",
       "                      [-0.1212, -0.0600,  0.0228,  ...,  0.0228, -0.0598,  0.0506],\n",
       "                      ...,\n",
       "                      [ 0.1229, -0.0805, -0.0205,  ..., -0.0983,  0.0183, -0.0219],\n",
       "                      [-0.0232, -0.0635,  0.1262,  ..., -0.0376, -0.0955, -0.0799],\n",
       "                      [-0.0197,  0.1336,  0.0897,  ..., -0.0458,  0.0250,  0.1059]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.out_proj.linear.weight',\n",
       "              tensor([[-0.0333, -0.0225, -0.1397,  ...,  0.0287, -0.1097,  0.0343],\n",
       "                      [ 0.0588,  0.0632, -0.0317,  ...,  0.0268,  0.0634,  0.0034],\n",
       "                      [-0.0107,  0.0967,  0.0278,  ...,  0.0786, -0.0893,  0.1054],\n",
       "                      ...,\n",
       "                      [-0.0976, -0.0559,  0.0473,  ..., -0.0813,  0.0495,  0.0046],\n",
       "                      [ 0.0225,  0.0332,  0.0545,  ..., -0.1186, -0.1136, -0.1212],\n",
       "                      [-0.0698, -0.0323,  0.1055,  ..., -0.1057,  0.0737, -0.0650]])),\n",
       "             ('encoder.layers.1.sequential.1.module.attention.out_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.2.conv.weight',\n",
       "              tensor([[[ 0.0832],\n",
       "                       [ 0.0518],\n",
       "                       [-0.0657],\n",
       "                       ...,\n",
       "                       [ 0.0469],\n",
       "                       [ 0.0087],\n",
       "                       [ 0.0170]],\n",
       "              \n",
       "                      [[ 0.0158],\n",
       "                       [-0.0290],\n",
       "                       [ 0.0021],\n",
       "                       ...,\n",
       "                       [ 0.0465],\n",
       "                       [ 0.0597],\n",
       "                       [ 0.0283]],\n",
       "              \n",
       "                      [[ 0.0761],\n",
       "                       [-0.0287],\n",
       "                       [ 0.0029],\n",
       "                       ...,\n",
       "                       [ 0.0251],\n",
       "                       [ 0.0515],\n",
       "                       [-0.0191]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0659],\n",
       "                       [ 0.0406],\n",
       "                       [ 0.0098],\n",
       "                       ...,\n",
       "                       [ 0.0761],\n",
       "                       [ 0.0500],\n",
       "                       [ 0.0306]],\n",
       "              \n",
       "                      [[ 0.0585],\n",
       "                       [ 0.0806],\n",
       "                       [ 0.0424],\n",
       "                       ...,\n",
       "                       [-0.0306],\n",
       "                       [ 0.0531],\n",
       "                       [-0.0677]],\n",
       "              \n",
       "                      [[ 0.0003],\n",
       "                       [-0.0211],\n",
       "                       [ 0.0239],\n",
       "                       ...,\n",
       "                       [ 0.0655],\n",
       "                       [ 0.0149],\n",
       "                       [ 0.0167]]])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.2.conv.bias',\n",
       "              tensor([ 1.9990e-02,  2.3615e-02,  5.3475e-02,  7.5252e-02,  6.3788e-02,\n",
       "                      -2.3104e-02, -6.4218e-02,  5.0166e-02, -5.8663e-02, -6.2853e-02,\n",
       "                       4.1942e-02, -6.1103e-02,  7.4094e-02,  8.2867e-02, -2.3645e-02,\n",
       "                      -8.2874e-02, -1.2427e-02, -1.6085e-02,  4.9155e-02, -4.7776e-02,\n",
       "                       5.7664e-02,  3.7975e-02,  6.2838e-02,  3.8736e-02, -1.4543e-02,\n",
       "                      -5.8764e-02, -2.2256e-02,  5.5539e-02, -3.9318e-02, -2.4214e-02,\n",
       "                       2.4337e-02,  2.5194e-02,  1.2973e-02,  7.1648e-02,  6.3090e-02,\n",
       "                       4.2101e-02,  7.4917e-02, -7.1270e-02, -8.9440e-03, -5.1529e-02,\n",
       "                      -3.3437e-02, -2.4575e-02, -1.4064e-02, -4.6442e-02, -3.9595e-02,\n",
       "                       3.8519e-03, -2.2643e-02, -4.4501e-03, -7.5121e-02, -5.1136e-02,\n",
       "                      -4.5443e-02,  1.9253e-02, -2.8088e-02, -3.4256e-02,  7.2655e-02,\n",
       "                       6.1312e-02,  6.5153e-02, -9.4153e-04,  2.8839e-02,  4.4187e-02,\n",
       "                       3.2892e-03,  7.1276e-03, -3.2184e-02, -6.5005e-02,  1.5986e-02,\n",
       "                       7.3142e-02, -3.5260e-02, -8.1105e-02, -1.1029e-02, -2.0766e-02,\n",
       "                      -4.9940e-02,  3.4921e-02, -6.9872e-02, -4.3894e-02,  6.4518e-02,\n",
       "                       3.0944e-02, -5.6822e-02, -5.7109e-03,  4.8106e-02,  5.0975e-03,\n",
       "                       8.3092e-02,  7.9795e-02, -7.2478e-02, -7.4549e-02, -5.0510e-02,\n",
       "                       4.9053e-02,  3.4907e-02,  4.0725e-02, -7.8843e-02,  7.3915e-02,\n",
       "                       6.4395e-02,  6.4713e-02, -6.5601e-02, -3.2393e-02,  4.1338e-02,\n",
       "                       2.2046e-02, -2.0063e-02, -1.7121e-02,  3.0422e-02, -1.4152e-02,\n",
       "                       4.8269e-04, -7.5052e-03, -5.3073e-02,  8.4418e-03,  7.2161e-02,\n",
       "                       4.1118e-02, -5.7803e-02,  5.8680e-02, -4.7368e-02,  4.6772e-02,\n",
       "                      -8.7803e-03,  1.4994e-02,  7.2738e-02,  3.5772e-02,  3.5235e-02,\n",
       "                       4.2095e-02, -7.1083e-02,  2.1064e-02, -3.2291e-02,  5.7388e-02,\n",
       "                      -7.9001e-02, -5.2602e-02,  2.3049e-02, -2.9468e-02, -8.2196e-02,\n",
       "                      -2.4702e-02, -5.9812e-02,  7.9329e-02,  8.1797e-02,  2.1277e-03,\n",
       "                      -3.2929e-02,  4.3993e-02,  4.0034e-02,  7.4744e-02, -6.9859e-02,\n",
       "                       1.5446e-02,  2.4195e-02, -2.9816e-02,  5.4098e-02, -6.1291e-02,\n",
       "                       7.9014e-02,  4.7313e-03, -5.0376e-02, -4.3772e-02, -6.3260e-02,\n",
       "                      -7.3988e-02, -6.6413e-02, -1.2358e-02,  7.1664e-02, -6.1584e-02,\n",
       "                       6.8032e-02,  1.8633e-02,  7.2618e-02,  6.5870e-02, -2.5260e-03,\n",
       "                       6.1251e-02, -7.3837e-02, -3.7627e-02, -8.3375e-03,  5.4689e-02,\n",
       "                       2.5746e-03, -3.7306e-02,  4.0434e-02,  3.0572e-02,  7.1050e-02,\n",
       "                      -8.1385e-02, -1.5495e-02, -2.5020e-02, -2.8721e-02,  4.2682e-02,\n",
       "                      -6.2883e-06,  5.5580e-02,  5.7847e-02, -1.2396e-02,  7.9274e-02,\n",
       "                       3.2123e-02, -2.8533e-02, -6.2124e-02, -5.9857e-02, -1.0586e-02,\n",
       "                      -3.9117e-02,  3.3811e-03,  4.3193e-02, -1.6109e-02,  1.5167e-02,\n",
       "                      -6.0467e-02, -6.9033e-02, -4.8452e-02, -7.2342e-02, -2.0475e-02,\n",
       "                       1.9636e-03,  5.6818e-03, -1.3101e-02,  8.0061e-02, -1.5974e-02,\n",
       "                      -1.4869e-02,  8.6061e-03, -7.2323e-02, -5.8874e-02, -1.5137e-02,\n",
       "                       7.1789e-02, -2.4038e-02, -1.8339e-02, -7.7228e-02,  6.9533e-03,\n",
       "                       5.5050e-02,  3.6010e-02,  4.7103e-02, -6.4373e-02, -1.9320e-02,\n",
       "                      -5.3286e-02, -1.4685e-02, -7.2661e-02, -6.9929e-02, -7.0276e-02,\n",
       "                       7.9664e-02, -1.1996e-02,  1.5249e-02, -6.0032e-02,  7.4083e-02,\n",
       "                       4.3923e-02,  2.4881e-02,  5.3516e-02, -6.4219e-02, -6.0885e-02,\n",
       "                      -2.0991e-03,  7.0027e-02,  7.4264e-02,  6.3793e-03,  4.2847e-02,\n",
       "                       4.4711e-02,  4.8855e-02,  3.7264e-02,  5.2747e-02, -5.4684e-02,\n",
       "                       4.9246e-02,  9.7991e-03, -9.3145e-03,  6.8051e-02,  2.9902e-02,\n",
       "                      -1.9725e-02,  5.7636e-02, -7.2835e-02, -6.0591e-03,  2.1744e-02,\n",
       "                       5.3029e-02, -3.5353e-02,  4.8835e-02,  7.4913e-02,  9.0255e-03,\n",
       "                      -6.5526e-02, -1.8791e-02, -6.9112e-02, -8.0034e-02,  7.4463e-02,\n",
       "                      -7.9132e-02, -1.2557e-02,  9.9514e-03, -4.7431e-02,  2.8108e-02,\n",
       "                      -2.3643e-02, -3.1439e-03, -7.2486e-02,  5.1844e-02,  6.6249e-02,\n",
       "                       2.9077e-02, -6.9333e-02, -2.7847e-02, -3.6997e-02,  7.9397e-02,\n",
       "                       6.9384e-02,  6.9128e-02, -7.5415e-02, -1.7872e-02,  7.3329e-02,\n",
       "                       1.3559e-02, -4.8933e-02,  2.2775e-02,  6.5049e-02, -5.2332e-02,\n",
       "                       6.3708e-02,  4.5873e-02,  7.1985e-03,  6.6744e-02, -7.3378e-04,\n",
       "                      -3.1171e-02,  1.7386e-02,  6.0988e-02])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.4.conv.weight',\n",
       "              tensor([[[-0.0638, -0.0325, -0.1716,  ...,  0.1499, -0.1650,  0.0027]],\n",
       "              \n",
       "                      [[-0.0412, -0.1315, -0.0886,  ...,  0.1330, -0.1394, -0.1054]],\n",
       "              \n",
       "                      [[ 0.1737,  0.0738,  0.1384,  ...,  0.1099,  0.0044,  0.0015]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0947, -0.1014,  0.1198,  ...,  0.0920,  0.1431,  0.1375]],\n",
       "              \n",
       "                      [[-0.1732, -0.1606, -0.0312,  ...,  0.0967, -0.0475,  0.1282]],\n",
       "              \n",
       "                      [[ 0.1660, -0.0106,  0.0416,  ...,  0.1348, -0.0977,  0.0833]]])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.5.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.5.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.5.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.5.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.7.conv.weight',\n",
       "              tensor([[[-0.0574],\n",
       "                       [-0.0546],\n",
       "                       [ 0.0160],\n",
       "                       ...,\n",
       "                       [-0.0596],\n",
       "                       [ 0.0750],\n",
       "                       [-0.0083]],\n",
       "              \n",
       "                      [[-0.0302],\n",
       "                       [-0.0727],\n",
       "                       [-0.0104],\n",
       "                       ...,\n",
       "                       [ 0.0427],\n",
       "                       [ 0.0543],\n",
       "                       [ 0.0055]],\n",
       "              \n",
       "                      [[ 0.0060],\n",
       "                       [ 0.0392],\n",
       "                       [ 0.0340],\n",
       "                       ...,\n",
       "                       [-0.0714],\n",
       "                       [ 0.0023],\n",
       "                       [ 0.0235]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0704],\n",
       "                       [-0.0145],\n",
       "                       [ 0.0831],\n",
       "                       ...,\n",
       "                       [-0.0426],\n",
       "                       [ 0.0469],\n",
       "                       [ 0.0234]],\n",
       "              \n",
       "                      [[-0.0422],\n",
       "                       [-0.0418],\n",
       "                       [ 0.0292],\n",
       "                       ...,\n",
       "                       [ 0.0413],\n",
       "                       [-0.0820],\n",
       "                       [ 0.0396]],\n",
       "              \n",
       "                      [[-0.0362],\n",
       "                       [ 0.0780],\n",
       "                       [ 0.0047],\n",
       "                       ...,\n",
       "                       [-0.0160],\n",
       "                       [ 0.0091],\n",
       "                       [-0.0722]]])),\n",
       "             ('encoder.layers.1.sequential.2.module.sequential.7.conv.bias',\n",
       "              tensor([ 0.0143, -0.0778, -0.0100,  0.0596, -0.0250, -0.0169, -0.0448,  0.0438,\n",
       "                      -0.0319, -0.0127,  0.0829,  0.0064, -0.0025, -0.0390, -0.0284,  0.0025,\n",
       "                       0.0064,  0.0632,  0.0520, -0.0494,  0.0743,  0.0116, -0.0021, -0.0100,\n",
       "                      -0.0323,  0.0677,  0.0501, -0.0169, -0.0068, -0.0139, -0.0643,  0.0295,\n",
       "                      -0.0302, -0.0014,  0.0249,  0.0153,  0.0083,  0.0537,  0.0593, -0.0659,\n",
       "                      -0.0124,  0.0692,  0.0230,  0.0432, -0.0541,  0.0130, -0.0150,  0.0754,\n",
       "                       0.0555, -0.0216, -0.0499, -0.0320,  0.0772, -0.0045,  0.0775, -0.0232,\n",
       "                       0.0244, -0.0357, -0.0199, -0.0010, -0.0060, -0.0103,  0.0570,  0.0237,\n",
       "                      -0.0729, -0.0830,  0.0223,  0.0250, -0.0673, -0.0311,  0.0056, -0.0597,\n",
       "                      -0.0311, -0.0143, -0.0708,  0.0646,  0.0550,  0.0651, -0.0800, -0.0187,\n",
       "                      -0.0429,  0.0102, -0.0376, -0.0196,  0.0603, -0.0030, -0.0219,  0.0357,\n",
       "                      -0.0203, -0.0032, -0.0137, -0.0289,  0.0601,  0.0242, -0.0504, -0.0831,\n",
       "                      -0.0513,  0.0336, -0.0296,  0.0403,  0.0513,  0.0364,  0.0758,  0.0774,\n",
       "                       0.0097,  0.0294, -0.0253,  0.0373,  0.0475,  0.0172,  0.0694,  0.0515,\n",
       "                      -0.0813, -0.0478, -0.0285,  0.0566, -0.0276, -0.0615,  0.0151,  0.0007,\n",
       "                      -0.0543,  0.0744, -0.0402,  0.0010,  0.0662,  0.0576, -0.0281, -0.0326,\n",
       "                      -0.0292,  0.0544, -0.0698, -0.0572,  0.0653, -0.0255, -0.0057,  0.0179,\n",
       "                      -0.0464,  0.0486,  0.0644,  0.0766,  0.0163, -0.0630,  0.0822, -0.0098])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0761,  0.0886, -0.0359,  ..., -0.0841,  0.0191, -0.0063],\n",
       "                      [ 0.0788,  0.0298, -0.0805,  ..., -0.0532, -0.0640,  0.0134],\n",
       "                      [ 0.0093,  0.0599, -0.0344,  ..., -0.0347, -0.0252,  0.0227],\n",
       "                      ...,\n",
       "                      [-0.0129, -0.0259, -0.0537,  ...,  0.0531,  0.0664,  0.0228],\n",
       "                      [-0.0667,  0.0534,  0.0403,  ...,  0.0412,  0.0190, -0.0168],\n",
       "                      [ 0.0304,  0.0776, -0.0387,  ...,  0.0428,  0.0910,  0.0380]])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.4.linear.weight',\n",
       "              tensor([[ 0.0902, -0.0493, -0.0450,  ...,  0.0687, -0.0233, -0.0144],\n",
       "                      [ 0.0670,  0.0026, -0.0269,  ...,  0.0144, -0.0173, -0.0154],\n",
       "                      [-0.0461, -0.0613, -0.0547,  ..., -0.0502,  0.0156, -0.0551],\n",
       "                      ...,\n",
       "                      [-0.0518, -0.0620,  0.0739,  ..., -0.0831,  0.0233, -0.0390],\n",
       "                      [ 0.0491, -0.0732,  0.0446,  ...,  0.0166,  0.0495,  0.0604],\n",
       "                      [ 0.0842, -0.0882,  0.0679,  ..., -0.0006, -0.0623,  0.0021]])),\n",
       "             ('encoder.layers.1.sequential.3.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.1.sequential.4.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.1.sequential.4.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0857, -0.0152,  0.0870,  ...,  0.0135,  0.0856,  0.0454],\n",
       "                      [-0.0577,  0.0483, -0.0821,  ...,  0.0860, -0.0138, -0.0168],\n",
       "                      [-0.0188,  0.0558,  0.0853,  ..., -0.0251,  0.0534,  0.0263],\n",
       "                      ...,\n",
       "                      [-0.0004,  0.0003, -0.0711,  ..., -0.0799,  0.0676, -0.0306],\n",
       "                      [ 0.0791, -0.0719,  0.0788,  ...,  0.0248, -0.0139, -0.0770],\n",
       "                      [-0.0200, -0.0759, -0.0372,  ...,  0.0045,  0.0841, -0.0645]])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.4.linear.weight',\n",
       "              tensor([[ 3.3916e-02, -8.6284e-02, -1.2327e-03,  ..., -1.3887e-02,\n",
       "                       -5.8344e-02,  4.8527e-02],\n",
       "                      [ 6.9268e-03, -2.6890e-02, -8.0088e-02,  ..., -7.0569e-02,\n",
       "                        5.0778e-02, -2.2477e-02],\n",
       "                      [ 7.6833e-02, -4.3482e-02,  4.3976e-02,  ...,  6.7078e-03,\n",
       "                       -7.6468e-03, -9.8746e-05],\n",
       "                      ...,\n",
       "                      [-4.9971e-02, -7.2877e-02,  8.5208e-02,  ..., -1.4495e-02,\n",
       "                       -6.2739e-02,  4.3870e-02],\n",
       "                      [ 2.6550e-02,  1.3188e-03,  8.2988e-02,  ...,  9.0197e-02,\n",
       "                       -9.0123e-02,  6.0693e-02],\n",
       "                      [-6.8090e-02, -4.2053e-02, -4.3128e-02,  ..., -9.7274e-03,\n",
       "                       -6.9941e-02,  8.0358e-02]])),\n",
       "             ('encoder.layers.2.sequential.0.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.1.module.positional_encoding.pe',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                         0.0000e+00,  1.0000e+00],\n",
       "                       [ 8.4147e-01,  5.4030e-01,  7.7069e-01,  ...,  1.0000e+00,\n",
       "                         1.1365e-04,  1.0000e+00],\n",
       "                       [ 9.0930e-01, -4.1615e-01,  9.8218e-01,  ...,  1.0000e+00,\n",
       "                         2.2729e-04,  1.0000e+00],\n",
       "                       ...,\n",
       "                       [ 4.3692e-01,  8.9950e-01,  1.2617e-01,  ...,  2.7600e-01,\n",
       "                         9.0701e-01,  4.2111e-01],\n",
       "                       [ 9.9297e-01,  1.1834e-01,  8.4491e-01,  ...,  2.7588e-01,\n",
       "                         9.0706e-01,  4.2101e-01],\n",
       "                       [ 6.3609e-01, -7.7162e-01,  9.5065e-01,  ...,  2.7576e-01,\n",
       "                         9.0710e-01,  4.2091e-01]]])),\n",
       "             ('encoder.layers.2.sequential.1.module.layer_norm.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.1.module.layer_norm.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.u_bias',\n",
       "              tensor([[ 0.1869, -0.2963,  0.3237, -0.1948,  0.1666,  0.1307,  0.3872, -0.2534,\n",
       "                        0.3220, -0.1425, -0.1218, -0.1945, -0.2862, -0.2959, -0.0912,  0.0673,\n",
       "                       -0.3389,  0.2283,  0.3312, -0.2321,  0.3842, -0.3768,  0.2096,  0.1237,\n",
       "                       -0.1574,  0.2487, -0.0764, -0.0531, -0.0302, -0.0884, -0.0513,  0.0988,\n",
       "                       -0.1005, -0.2136,  0.3007,  0.2042],\n",
       "                      [-0.2056, -0.1267, -0.1840,  0.3468,  0.0766,  0.3405,  0.1173, -0.0575,\n",
       "                        0.1069, -0.2596,  0.1734,  0.2552,  0.1210, -0.3225, -0.3476,  0.2397,\n",
       "                        0.1120, -0.3738, -0.1064, -0.1589,  0.2148, -0.1386,  0.1778,  0.1327,\n",
       "                        0.0315, -0.1169,  0.3809, -0.1028,  0.0571, -0.3645, -0.1734,  0.3429,\n",
       "                       -0.3278, -0.1769,  0.3767, -0.3695],\n",
       "                      [-0.1581, -0.1341,  0.2336, -0.2931, -0.2419, -0.2427, -0.2749, -0.3056,\n",
       "                        0.3684, -0.1444, -0.1096, -0.3561,  0.0649, -0.2918,  0.0696, -0.2852,\n",
       "                       -0.0645, -0.0212,  0.3274,  0.1289,  0.1051,  0.3291,  0.1671,  0.0830,\n",
       "                       -0.3478,  0.0390,  0.2335, -0.2171,  0.0387,  0.2874, -0.3461, -0.1582,\n",
       "                       -0.2561, -0.3344, -0.1566,  0.0304],\n",
       "                      [-0.3353, -0.2670,  0.0672,  0.0427, -0.1214,  0.3787, -0.2922, -0.0072,\n",
       "                        0.2655,  0.0146,  0.1010,  0.0023,  0.0645, -0.0575,  0.3574,  0.0314,\n",
       "                        0.2789,  0.2786, -0.2720, -0.3550, -0.1396, -0.3670,  0.1393, -0.0282,\n",
       "                       -0.3769,  0.3676, -0.3736, -0.2779,  0.1675, -0.1731, -0.2651, -0.2373,\n",
       "                       -0.1918, -0.1299, -0.0312, -0.3375]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.v_bias',\n",
       "              tensor([[ 0.1659,  0.2489,  0.2791, -0.1292, -0.2911, -0.2853, -0.2617,  0.0601,\n",
       "                        0.2127, -0.2727, -0.3055,  0.1601,  0.0200, -0.3562,  0.1931,  0.2078,\n",
       "                        0.1978, -0.1644,  0.1363,  0.0310, -0.0267,  0.3243,  0.0481,  0.2849,\n",
       "                        0.1161,  0.0731, -0.2825,  0.3109,  0.2942, -0.1922, -0.3239,  0.0412,\n",
       "                       -0.0965,  0.3401, -0.3056, -0.1067],\n",
       "                      [-0.2886,  0.2823, -0.1299, -0.1650, -0.0022,  0.2665,  0.2767, -0.0142,\n",
       "                       -0.2698, -0.3340, -0.3720,  0.2661,  0.2876,  0.1907,  0.1300,  0.1380,\n",
       "                       -0.3544, -0.1863,  0.2130,  0.3481, -0.3556,  0.3389, -0.1442, -0.0674,\n",
       "                       -0.1889, -0.2619,  0.0030,  0.2149, -0.1263,  0.0305,  0.3417, -0.1448,\n",
       "                        0.2521, -0.2924, -0.0374, -0.3364],\n",
       "                      [-0.0987,  0.2236,  0.0695,  0.1517, -0.2415,  0.3183,  0.1997,  0.1768,\n",
       "                        0.0463, -0.0434,  0.2572,  0.2238,  0.1758, -0.2500, -0.1390,  0.2100,\n",
       "                       -0.2122, -0.2147,  0.2668, -0.1565,  0.2595,  0.2499, -0.0278, -0.2161,\n",
       "                       -0.2980, -0.3804,  0.1492, -0.3771, -0.1567, -0.3580, -0.1380,  0.2161,\n",
       "                        0.1649,  0.3457,  0.0904,  0.2370],\n",
       "                      [-0.2605, -0.3817,  0.2953,  0.0345,  0.0436,  0.3229, -0.1335,  0.2971,\n",
       "                        0.0370, -0.0262, -0.3202, -0.0120, -0.2903, -0.2914,  0.1858, -0.0522,\n",
       "                        0.2750, -0.0935,  0.1393,  0.1662,  0.2277,  0.0052,  0.3195, -0.2129,\n",
       "                        0.2976,  0.3530,  0.3785,  0.1855, -0.3572,  0.1508, -0.2247,  0.0238,\n",
       "                        0.1604,  0.2368,  0.1186,  0.0131]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.query_proj.linear.weight',\n",
       "              tensor([[ 0.1328,  0.0705, -0.0947,  ..., -0.0353, -0.0950, -0.1076],\n",
       "                      [ 0.0526,  0.0373, -0.0607,  ...,  0.1395, -0.0910, -0.1341],\n",
       "                      [ 0.0610, -0.1265,  0.1171,  ...,  0.0248,  0.0912, -0.0722],\n",
       "                      ...,\n",
       "                      [-0.0928,  0.0342, -0.0233,  ..., -0.0228,  0.0171, -0.0460],\n",
       "                      [ 0.0728,  0.0509, -0.0505,  ...,  0.0919, -0.1233, -0.1319],\n",
       "                      [ 0.1001, -0.1028, -0.1440,  ..., -0.0190, -0.1407, -0.1148]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.key_proj.linear.weight',\n",
       "              tensor([[-0.1420, -0.0232,  0.0382,  ...,  0.0546,  0.1318, -0.0909],\n",
       "                      [-0.0499, -0.0806, -0.1333,  ...,  0.1415,  0.0585,  0.0763],\n",
       "                      [-0.1226, -0.0961, -0.0206,  ..., -0.0442,  0.0236,  0.0617],\n",
       "                      ...,\n",
       "                      [ 0.0677, -0.1219, -0.0879,  ..., -0.0395,  0.1040, -0.1020],\n",
       "                      [ 0.0561,  0.0279, -0.1338,  ..., -0.1024,  0.0130,  0.0663],\n",
       "                      [ 0.0961,  0.1053, -0.0318,  ...,  0.0124,  0.0269, -0.0823]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.value_proj.linear.weight',\n",
       "              tensor([[-0.0434, -0.0628, -0.1126,  ..., -0.1354, -0.0073,  0.1091],\n",
       "                      [ 0.1155,  0.0096, -0.0421,  ..., -0.0699,  0.0269,  0.1289],\n",
       "                      [-0.0604, -0.0059, -0.0372,  ...,  0.0568,  0.1230, -0.0073],\n",
       "                      ...,\n",
       "                      [ 0.1417, -0.1245,  0.0868,  ...,  0.1008, -0.1308,  0.1265],\n",
       "                      [-0.0845, -0.1238, -0.0967,  ...,  0.1114, -0.0050,  0.1079],\n",
       "                      [ 0.0656,  0.0996, -0.1334,  ...,  0.0436,  0.0248,  0.0295]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.pos_proj.linear.weight',\n",
       "              tensor([[-0.0972,  0.0362, -0.0325,  ...,  0.0568, -0.1254, -0.1321],\n",
       "                      [-0.1332, -0.0584, -0.0750,  ..., -0.1111, -0.0517,  0.0659],\n",
       "                      [-0.1399, -0.0170,  0.0587,  ...,  0.0304, -0.0171, -0.0460],\n",
       "                      ...,\n",
       "                      [ 0.1259, -0.0719, -0.1325,  ...,  0.0206, -0.0154, -0.0046],\n",
       "                      [ 0.0077, -0.0245,  0.1093,  ...,  0.0749,  0.0370,  0.0856],\n",
       "                      [ 0.1333, -0.0147, -0.0931,  ..., -0.1164, -0.1095,  0.0962]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.out_proj.linear.weight',\n",
       "              tensor([[-0.0038, -0.0776, -0.0412,  ..., -0.0813, -0.0575, -0.1061],\n",
       "                      [-0.0491, -0.1100,  0.0412,  ..., -0.0556,  0.0948, -0.0490],\n",
       "                      [-0.0065,  0.0765, -0.0114,  ...,  0.1394,  0.1440,  0.1365],\n",
       "                      ...,\n",
       "                      [-0.0227,  0.0072, -0.0196,  ..., -0.1131, -0.0798,  0.0297],\n",
       "                      [-0.0438,  0.1282, -0.0698,  ..., -0.1406,  0.0120,  0.0005],\n",
       "                      [-0.1384,  0.0414, -0.1156,  ...,  0.1035,  0.1309,  0.1080]])),\n",
       "             ('encoder.layers.2.sequential.1.module.attention.out_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.2.conv.weight',\n",
       "              tensor([[[-0.0664],\n",
       "                       [-0.0304],\n",
       "                       [-0.0667],\n",
       "                       ...,\n",
       "                       [-0.0345],\n",
       "                       [-0.0135],\n",
       "                       [ 0.0387]],\n",
       "              \n",
       "                      [[ 0.0568],\n",
       "                       [ 0.0023],\n",
       "                       [-0.0212],\n",
       "                       ...,\n",
       "                       [-0.0481],\n",
       "                       [ 0.0641],\n",
       "                       [-0.0278]],\n",
       "              \n",
       "                      [[-0.0511],\n",
       "                       [-0.0355],\n",
       "                       [-0.0486],\n",
       "                       ...,\n",
       "                       [-0.0456],\n",
       "                       [-0.0808],\n",
       "                       [-0.0640]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0720],\n",
       "                       [ 0.0605],\n",
       "                       [-0.0418],\n",
       "                       ...,\n",
       "                       [ 0.0607],\n",
       "                       [ 0.0101],\n",
       "                       [ 0.0520]],\n",
       "              \n",
       "                      [[-0.0045],\n",
       "                       [ 0.0439],\n",
       "                       [-0.0739],\n",
       "                       ...,\n",
       "                       [ 0.0250],\n",
       "                       [ 0.0323],\n",
       "                       [-0.0385]],\n",
       "              \n",
       "                      [[ 0.0606],\n",
       "                       [-0.0494],\n",
       "                       [-0.0164],\n",
       "                       ...,\n",
       "                       [ 0.0789],\n",
       "                       [-0.0824],\n",
       "                       [ 0.0733]]])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.2.conv.bias',\n",
       "              tensor([ 7.9324e-02, -4.5130e-02, -3.4334e-02,  5.1418e-02, -3.5039e-02,\n",
       "                       7.5118e-02, -1.0408e-02,  3.5454e-02,  4.8525e-04, -6.1431e-02,\n",
       "                       3.6961e-02, -7.6733e-02,  6.5003e-02,  2.9905e-02, -3.1551e-02,\n",
       "                       2.5212e-02, -5.1967e-02,  5.7647e-02,  7.8450e-02,  2.9941e-02,\n",
       "                      -5.5693e-02,  7.5033e-02,  1.5562e-02,  5.0448e-02, -2.5524e-02,\n",
       "                      -6.7980e-02, -1.4205e-02, -6.1331e-02,  5.3490e-02,  5.8713e-02,\n",
       "                       2.7343e-02,  3.9483e-02, -5.8714e-02,  2.3971e-02, -3.0210e-02,\n",
       "                      -1.0244e-02,  2.7063e-02,  5.0595e-03, -2.5181e-02,  4.0050e-03,\n",
       "                      -3.0305e-02, -4.5072e-02, -7.0692e-02,  5.1690e-02,  7.1217e-02,\n",
       "                       2.4440e-02,  1.5689e-02,  2.5526e-02, -7.3407e-02, -3.7075e-02,\n",
       "                       2.9853e-02,  5.5794e-02,  3.7662e-02, -8.1934e-03, -2.6808e-02,\n",
       "                      -4.0553e-02, -8.0905e-02,  3.4136e-02,  3.9200e-02, -1.4335e-02,\n",
       "                       4.9921e-02, -5.7565e-02,  9.8238e-05, -1.5995e-02,  4.4889e-02,\n",
       "                       6.0260e-02,  3.7299e-02,  3.3461e-04, -5.8286e-02, -3.1033e-03,\n",
       "                       7.0179e-02, -6.1949e-02, -3.8418e-02,  3.9209e-02,  7.8285e-02,\n",
       "                      -6.3832e-02,  7.9260e-02,  6.6658e-02, -8.2127e-02, -4.1965e-02,\n",
       "                       1.8296e-02,  3.7935e-02, -5.8090e-02, -2.3746e-02, -1.1622e-02,\n",
       "                      -2.5471e-02,  2.2253e-02, -3.3201e-02, -3.9774e-02,  6.0693e-02,\n",
       "                      -1.6485e-03,  5.7439e-02, -4.8914e-02,  1.5163e-02, -5.3541e-02,\n",
       "                      -4.5477e-02,  1.7207e-02,  6.0992e-02, -6.5075e-02,  2.2982e-02,\n",
       "                      -3.9660e-02,  4.8373e-02,  1.6946e-02,  2.7085e-02,  7.2334e-03,\n",
       "                      -2.9726e-02, -7.2341e-02, -8.2684e-02,  4.8259e-02,  4.6978e-02,\n",
       "                      -4.1251e-03,  8.4006e-03,  4.9136e-02,  1.8900e-02,  4.1770e-03,\n",
       "                      -5.1581e-02,  6.7673e-02,  4.7686e-02,  5.3690e-02, -1.5892e-02,\n",
       "                      -4.1406e-02, -6.2286e-02,  6.8995e-02,  2.0390e-02, -2.0402e-02,\n",
       "                      -8.2400e-02,  3.6041e-02,  1.1242e-02, -5.7955e-02,  3.4603e-02,\n",
       "                       3.8649e-03,  1.0644e-02, -7.0388e-02, -3.6011e-02,  6.3246e-02,\n",
       "                       5.0039e-02,  5.9974e-02, -7.0012e-02, -6.0056e-02, -3.0728e-02,\n",
       "                      -4.5390e-02,  4.4035e-02, -7.6443e-02, -4.1113e-02,  2.3448e-02,\n",
       "                      -5.9559e-02, -8.1963e-02, -7.8575e-02,  4.3512e-02,  1.0471e-02,\n",
       "                       2.7291e-02, -4.7037e-02, -3.6665e-02, -1.1655e-02, -1.7491e-02,\n",
       "                      -3.7881e-02,  6.0364e-03, -4.2248e-02,  1.2584e-02,  1.4398e-02,\n",
       "                      -7.4360e-02,  3.4071e-02, -1.7258e-02,  3.1319e-04,  1.6847e-02,\n",
       "                      -6.2105e-02,  1.4632e-02,  1.6818e-02,  6.0257e-02, -5.0007e-02,\n",
       "                       1.0187e-03,  3.3634e-02, -6.4188e-02, -1.2940e-02,  5.9463e-02,\n",
       "                       3.2479e-02,  7.9017e-02,  3.0355e-02,  7.1371e-02, -7.7633e-02,\n",
       "                      -7.2707e-02, -5.3454e-02,  4.1730e-02, -6.9645e-02, -6.1624e-02,\n",
       "                       5.7917e-02,  3.6755e-02, -2.5074e-02,  6.3592e-03,  6.0198e-02,\n",
       "                      -4.7999e-02, -2.2226e-02,  3.8157e-02, -2.2651e-02, -8.0507e-02,\n",
       "                       8.2274e-02, -3.7937e-02,  3.6854e-02,  4.2513e-03, -3.7033e-03,\n",
       "                       1.8208e-02,  2.4995e-02,  6.4928e-02, -1.5798e-02,  2.3431e-02,\n",
       "                       3.2490e-03,  3.9290e-02,  7.6504e-02,  5.6363e-02,  6.7400e-02,\n",
       "                       4.8317e-02, -6.8665e-02,  4.5087e-02,  1.1619e-02,  7.3236e-03,\n",
       "                      -6.4667e-02, -1.7815e-02, -3.2287e-02,  1.9876e-03, -4.5700e-02,\n",
       "                      -7.5956e-02,  2.8104e-02,  6.1209e-02, -5.2894e-02,  5.1320e-02,\n",
       "                       7.3767e-02,  5.1293e-02,  5.2178e-02, -5.4565e-02, -7.3315e-02,\n",
       "                      -8.2112e-02, -1.0177e-02,  2.7660e-02, -5.2338e-02, -1.3096e-02,\n",
       "                       5.3942e-02, -7.0444e-02, -3.4553e-02, -8.1219e-02, -6.7310e-02,\n",
       "                      -9.4208e-03,  4.0593e-02,  3.7447e-02,  4.0161e-02,  7.5283e-02,\n",
       "                       1.8648e-02,  5.2003e-02, -7.5959e-04,  2.8971e-02, -5.3442e-02,\n",
       "                       5.0983e-02,  7.7797e-02, -5.7901e-02, -4.4628e-02, -3.6338e-02,\n",
       "                       3.4370e-02,  1.2530e-02, -5.9004e-02, -4.0772e-02, -1.9014e-02,\n",
       "                      -6.5436e-05, -3.2317e-02,  1.0253e-02, -3.2638e-02,  1.2048e-02,\n",
       "                      -8.0677e-02, -3.8241e-02, -5.7632e-02, -8.0768e-02, -6.9105e-02,\n",
       "                      -7.3350e-02, -4.6682e-03,  6.6846e-02, -5.7211e-02, -7.0417e-02,\n",
       "                      -4.2167e-02,  6.9014e-02, -3.9908e-02,  3.3279e-02,  3.9111e-02,\n",
       "                       4.6753e-02, -2.8598e-02,  9.7450e-04, -3.4306e-02, -4.3671e-02,\n",
       "                       8.0201e-02,  1.6933e-02, -5.9698e-02])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.4.conv.weight',\n",
       "              tensor([[[-0.0461,  0.0711, -0.1120,  ..., -0.1516, -0.0252,  0.1202]],\n",
       "              \n",
       "                      [[-0.0795, -0.0651,  0.0716,  ..., -0.1460, -0.0239,  0.0886]],\n",
       "              \n",
       "                      [[ 0.0493,  0.0106, -0.0326,  ...,  0.1115,  0.0402,  0.1074]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.1338,  0.0542,  0.0675,  ...,  0.1012,  0.0252, -0.0864]],\n",
       "              \n",
       "                      [[-0.0016, -0.0173, -0.1078,  ..., -0.1039, -0.0638, -0.1658]],\n",
       "              \n",
       "                      [[-0.0330, -0.1579, -0.1614,  ..., -0.0634,  0.1711, -0.0305]]])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.5.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.5.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.5.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.5.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.7.conv.weight',\n",
       "              tensor([[[-0.0548],\n",
       "                       [-0.0553],\n",
       "                       [-0.0700],\n",
       "                       ...,\n",
       "                       [ 0.0711],\n",
       "                       [-0.0285],\n",
       "                       [-0.0206]],\n",
       "              \n",
       "                      [[ 0.0522],\n",
       "                       [ 0.0196],\n",
       "                       [ 0.0542],\n",
       "                       ...,\n",
       "                       [-0.0080],\n",
       "                       [ 0.0105],\n",
       "                       [ 0.0641]],\n",
       "              \n",
       "                      [[-0.0349],\n",
       "                       [ 0.0144],\n",
       "                       [-0.0372],\n",
       "                       ...,\n",
       "                       [ 0.0824],\n",
       "                       [ 0.0504],\n",
       "                       [-0.0436]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0182],\n",
       "                       [ 0.0750],\n",
       "                       [ 0.0021],\n",
       "                       ...,\n",
       "                       [-0.0809],\n",
       "                       [ 0.0203],\n",
       "                       [ 0.0374]],\n",
       "              \n",
       "                      [[-0.0580],\n",
       "                       [-0.0584],\n",
       "                       [ 0.0612],\n",
       "                       ...,\n",
       "                       [-0.0285],\n",
       "                       [ 0.0103],\n",
       "                       [ 0.0409]],\n",
       "              \n",
       "                      [[ 0.0609],\n",
       "                       [-0.0347],\n",
       "                       [ 0.0732],\n",
       "                       ...,\n",
       "                       [-0.0442],\n",
       "                       [-0.0261],\n",
       "                       [ 0.0466]]])),\n",
       "             ('encoder.layers.2.sequential.2.module.sequential.7.conv.bias',\n",
       "              tensor([ 0.0248, -0.0393, -0.0317, -0.0505, -0.0381,  0.0396,  0.0182, -0.0012,\n",
       "                       0.0415,  0.0158, -0.0560, -0.0552, -0.0634,  0.0257, -0.0653,  0.0144,\n",
       "                       0.0276,  0.0809, -0.0780, -0.0491,  0.0206, -0.0050,  0.0537, -0.0268,\n",
       "                       0.0647, -0.0772, -0.0032,  0.0756,  0.0735, -0.0068,  0.0465,  0.0089,\n",
       "                      -0.0713, -0.0340,  0.0831,  0.0398, -0.0597,  0.0224,  0.0451, -0.0403,\n",
       "                      -0.0118, -0.0582, -0.0551,  0.0253,  0.0050,  0.0479, -0.0452, -0.0304,\n",
       "                       0.0800, -0.0017,  0.0431, -0.0406, -0.0113,  0.0372,  0.0560, -0.0166,\n",
       "                       0.0081,  0.0443, -0.0684, -0.0715, -0.0763,  0.0703, -0.0807, -0.0688,\n",
       "                      -0.0481,  0.0074, -0.0613,  0.0621,  0.0803, -0.0822,  0.0170, -0.0255,\n",
       "                      -0.0671, -0.0807, -0.0755, -0.0440, -0.0090,  0.0721,  0.0175, -0.0521,\n",
       "                      -0.0128, -0.0276, -0.0471, -0.0301, -0.0776,  0.0622, -0.0818,  0.0100,\n",
       "                      -0.0830,  0.0680,  0.0171,  0.0208, -0.0828,  0.0760,  0.0256, -0.0831,\n",
       "                      -0.0722, -0.0788, -0.0677, -0.0283, -0.0548, -0.0172,  0.0558, -0.0608,\n",
       "                       0.0721, -0.0312,  0.0328,  0.0796, -0.0143,  0.0360, -0.0590, -0.0221,\n",
       "                       0.0730,  0.0288, -0.0627, -0.0317,  0.0223, -0.0603, -0.0370,  0.0405,\n",
       "                      -0.0042, -0.0822,  0.0665,  0.0191,  0.0670, -0.0624,  0.0406,  0.0820,\n",
       "                      -0.0119, -0.0399,  0.0666,  0.0013,  0.0445, -0.0132,  0.0389, -0.0333,\n",
       "                       0.0041, -0.0394,  0.0467, -0.0313,  0.0782,  0.0752,  0.0117, -0.0422])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0171, -0.0624, -0.0051,  ...,  0.0356,  0.0516,  0.0135],\n",
       "                      [-0.0816, -0.0096, -0.0791,  ..., -0.0390,  0.0335,  0.0563],\n",
       "                      [ 0.0472,  0.0853,  0.0292,  ..., -0.0476, -0.0352, -0.0887],\n",
       "                      ...,\n",
       "                      [ 0.0300, -0.0609, -0.0126,  ...,  0.0296, -0.0482, -0.0681],\n",
       "                      [ 0.0360, -0.0006, -0.0542,  ...,  0.0827,  0.0345,  0.0187],\n",
       "                      [ 0.0274, -0.0659, -0.0554,  ...,  0.0894,  0.0652,  0.0181]])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.4.linear.weight',\n",
       "              tensor([[-0.0699,  0.0400,  0.0466,  ...,  0.0124, -0.0788,  0.0526],\n",
       "                      [ 0.0262,  0.0606,  0.0103,  ..., -0.0009, -0.0236,  0.0761],\n",
       "                      [-0.0190, -0.0044, -0.0799,  ..., -0.0693,  0.0530,  0.0851],\n",
       "                      ...,\n",
       "                      [ 0.0270,  0.0667,  0.0629,  ..., -0.0571,  0.0684,  0.0084],\n",
       "                      [ 0.0040,  0.0833, -0.0190,  ...,  0.0409,  0.0355, -0.0335],\n",
       "                      [ 0.0142,  0.0313,  0.0581,  ...,  0.0135, -0.0099, -0.0132]])),\n",
       "             ('encoder.layers.2.sequential.3.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.2.sequential.4.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.2.sequential.4.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0863, -0.0791, -0.0443,  ...,  0.0522, -0.0862, -0.0172],\n",
       "                      [-0.0643,  0.0321,  0.0374,  ..., -0.0190, -0.0853, -0.0446],\n",
       "                      [ 0.0442,  0.0836,  0.0033,  ..., -0.0262, -0.0901, -0.0152],\n",
       "                      ...,\n",
       "                      [-0.0421,  0.0568, -0.0556,  ..., -0.0455, -0.0137,  0.0553],\n",
       "                      [-0.0553, -0.0833,  0.0876,  ...,  0.0463,  0.0776,  0.0318],\n",
       "                      [ 0.0231,  0.0750, -0.0615,  ...,  0.0297,  0.0546,  0.0583]])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.4.linear.weight',\n",
       "              tensor([[-0.0514,  0.0752, -0.0899,  ..., -0.0753,  0.0424,  0.0559],\n",
       "                      [-0.0582,  0.0536,  0.0198,  ..., -0.0147,  0.0204,  0.0738],\n",
       "                      [ 0.0281, -0.0739, -0.0592,  ...,  0.0562, -0.0847,  0.0153],\n",
       "                      ...,\n",
       "                      [-0.0880, -0.0616,  0.0692,  ..., -0.0267, -0.0834, -0.0669],\n",
       "                      [-0.0226,  0.0678,  0.0451,  ...,  0.0438, -0.0693,  0.0545],\n",
       "                      [-0.0311,  0.0573, -0.0689,  ..., -0.0317, -0.0482, -0.0888]])),\n",
       "             ('encoder.layers.3.sequential.0.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.1.module.positional_encoding.pe',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                         0.0000e+00,  1.0000e+00],\n",
       "                       [ 8.4147e-01,  5.4030e-01,  7.7069e-01,  ...,  1.0000e+00,\n",
       "                         1.1365e-04,  1.0000e+00],\n",
       "                       [ 9.0930e-01, -4.1615e-01,  9.8218e-01,  ...,  1.0000e+00,\n",
       "                         2.2729e-04,  1.0000e+00],\n",
       "                       ...,\n",
       "                       [ 4.3692e-01,  8.9950e-01,  1.2617e-01,  ...,  2.7600e-01,\n",
       "                         9.0701e-01,  4.2111e-01],\n",
       "                       [ 9.9297e-01,  1.1834e-01,  8.4491e-01,  ...,  2.7588e-01,\n",
       "                         9.0706e-01,  4.2101e-01],\n",
       "                       [ 6.3609e-01, -7.7162e-01,  9.5065e-01,  ...,  2.7576e-01,\n",
       "                         9.0710e-01,  4.2091e-01]]])),\n",
       "             ('encoder.layers.3.sequential.1.module.layer_norm.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.1.module.layer_norm.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.u_bias',\n",
       "              tensor([[-0.1643,  0.0643,  0.2401,  0.2093,  0.3197, -0.0947, -0.0829, -0.3510,\n",
       "                       -0.0151,  0.0284, -0.3371,  0.3159,  0.0835,  0.2641, -0.2771,  0.2651,\n",
       "                       -0.2837,  0.2567, -0.3009, -0.0052, -0.0178, -0.0387, -0.3481,  0.0303,\n",
       "                        0.1872,  0.0210, -0.2853,  0.0304, -0.1367,  0.1221,  0.0155, -0.3091,\n",
       "                        0.1889, -0.0431,  0.2923,  0.3023],\n",
       "                      [-0.1718, -0.2511, -0.1948,  0.0855,  0.3412, -0.0945, -0.3556, -0.2907,\n",
       "                        0.1811, -0.2617,  0.3447, -0.2429,  0.2891,  0.2186,  0.0184, -0.1211,\n",
       "                       -0.3716,  0.1170, -0.2410, -0.0917, -0.0492,  0.1676, -0.1344,  0.0483,\n",
       "                        0.2337, -0.0614,  0.1604,  0.2907,  0.1272,  0.0020, -0.2357, -0.0429,\n",
       "                        0.0257, -0.1101, -0.1770,  0.3617],\n",
       "                      [ 0.3218,  0.1133, -0.3814, -0.3479,  0.1118,  0.3552, -0.2680,  0.3568,\n",
       "                       -0.2029,  0.0121,  0.1376,  0.3094, -0.1301,  0.1388, -0.0392,  0.0445,\n",
       "                       -0.1366, -0.3620,  0.2105, -0.0523,  0.2338,  0.2723, -0.2968, -0.1200,\n",
       "                        0.3749, -0.2078,  0.2078, -0.2239,  0.0930, -0.0858, -0.1273,  0.3379,\n",
       "                       -0.2959,  0.2396, -0.1794,  0.0275],\n",
       "                      [ 0.0435,  0.3419, -0.3756,  0.2227, -0.3610,  0.2848, -0.0638, -0.2227,\n",
       "                       -0.0998, -0.1680, -0.0180,  0.2403,  0.1773,  0.3826, -0.0667,  0.0434,\n",
       "                       -0.2651, -0.0553, -0.2013, -0.0664, -0.3520,  0.1371,  0.2073,  0.3155,\n",
       "                       -0.1617, -0.0123,  0.2043,  0.1276, -0.3068, -0.0207,  0.2671, -0.3776,\n",
       "                       -0.1864, -0.2509, -0.0714,  0.2015]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.v_bias',\n",
       "              tensor([[ 0.1859,  0.2803, -0.3606,  0.1112, -0.3664, -0.0056,  0.0895,  0.3663,\n",
       "                        0.0202,  0.0050,  0.1087, -0.0340, -0.1452,  0.3619, -0.2306, -0.1678,\n",
       "                       -0.1322,  0.1794,  0.1230, -0.2618, -0.3832,  0.1973, -0.3490, -0.3098,\n",
       "                       -0.1823,  0.2007, -0.3174, -0.0192, -0.1655,  0.1832, -0.2964, -0.0059,\n",
       "                        0.2471,  0.2598,  0.2460,  0.1655],\n",
       "                      [ 0.2963, -0.1849,  0.1141, -0.0799, -0.2620,  0.1369,  0.3047, -0.3449,\n",
       "                        0.3209,  0.2884, -0.3510,  0.3261,  0.0960,  0.2460,  0.3146,  0.3204,\n",
       "                        0.2102,  0.1092, -0.1847,  0.3064, -0.2421, -0.2024,  0.0402, -0.3602,\n",
       "                        0.2224,  0.0841,  0.1451, -0.1526,  0.1863, -0.1647, -0.1167, -0.1596,\n",
       "                       -0.0561, -0.0204, -0.1887,  0.3670],\n",
       "                      [-0.0045,  0.2925,  0.0508, -0.0420, -0.0908,  0.0237, -0.3275, -0.0416,\n",
       "                       -0.0967,  0.2545,  0.3679,  0.1830,  0.3018,  0.0792, -0.2017,  0.0588,\n",
       "                       -0.2661,  0.1124, -0.0302,  0.3794, -0.0683, -0.1434, -0.3036,  0.3012,\n",
       "                       -0.1328,  0.1092,  0.0726, -0.3319,  0.2143, -0.2888, -0.0394,  0.3138,\n",
       "                        0.0851,  0.0202,  0.3131,  0.3751],\n",
       "                      [ 0.2456,  0.3133,  0.1840,  0.0013,  0.0077, -0.1817, -0.2686, -0.2849,\n",
       "                       -0.1388,  0.1074,  0.2324, -0.3768,  0.3380, -0.3855,  0.0380, -0.1561,\n",
       "                       -0.0278,  0.1757, -0.1135, -0.1997,  0.3123, -0.2799,  0.2737,  0.1161,\n",
       "                        0.2353,  0.3321, -0.3616, -0.3476,  0.2904,  0.2844, -0.2664, -0.0980,\n",
       "                       -0.3366,  0.3756,  0.2262, -0.1745]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.query_proj.linear.weight',\n",
       "              tensor([[-0.1021, -0.0348, -0.0576,  ...,  0.0742,  0.0800, -0.0554],\n",
       "                      [ 0.0710,  0.0680,  0.0819,  ..., -0.0111, -0.1240,  0.0936],\n",
       "                      [-0.1151,  0.1144, -0.0590,  ..., -0.0319,  0.0048, -0.1275],\n",
       "                      ...,\n",
       "                      [-0.1316, -0.0343, -0.0120,  ...,  0.0383,  0.1398,  0.0711],\n",
       "                      [-0.1373,  0.0494, -0.0076,  ...,  0.0306, -0.0395, -0.1318],\n",
       "                      [ 0.1013,  0.0802,  0.0421,  ...,  0.0816, -0.0866, -0.1138]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.key_proj.linear.weight',\n",
       "              tensor([[ 0.1359, -0.1085,  0.0520,  ...,  0.0773, -0.0577, -0.1097],\n",
       "                      [ 0.1043, -0.0035, -0.0645,  ..., -0.1182, -0.0122,  0.0959],\n",
       "                      [-0.0735,  0.0823, -0.0541,  ...,  0.1076, -0.1305, -0.1097],\n",
       "                      ...,\n",
       "                      [ 0.0203, -0.0921,  0.1154,  ...,  0.0826,  0.0399, -0.0016],\n",
       "                      [-0.1226,  0.1231, -0.1332,  ..., -0.0385, -0.0623, -0.1240],\n",
       "                      [-0.1373,  0.0865, -0.0565,  ...,  0.1398, -0.0285, -0.1341]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.value_proj.linear.weight',\n",
       "              tensor([[-0.0978, -0.1443, -0.0163,  ..., -0.0692,  0.0042,  0.0064],\n",
       "                      [-0.0577, -0.0109, -0.1345,  ..., -0.1001, -0.0336, -0.0752],\n",
       "                      [ 0.0218,  0.1204, -0.0129,  ..., -0.1171,  0.0820,  0.0869],\n",
       "                      ...,\n",
       "                      [ 0.0839,  0.0044, -0.0340,  ...,  0.1395,  0.0271,  0.1004],\n",
       "                      [ 0.0162,  0.0273,  0.1197,  ..., -0.0912, -0.0006,  0.1167],\n",
       "                      [ 0.0099, -0.0660, -0.1034,  ..., -0.1104, -0.0933,  0.0504]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.pos_proj.linear.weight',\n",
       "              tensor([[ 0.1108, -0.1132,  0.0214,  ...,  0.0105,  0.0202, -0.1229],\n",
       "                      [ 0.0789,  0.0779, -0.1004,  ...,  0.1209,  0.1169,  0.0368],\n",
       "                      [-0.1105,  0.0829,  0.1146,  ..., -0.0116,  0.0814, -0.0799],\n",
       "                      ...,\n",
       "                      [-0.0231, -0.0968,  0.0722,  ..., -0.1068, -0.0865, -0.0883],\n",
       "                      [-0.1235, -0.0135, -0.0223,  ..., -0.1154,  0.0015,  0.0865],\n",
       "                      [-0.1380, -0.0929,  0.1369,  ...,  0.0611, -0.1185, -0.0562]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.out_proj.linear.weight',\n",
       "              tensor([[-0.0821,  0.0174, -0.0003,  ..., -0.0649,  0.1227,  0.0955],\n",
       "                      [-0.1000, -0.0493,  0.0347,  ..., -0.1366,  0.0699, -0.0256],\n",
       "                      [-0.0716,  0.0400,  0.0667,  ...,  0.0710, -0.0018, -0.0115],\n",
       "                      ...,\n",
       "                      [-0.1265, -0.0296, -0.0791,  ...,  0.1297, -0.1396,  0.0837],\n",
       "                      [-0.0901, -0.1323, -0.0567,  ..., -0.1365, -0.0020, -0.1060],\n",
       "                      [ 0.0017,  0.0646, -0.0672,  ...,  0.1106, -0.1163,  0.1164]])),\n",
       "             ('encoder.layers.3.sequential.1.module.attention.out_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.2.conv.weight',\n",
       "              tensor([[[ 0.0481],\n",
       "                       [-0.0491],\n",
       "                       [-0.0615],\n",
       "                       ...,\n",
       "                       [ 0.0789],\n",
       "                       [-0.0466],\n",
       "                       [-0.0052]],\n",
       "              \n",
       "                      [[-0.0537],\n",
       "                       [ 0.0289],\n",
       "                       [ 0.0739],\n",
       "                       ...,\n",
       "                       [-0.0536],\n",
       "                       [-0.0299],\n",
       "                       [ 0.0785]],\n",
       "              \n",
       "                      [[ 0.0739],\n",
       "                       [-0.0149],\n",
       "                       [-0.0069],\n",
       "                       ...,\n",
       "                       [ 0.0822],\n",
       "                       [-0.0075],\n",
       "                       [ 0.0564]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0419],\n",
       "                       [ 0.0549],\n",
       "                       [-0.0833],\n",
       "                       ...,\n",
       "                       [-0.0101],\n",
       "                       [ 0.0004],\n",
       "                       [-0.0556]],\n",
       "              \n",
       "                      [[ 0.0036],\n",
       "                       [ 0.0481],\n",
       "                       [ 0.0394],\n",
       "                       ...,\n",
       "                       [ 0.0156],\n",
       "                       [-0.0678],\n",
       "                       [ 0.0590]],\n",
       "              \n",
       "                      [[ 0.0782],\n",
       "                       [ 0.0554],\n",
       "                       [ 0.0178],\n",
       "                       ...,\n",
       "                       [-0.0415],\n",
       "                       [ 0.0392],\n",
       "                       [-0.0108]]])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.2.conv.bias',\n",
       "              tensor([ 0.0829, -0.0060, -0.0688,  0.0832,  0.0198, -0.0763, -0.0198, -0.0123,\n",
       "                       0.0104,  0.0627, -0.0714,  0.0319, -0.0681, -0.0342, -0.0616, -0.0038,\n",
       "                      -0.0552, -0.0502,  0.0410,  0.0821,  0.0162, -0.0195, -0.0392, -0.0230,\n",
       "                      -0.0428,  0.0485,  0.0217,  0.0267, -0.0729, -0.0585, -0.0214,  0.0811,\n",
       "                       0.0226,  0.0359,  0.0255, -0.0500,  0.0618, -0.0795,  0.0763, -0.0505,\n",
       "                       0.0557,  0.0734,  0.0388,  0.0360,  0.0463,  0.0148, -0.0465,  0.0014,\n",
       "                      -0.0552,  0.0468,  0.0156, -0.0664, -0.0170, -0.0600, -0.0732, -0.0273,\n",
       "                       0.0032,  0.0181,  0.0445, -0.0674,  0.0617, -0.0107,  0.0535,  0.0665,\n",
       "                      -0.0534,  0.0034, -0.0114,  0.0607, -0.0072,  0.0126,  0.0541,  0.0604,\n",
       "                       0.0422,  0.0732,  0.0352, -0.0783, -0.0534, -0.0151, -0.0797, -0.0435,\n",
       "                      -0.0209, -0.0511, -0.0420,  0.0068, -0.0336,  0.0079, -0.0049, -0.0143,\n",
       "                       0.0759, -0.0137,  0.0278, -0.0790,  0.0337,  0.0684,  0.0121, -0.0138,\n",
       "                       0.0014, -0.0364,  0.0315,  0.0201,  0.0137,  0.0134,  0.0817, -0.0199,\n",
       "                       0.0701, -0.0432,  0.0728, -0.0259,  0.0824, -0.0551,  0.0663, -0.0339,\n",
       "                      -0.0714,  0.0579,  0.0569,  0.0009,  0.0279, -0.0788,  0.0618, -0.0434,\n",
       "                       0.0684,  0.0252, -0.0629, -0.0148,  0.0085,  0.0828, -0.0827, -0.0078,\n",
       "                      -0.0799, -0.0571,  0.0715,  0.0747,  0.0158,  0.0744, -0.0653,  0.0677,\n",
       "                       0.0627, -0.0005,  0.0120, -0.0238,  0.0470,  0.0088, -0.0428,  0.0485,\n",
       "                      -0.0290,  0.0816, -0.0332, -0.0028, -0.0057,  0.0149, -0.0478, -0.0112,\n",
       "                       0.0101, -0.0308,  0.0549, -0.0179,  0.0025,  0.0081, -0.0800, -0.0667,\n",
       "                      -0.0064,  0.0774, -0.0188,  0.0299,  0.0749, -0.0564,  0.0571, -0.0406,\n",
       "                       0.0250, -0.0385,  0.0407,  0.0366,  0.0544, -0.0066,  0.0332, -0.0059,\n",
       "                       0.0360, -0.0086, -0.0147, -0.0343,  0.0018,  0.0303, -0.0704,  0.0023,\n",
       "                      -0.0237, -0.0062,  0.0548, -0.0472, -0.0324, -0.0489,  0.0804,  0.0253,\n",
       "                      -0.0546, -0.0098, -0.0105, -0.0406, -0.0689,  0.0794,  0.0300,  0.0544,\n",
       "                       0.0707, -0.0118,  0.0341,  0.0185,  0.0601, -0.0504, -0.0607,  0.0306,\n",
       "                      -0.0756,  0.0432,  0.0600,  0.0201, -0.0770,  0.0125, -0.0701,  0.0299,\n",
       "                       0.0032, -0.0174,  0.0690,  0.0241, -0.0013, -0.0384,  0.0195, -0.0258,\n",
       "                       0.0765,  0.0191,  0.0730,  0.0181,  0.0422,  0.0719,  0.0650, -0.0438,\n",
       "                       0.0788, -0.0500,  0.0112,  0.0073, -0.0133,  0.0283,  0.0111,  0.0555,\n",
       "                       0.0446, -0.0060, -0.0455, -0.0769, -0.0806,  0.0553,  0.0630,  0.0537,\n",
       "                      -0.0400,  0.0362, -0.0105,  0.0833, -0.0283,  0.0547, -0.0372,  0.0720,\n",
       "                       0.0222, -0.0201,  0.0601, -0.0288, -0.0553, -0.0753, -0.0825,  0.0261,\n",
       "                      -0.0725, -0.0122, -0.0281,  0.0768,  0.0145,  0.0668, -0.0767, -0.0289,\n",
       "                      -0.0280, -0.0216,  0.0045,  0.0530, -0.0719, -0.0796, -0.0658, -0.0288,\n",
       "                      -0.0270,  0.0008, -0.0384,  0.0396,  0.0552, -0.0105,  0.0204, -0.0641])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.4.conv.weight',\n",
       "              tensor([[[ 0.0386, -0.0308,  0.1714,  ...,  0.1188, -0.0587, -0.0152]],\n",
       "              \n",
       "                      [[ 0.1314,  0.0935,  0.1303,  ...,  0.1383,  0.1647, -0.1594]],\n",
       "              \n",
       "                      [[-0.0333, -0.0176,  0.1775,  ..., -0.1419,  0.1673, -0.1251]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.1035,  0.1554, -0.0043,  ...,  0.1161, -0.1788, -0.0389]],\n",
       "              \n",
       "                      [[ 0.0325, -0.1223,  0.0816,  ...,  0.0666, -0.1576,  0.0583]],\n",
       "              \n",
       "                      [[ 0.0568, -0.0223, -0.1592,  ...,  0.1425, -0.0699, -0.1756]]])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.5.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.5.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.5.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.5.num_batches_tracked',\n",
       "              tensor(0)),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.7.conv.weight',\n",
       "              tensor([[[ 0.0667],\n",
       "                       [ 0.0361],\n",
       "                       [ 0.0276],\n",
       "                       ...,\n",
       "                       [-0.0060],\n",
       "                       [ 0.0716],\n",
       "                       [ 0.0756]],\n",
       "              \n",
       "                      [[ 0.0370],\n",
       "                       [-0.0256],\n",
       "                       [-0.0357],\n",
       "                       ...,\n",
       "                       [ 0.0205],\n",
       "                       [ 0.0408],\n",
       "                       [ 0.0182]],\n",
       "              \n",
       "                      [[ 0.0484],\n",
       "                       [ 0.0773],\n",
       "                       [-0.0282],\n",
       "                       ...,\n",
       "                       [-0.0242],\n",
       "                       [ 0.0003],\n",
       "                       [ 0.0411]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0492],\n",
       "                       [ 0.0749],\n",
       "                       [-0.0501],\n",
       "                       ...,\n",
       "                       [-0.0384],\n",
       "                       [-0.0158],\n",
       "                       [-0.0280]],\n",
       "              \n",
       "                      [[-0.0205],\n",
       "                       [ 0.0011],\n",
       "                       [ 0.0549],\n",
       "                       ...,\n",
       "                       [-0.0703],\n",
       "                       [-0.0224],\n",
       "                       [ 0.0455]],\n",
       "              \n",
       "                      [[ 0.0087],\n",
       "                       [-0.0644],\n",
       "                       [ 0.0806],\n",
       "                       ...,\n",
       "                       [-0.0345],\n",
       "                       [-0.0146],\n",
       "                       [ 0.0830]]])),\n",
       "             ('encoder.layers.3.sequential.2.module.sequential.7.conv.bias',\n",
       "              tensor([ 0.0233, -0.0423, -0.0564, -0.0321, -0.0298,  0.0330,  0.0214, -0.0519,\n",
       "                       0.0137,  0.0802, -0.0290, -0.0253,  0.0150,  0.0029,  0.0558,  0.0733,\n",
       "                      -0.0248,  0.0113,  0.0622, -0.0358, -0.0038, -0.0618, -0.0198, -0.0368,\n",
       "                       0.0540,  0.0458, -0.0712, -0.0683,  0.0666,  0.0789, -0.0787,  0.0545,\n",
       "                      -0.0085,  0.0136, -0.0797, -0.0186,  0.0113, -0.0741,  0.0274, -0.0254,\n",
       "                       0.0287,  0.0203,  0.0668, -0.0439, -0.0625, -0.0263, -0.0122,  0.0664,\n",
       "                      -0.0364, -0.0129, -0.0331, -0.0299, -0.0514,  0.0339,  0.0721,  0.0342,\n",
       "                       0.0769,  0.0186,  0.0726, -0.0234,  0.0829,  0.0457,  0.0775,  0.0640,\n",
       "                      -0.0395, -0.0393, -0.0246, -0.0019, -0.0756,  0.0189, -0.0808,  0.0671,\n",
       "                      -0.0460, -0.0466, -0.0284,  0.0302,  0.0205,  0.0194,  0.0551, -0.0730,\n",
       "                       0.0116,  0.0661,  0.0251, -0.0348,  0.0131, -0.0603,  0.0500,  0.0055,\n",
       "                      -0.0443,  0.0578,  0.0348, -0.0382,  0.0260, -0.0810,  0.0758, -0.0308,\n",
       "                       0.0697, -0.0322, -0.0200, -0.0526,  0.0118, -0.0641,  0.0600,  0.0759,\n",
       "                      -0.0284,  0.0241,  0.0411, -0.0304, -0.0263, -0.0482, -0.0307,  0.0202,\n",
       "                      -0.0066,  0.0442, -0.0743, -0.0081,  0.0500,  0.0053, -0.0514,  0.0212,\n",
       "                      -0.0037, -0.0289,  0.0377, -0.0065,  0.0261,  0.0221,  0.0374,  0.0616,\n",
       "                       0.0429, -0.0579,  0.0169,  0.0060, -0.0264,  0.0752, -0.0508,  0.0004,\n",
       "                       0.0317, -0.0182,  0.0266, -0.0109, -0.0020,  0.0114,  0.0427,  0.0711])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.0.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.0.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.1.linear.weight',\n",
       "              tensor([[-0.0212,  0.0409, -0.0049,  ..., -0.0376,  0.0639, -0.0337],\n",
       "                      [-0.0676,  0.0708, -0.0227,  ...,  0.0230,  0.0292, -0.0711],\n",
       "                      [ 0.0595,  0.0284,  0.0307,  ...,  0.0203,  0.0041, -0.0631],\n",
       "                      ...,\n",
       "                      [ 0.0035, -0.0297, -0.0239,  ...,  0.0735,  0.0252,  0.0508],\n",
       "                      [ 0.0183, -0.0218,  0.0786,  ...,  0.0786, -0.0274,  0.0170],\n",
       "                      [ 0.0167,  0.0269,  0.0352,  ..., -0.0745, -0.0489, -0.0540]])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.1.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.4.linear.weight',\n",
       "              tensor([[-8.1165e-02, -7.3334e-02, -6.8866e-02,  ...,  5.7675e-02,\n",
       "                        6.9516e-02,  8.2583e-02],\n",
       "                      [-6.2894e-02,  7.3574e-02,  6.1723e-02,  ...,  5.0764e-02,\n",
       "                        1.0739e-02, -4.8795e-02],\n",
       "                      [ 7.6397e-02, -6.3122e-02, -7.8183e-02,  ...,  6.4689e-02,\n",
       "                        6.4939e-02,  3.6701e-02],\n",
       "                      ...,\n",
       "                      [ 6.1699e-03, -7.3346e-03,  8.4325e-03,  ...,  4.9149e-02,\n",
       "                        8.3181e-02, -3.4621e-02],\n",
       "                      [ 4.1653e-02,  8.0266e-02, -6.7093e-02,  ..., -2.3057e-04,\n",
       "                        9.4883e-05, -5.4282e-02],\n",
       "                      [-2.3226e-02,  2.0231e-03, -1.0789e-02,  ..., -5.2033e-02,\n",
       "                        5.2885e-03, -3.9758e-02]])),\n",
       "             ('encoder.layers.3.sequential.3.module.sequential.4.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.layers.3.sequential.4.gamma',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.layers.3.sequential.4.beta',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.fc.0.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.fc.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.fc.0.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('encoder.fc.0.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('encoder.fc.0.num_batches_tracked', tensor(0)),\n",
       "             ('encoder.fc.3.linear.weight',\n",
       "              tensor([[ 7.8327e-02, -9.0355e-02, -1.1569e-01,  ..., -8.4289e-02,\n",
       "                       -3.4413e-03, -1.1500e-01],\n",
       "                      [-1.0568e-01,  9.0642e-02,  1.3962e-01,  ..., -2.8032e-02,\n",
       "                        1.0025e-01,  1.1990e-01],\n",
       "                      [-1.0295e-01, -8.1366e-02, -8.8946e-02,  ..., -2.4112e-02,\n",
       "                       -1.3648e-02, -6.2872e-02],\n",
       "                      ...,\n",
       "                      [ 6.4499e-02, -1.2093e-02, -1.6976e-02,  ...,  3.5716e-02,\n",
       "                        4.5969e-03, -1.3542e-01],\n",
       "                      [ 1.2769e-01, -2.5300e-02, -3.1983e-02,  ..., -5.3870e-03,\n",
       "                        4.8442e-02, -7.8092e-02],\n",
       "                      [-1.6297e-02,  6.2921e-05,  5.5404e-02,  ..., -2.4209e-02,\n",
       "                       -9.9881e-02, -1.2302e-03]])),\n",
       "             ('decoder.embedding.weight',\n",
       "              tensor([[ 0.2049,  1.3492, -0.4680,  ...,  0.6828, -0.0046,  1.3497],\n",
       "                      [-1.1603,  0.9307, -1.0488,  ...,  0.8341,  0.5021, -0.2632],\n",
       "                      [ 2.6394, -1.8015, -1.2742,  ..., -1.8764,  0.5959, -0.5925],\n",
       "                      ...,\n",
       "                      [-0.3542,  0.4788,  1.3446,  ...,  0.9095,  1.9369,  0.1211],\n",
       "                      [-0.9465, -0.7206, -0.4629,  ...,  0.1370, -1.1523,  0.4829],\n",
       "                      [-0.6916,  1.2433,  0.5777,  ..., -0.2032,  1.8217, -0.4052]])),\n",
       "             ('decoder.rnn.weight_ih_l0',\n",
       "              tensor([[ 0.0131,  0.0519, -0.0683,  ...,  0.0182,  0.0233,  0.0032],\n",
       "                      [-0.0542, -0.0122, -0.0349,  ..., -0.0168, -0.0016,  0.0269],\n",
       "                      [ 0.0391, -0.0412, -0.0741,  ...,  0.0333, -0.0413, -0.0551],\n",
       "                      ...,\n",
       "                      [-0.0520,  0.0283, -0.0009,  ..., -0.0479,  0.0366, -0.0210],\n",
       "                      [-0.0291,  0.0723, -0.0228,  ..., -0.0218, -0.0053,  0.0340],\n",
       "                      [-0.0372,  0.0243,  0.0660,  ..., -0.0238, -0.0684,  0.0298]])),\n",
       "             ('decoder.rnn.weight_hh_l0',\n",
       "              tensor([[ 0.0718, -0.0291, -0.0648,  ...,  0.0278, -0.0648, -0.0321],\n",
       "                      [ 0.0395, -0.0230,  0.0243,  ..., -0.0243, -0.0228, -0.0074],\n",
       "                      [ 0.0016, -0.0373, -0.0110,  ..., -0.0645, -0.0707, -0.0758],\n",
       "                      ...,\n",
       "                      [ 0.0423, -0.0178,  0.0766,  ...,  0.0565,  0.0374,  0.0561],\n",
       "                      [-0.0617,  0.0222,  0.0055,  ..., -0.0465,  0.0331, -0.0137],\n",
       "                      [-0.0438, -0.0248, -0.0577,  ...,  0.0211, -0.0355,  0.0511]])),\n",
       "             ('decoder.rnn.bias_ih_l0',\n",
       "              tensor([ 0.0664,  0.0012,  0.0004,  0.0254,  0.0511,  0.0287,  0.0684,  0.0515,\n",
       "                       0.0492,  0.0091,  0.0600,  0.0603,  0.0552, -0.0297,  0.0077, -0.0440,\n",
       "                       0.0347, -0.0032, -0.0386, -0.0665, -0.0234, -0.0065,  0.0555, -0.0172,\n",
       "                      -0.0104, -0.0134,  0.0153,  0.0318,  0.0261,  0.0370, -0.0675,  0.0329,\n",
       "                      -0.0771, -0.0216, -0.0010, -0.0609, -0.0816,  0.0162, -0.0149, -0.0169,\n",
       "                      -0.0522,  0.0064,  0.0523,  0.0831, -0.0128,  0.0107, -0.0235,  0.0194,\n",
       "                       0.0178, -0.0166, -0.0506,  0.0414,  0.0548,  0.0311, -0.0173,  0.0225,\n",
       "                       0.0212, -0.0671,  0.0085,  0.0667,  0.0083,  0.0142, -0.0526, -0.0721,\n",
       "                      -0.0311,  0.0287,  0.0554,  0.0386,  0.0572, -0.0113,  0.0799,  0.0381,\n",
       "                       0.0308,  0.0244, -0.0154,  0.0131, -0.0805,  0.0694,  0.0767, -0.0826,\n",
       "                       0.0611, -0.0415, -0.0310,  0.0033, -0.0578, -0.0372,  0.0698,  0.0467,\n",
       "                      -0.0358, -0.0532,  0.0306, -0.0503, -0.0573,  0.0159,  0.0365,  0.0821,\n",
       "                      -0.0334,  0.0194, -0.0180,  0.0414,  0.0713,  0.0652, -0.0715, -0.0674,\n",
       "                      -0.0525, -0.0003, -0.0032,  0.0287,  0.0203, -0.0384,  0.0443,  0.0489,\n",
       "                       0.0200,  0.0542,  0.0110,  0.0319, -0.0473,  0.0703,  0.0478, -0.0661,\n",
       "                       0.0035, -0.0537, -0.0265,  0.0619, -0.0185, -0.0180,  0.0295, -0.0797,\n",
       "                       0.0238,  0.0294, -0.0562,  0.0516,  0.0169, -0.0114, -0.0706, -0.0817,\n",
       "                       0.0188, -0.0196,  0.0678, -0.0047, -0.0801, -0.0086, -0.0775, -0.0733,\n",
       "                       0.0290,  0.0632,  0.0229,  0.0368,  0.0512, -0.0416,  0.0483, -0.0561,\n",
       "                       0.0731,  0.0357, -0.0833,  0.0371,  0.0430, -0.0794, -0.0403,  0.0270,\n",
       "                       0.0335,  0.0100,  0.0637,  0.0346,  0.0668,  0.0013,  0.0699, -0.0396,\n",
       "                      -0.0804,  0.0351,  0.0317,  0.0079,  0.0401,  0.0597, -0.0525,  0.0471,\n",
       "                       0.0501,  0.0335,  0.0548, -0.0730, -0.0415, -0.0230, -0.0702, -0.0266,\n",
       "                      -0.0590,  0.0177, -0.0285, -0.0445,  0.0157, -0.0606,  0.0318,  0.0685,\n",
       "                       0.0119,  0.0388,  0.0224, -0.0586, -0.0625, -0.0651, -0.0068, -0.0310,\n",
       "                       0.0571, -0.0299,  0.0190, -0.0430,  0.0564, -0.0126,  0.0255,  0.0704,\n",
       "                       0.0588, -0.0249,  0.0587,  0.0285,  0.0451,  0.0141, -0.0660,  0.0137,\n",
       "                       0.0559, -0.0235, -0.0349,  0.0756, -0.0238,  0.0554,  0.0195, -0.0478,\n",
       "                      -0.0036, -0.0217, -0.0357,  0.0732, -0.0307, -0.0749,  0.0283,  0.0205,\n",
       "                       0.0443, -0.0653, -0.0299, -0.0599, -0.0265,  0.0079,  0.0441,  0.0328,\n",
       "                       0.0640,  0.0304, -0.0774, -0.0386,  0.0530,  0.0801, -0.0232,  0.0396,\n",
       "                      -0.0508, -0.0127,  0.0569, -0.0242, -0.0817,  0.0532, -0.0742, -0.0809,\n",
       "                      -0.0310, -0.0713, -0.0172,  0.0394,  0.0607, -0.0761,  0.0462,  0.0167,\n",
       "                      -0.0789,  0.0164,  0.0413, -0.0122,  0.0576,  0.0509, -0.0103, -0.0100,\n",
       "                       0.0087,  0.0116, -0.0714,  0.0103, -0.0731,  0.0540, -0.0031, -0.0729,\n",
       "                       0.0345, -0.0301,  0.0072,  0.0591, -0.0235, -0.0333,  0.0815, -0.0372,\n",
       "                      -0.0280,  0.0163,  0.0338,  0.0134, -0.0201,  0.0439, -0.0674, -0.0806,\n",
       "                       0.0460,  0.0135,  0.0206, -0.0709,  0.0281,  0.0757,  0.0487,  0.0798,\n",
       "                      -0.0407,  0.0414, -0.0761,  0.0629, -0.0251, -0.0151, -0.0265,  0.0524,\n",
       "                       0.0154,  0.0560, -0.0649,  0.0722, -0.0821,  0.0175, -0.0350, -0.0673,\n",
       "                      -0.0498, -0.0455, -0.0647, -0.0495,  0.0652,  0.0275, -0.0084,  0.0006,\n",
       "                       0.0325, -0.0616, -0.0433,  0.0480, -0.0642,  0.0790,  0.0704, -0.0451,\n",
       "                       0.0157, -0.0212, -0.0273,  0.0815,  0.0441,  0.0716,  0.0451, -0.0638,\n",
       "                       0.0616,  0.0045,  0.0659,  0.0166,  0.0106, -0.0798, -0.0534, -0.0548,\n",
       "                       0.0168, -0.0126,  0.0090,  0.0473,  0.0496, -0.0259, -0.0316,  0.0516,\n",
       "                       0.0639,  0.0180,  0.0406, -0.0187,  0.0545,  0.0350, -0.0330, -0.0770,\n",
       "                       0.0484,  0.0785, -0.0443, -0.0296,  0.0243, -0.0745, -0.0201, -0.0231,\n",
       "                       0.0545,  0.0783,  0.0710, -0.0498,  0.0508, -0.0169,  0.0193, -0.0095,\n",
       "                      -0.0221,  0.0454,  0.0162, -0.0501, -0.0247,  0.0450, -0.0662,  0.0604,\n",
       "                       0.0826,  0.0124, -0.0521, -0.0619,  0.0819, -0.0337, -0.0360, -0.0641,\n",
       "                       0.0243, -0.0441,  0.0021,  0.0016,  0.0554, -0.0206,  0.0069,  0.0304,\n",
       "                      -0.0633,  0.0544,  0.0526,  0.0436, -0.0613,  0.0225, -0.0613,  0.0210,\n",
       "                       0.0574, -0.0691,  0.0470,  0.0790,  0.0482, -0.0799,  0.0363,  0.0626,\n",
       "                      -0.0082,  0.0455, -0.0179, -0.0617, -0.0784,  0.0144, -0.0109, -0.0180])),\n",
       "             ('decoder.rnn.bias_hh_l0',\n",
       "              tensor([ 4.3410e-02, -5.2246e-02, -1.2452e-02, -5.0510e-02,  4.9313e-02,\n",
       "                      -5.8733e-02, -5.4979e-03, -4.4892e-02,  8.0981e-02, -4.7481e-02,\n",
       "                      -5.8302e-02, -1.5390e-02, -4.3444e-04,  1.5654e-02,  3.6505e-04,\n",
       "                      -4.3827e-02,  2.8183e-02, -6.2694e-02, -1.5749e-02, -2.2637e-03,\n",
       "                      -4.9845e-02, -3.9416e-02, -8.0816e-02, -6.5228e-02,  7.8475e-02,\n",
       "                      -6.2875e-02,  1.6903e-04, -6.7130e-02,  1.2713e-02,  3.9734e-03,\n",
       "                      -5.8353e-02,  4.7698e-02,  6.5336e-02,  7.4721e-02,  5.2141e-02,\n",
       "                      -2.0922e-02,  2.4623e-02, -9.1662e-03, -1.7260e-02,  4.7518e-02,\n",
       "                       5.5844e-02,  1.7700e-02, -3.9399e-03,  3.3512e-02,  3.8932e-02,\n",
       "                      -6.9824e-02, -1.2934e-02, -7.6302e-02,  5.4955e-02, -5.9471e-03,\n",
       "                       3.7178e-02,  8.2152e-02,  9.6307e-04,  1.7628e-02,  1.2184e-02,\n",
       "                      -7.0574e-02, -8.1079e-02,  5.0268e-02, -5.6835e-02,  4.4919e-02,\n",
       "                       6.5127e-02,  2.5990e-02, -8.3219e-02, -7.7947e-02,  6.1174e-02,\n",
       "                      -7.5580e-02, -5.0028e-02, -7.6284e-02, -6.2213e-02, -6.4893e-03,\n",
       "                      -2.6578e-02, -6.2894e-02, -2.5368e-02, -3.6508e-03,  7.2637e-02,\n",
       "                       7.8894e-02, -2.7063e-02, -7.7097e-02,  7.3258e-02,  2.4238e-02,\n",
       "                       5.9684e-03,  3.6810e-02,  6.4750e-02, -5.1371e-02,  1.3071e-02,\n",
       "                       3.3219e-02,  7.4959e-02, -6.9050e-02,  1.5990e-02, -7.1385e-02,\n",
       "                       1.1538e-02, -6.5777e-03, -1.7216e-02, -6.0002e-02, -2.9755e-02,\n",
       "                       8.7616e-03, -5.1049e-02,  3.1595e-02,  8.7928e-03, -7.2727e-02,\n",
       "                      -7.4837e-02, -3.4398e-02, -2.7251e-02, -6.2464e-02,  8.2440e-02,\n",
       "                       2.9485e-02, -3.6243e-02,  3.9557e-02, -1.2940e-02, -5.1940e-02,\n",
       "                       7.5968e-03, -8.4861e-03,  6.1515e-02,  4.8467e-02, -6.8657e-02,\n",
       "                       7.6375e-02, -3.4004e-03,  2.5361e-02, -8.2804e-02,  4.9167e-02,\n",
       "                       6.3791e-02, -2.8619e-02,  8.0134e-02, -1.7569e-03,  7.0650e-02,\n",
       "                      -7.9443e-02, -4.6847e-03,  4.3737e-02,  6.9018e-02,  2.2484e-02,\n",
       "                       1.7251e-02,  3.9807e-02,  4.1005e-02,  3.3780e-03,  2.7713e-02,\n",
       "                      -4.6516e-02, -4.0130e-04, -2.8417e-02, -1.1874e-02,  6.5455e-02,\n",
       "                       5.9515e-02,  1.2577e-05,  7.4807e-02, -1.8465e-02,  4.1285e-03,\n",
       "                       3.9480e-02,  7.0363e-03, -4.6395e-02, -6.1581e-02, -4.6642e-02,\n",
       "                       7.5037e-02, -8.3241e-02, -7.4782e-02,  3.5002e-02, -7.1640e-03,\n",
       "                       4.3822e-02,  6.4771e-02,  3.5485e-02, -7.3478e-02,  8.1022e-02,\n",
       "                      -1.6438e-02,  6.4731e-02,  3.3442e-03,  1.4505e-02,  8.9712e-03,\n",
       "                      -6.2306e-02,  2.6690e-03, -5.7327e-02,  7.3017e-02, -4.3773e-03,\n",
       "                      -7.3093e-02,  3.8682e-02,  7.2283e-02,  3.8464e-03, -7.4431e-02,\n",
       "                       1.7746e-02, -5.1927e-02,  4.3951e-02,  6.9755e-02, -4.3995e-03,\n",
       "                      -4.2204e-03, -3.4083e-02,  8.3342e-03,  2.9487e-02,  8.5293e-04,\n",
       "                      -4.6459e-02,  2.9475e-02,  4.6060e-02,  6.2000e-02,  6.9094e-02,\n",
       "                      -5.4743e-02,  2.1287e-02, -1.7428e-02,  6.6129e-02,  5.1523e-02,\n",
       "                      -4.8190e-02, -4.5070e-02,  1.1741e-02, -7.3478e-02,  4.2976e-02,\n",
       "                       5.0796e-02,  2.2698e-04,  4.9482e-02, -4.7308e-02, -8.1726e-02,\n",
       "                      -7.0748e-02,  3.3925e-02,  2.4425e-02,  5.3011e-02,  4.2660e-02,\n",
       "                       2.5794e-02,  6.8875e-02,  2.2119e-02,  3.7880e-02,  6.9268e-02,\n",
       "                       3.3198e-02, -4.0725e-02, -8.0558e-02,  7.5899e-02, -6.6941e-02,\n",
       "                       2.7315e-02,  5.5157e-02,  7.4460e-02,  3.5707e-02,  3.8193e-02,\n",
       "                       1.3045e-02, -4.9061e-02,  5.8777e-02, -7.8334e-03, -3.8893e-02,\n",
       "                       7.4288e-03, -4.2218e-02, -5.8762e-02, -4.9543e-02, -1.2091e-02,\n",
       "                      -1.1963e-02, -5.3079e-02, -2.2157e-02,  2.0931e-02,  3.7977e-02,\n",
       "                      -6.5855e-02,  3.8755e-02,  2.0619e-02,  6.7237e-02, -2.6291e-02,\n",
       "                      -3.2874e-02,  7.7848e-02,  8.9290e-03, -6.3752e-03,  3.0405e-02,\n",
       "                       2.4592e-02,  3.1441e-02, -6.4161e-02, -4.7459e-03, -6.3841e-02,\n",
       "                      -3.0949e-03, -9.5094e-03, -4.9007e-02,  3.2970e-02, -2.5699e-02,\n",
       "                      -1.3973e-02,  1.6301e-02, -4.9760e-02,  7.2563e-02,  6.4978e-02,\n",
       "                       7.4737e-03, -6.3038e-02,  1.7186e-02, -8.3951e-03,  3.3860e-02,\n",
       "                       2.0053e-02, -4.9319e-02, -2.9429e-03,  5.2435e-02,  5.0274e-02,\n",
       "                      -7.6827e-02,  1.0403e-02,  7.0876e-02, -7.3829e-02, -3.2172e-03,\n",
       "                      -5.1635e-02,  8.0211e-03, -9.8756e-03, -4.7664e-02,  6.3129e-02,\n",
       "                       6.5809e-02, -3.5754e-02, -4.9904e-02,  6.7901e-02, -3.5671e-02,\n",
       "                      -3.2315e-02, -6.5828e-02, -5.3757e-03,  1.1588e-02,  1.4938e-02,\n",
       "                      -1.8061e-02, -8.2639e-02,  7.4544e-02,  8.6759e-03,  3.2971e-02,\n",
       "                       3.5680e-02,  6.2429e-02, -4.3408e-02, -6.8452e-02,  1.5682e-02,\n",
       "                      -8.1778e-02,  7.0398e-02,  6.6053e-02, -3.5751e-02,  5.6183e-02,\n",
       "                      -5.5400e-02, -7.6461e-02,  6.9630e-02,  7.9492e-02,  4.3622e-02,\n",
       "                      -5.8877e-02, -5.1467e-02, -3.8753e-02,  6.0549e-03, -3.9544e-02,\n",
       "                      -3.6964e-02,  2.9602e-02, -5.9007e-02, -5.1043e-02,  7.5998e-03,\n",
       "                       8.2265e-02,  6.3536e-02, -2.4533e-02, -7.9808e-04, -6.3853e-02,\n",
       "                       1.4883e-02, -6.0794e-03, -1.2580e-02, -7.6525e-02,  6.8740e-02,\n",
       "                       8.1058e-02,  2.8832e-03,  6.5058e-02,  1.7726e-02, -4.8056e-02,\n",
       "                       4.4014e-02,  5.3410e-02,  7.9135e-02, -4.7517e-03,  6.8149e-03,\n",
       "                       5.3943e-02,  7.5053e-02, -3.8841e-02,  5.3104e-02, -6.4790e-02,\n",
       "                       5.6215e-02, -7.5748e-02, -3.9347e-03,  5.0912e-03, -6.1788e-02,\n",
       "                      -5.3024e-02, -3.0178e-02, -8.0692e-02,  3.1383e-02,  2.4145e-02,\n",
       "                       4.4040e-02,  4.7617e-03,  3.3623e-02,  4.5604e-02,  2.7200e-02,\n",
       "                      -7.4443e-02,  4.9091e-02, -6.7492e-02,  2.2038e-02,  7.5484e-02,\n",
       "                       2.4904e-02,  5.3919e-02, -4.3166e-02,  1.7965e-02,  5.8955e-02,\n",
       "                       5.0222e-02, -7.5273e-02,  1.1885e-02,  2.6514e-05, -2.8233e-02,\n",
       "                      -6.5526e-03,  8.2298e-02,  1.7732e-02, -7.6231e-02,  1.8323e-02,\n",
       "                      -1.3401e-02, -5.2485e-02, -4.9268e-02, -6.5094e-02, -5.0529e-02,\n",
       "                      -6.9059e-02, -4.8964e-02,  1.0664e-02,  7.8514e-04,  2.6835e-02,\n",
       "                      -5.2417e-03, -4.6424e-02,  4.8477e-02,  6.3149e-02, -1.6350e-02,\n",
       "                       2.5994e-02, -4.7264e-02, -2.5862e-02, -5.5241e-02, -3.1483e-02,\n",
       "                      -1.9805e-02,  4.1666e-02,  8.2349e-02, -1.9505e-02,  5.8263e-02,\n",
       "                       1.0983e-03,  7.1592e-03, -6.1887e-02, -1.5032e-02, -3.0667e-02,\n",
       "                       1.2299e-02,  6.2245e-02, -7.2581e-02,  5.5806e-02,  5.0282e-02,\n",
       "                       7.3034e-02, -3.5822e-02, -3.0761e-02,  8.2393e-02, -7.6766e-03,\n",
       "                      -3.8812e-02,  4.2486e-02, -4.4028e-02,  6.0722e-02,  6.1671e-02,\n",
       "                       7.0607e-02, -3.3611e-02])),\n",
       "             ('decoder.attention.query_proj.linear.weight',\n",
       "              tensor([[-0.0658,  0.0538,  0.0031,  ...,  0.0529, -0.0323,  0.0803],\n",
       "                      [ 0.0098, -0.1313,  0.0019,  ...,  0.1389,  0.1375,  0.0372],\n",
       "                      [ 0.1382, -0.0559,  0.1399,  ..., -0.1325,  0.1154,  0.1185],\n",
       "                      ...,\n",
       "                      [ 0.1379,  0.1399,  0.1072,  ..., -0.0904,  0.1119,  0.0838],\n",
       "                      [-0.0934,  0.1294, -0.0011,  ...,  0.0026, -0.0364,  0.0127],\n",
       "                      [ 0.1358, -0.0450, -0.1138,  ..., -0.1248, -0.1381, -0.1118]])),\n",
       "             ('decoder.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.attention.key_proj.linear.weight',\n",
       "              tensor([[-0.1106,  0.0313, -0.0142,  ...,  0.0213,  0.1115, -0.0766],\n",
       "                      [-0.0160, -0.0988, -0.1149,  ...,  0.0460,  0.1052,  0.0415],\n",
       "                      [-0.0324,  0.1386, -0.1162,  ..., -0.0786,  0.0029, -0.1111],\n",
       "                      ...,\n",
       "                      [ 0.1170, -0.1384, -0.0945,  ...,  0.1160, -0.0696, -0.0235],\n",
       "                      [ 0.0403,  0.0115, -0.0973,  ...,  0.0631,  0.0891,  0.0722],\n",
       "                      [ 0.1021, -0.0334,  0.0708,  ..., -0.0290,  0.1085,  0.0391]])),\n",
       "             ('decoder.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.attention.value_proj.linear.weight',\n",
       "              tensor([[-0.0771,  0.0138, -0.1114,  ...,  0.0280, -0.1119, -0.0025],\n",
       "                      [ 0.1188, -0.1331,  0.0661,  ...,  0.0008,  0.0891, -0.0385],\n",
       "                      [-0.0597, -0.0838, -0.0573,  ...,  0.0533, -0.0409,  0.1067],\n",
       "                      ...,\n",
       "                      [ 0.1149,  0.0032, -0.0729,  ...,  0.0371, -0.0315, -0.1306],\n",
       "                      [ 0.0003,  0.0986, -0.0209,  ..., -0.1371, -0.1142,  0.1070],\n",
       "                      [-0.1400,  0.1023,  0.1318,  ...,  0.0961, -0.1269,  0.1436]])),\n",
       "             ('decoder.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.fc.0.weight',\n",
       "              tensor([[ 0.0432, -0.0435,  0.0200,  ..., -0.0462, -0.0007,  0.0048],\n",
       "                      [ 0.0152, -0.0231,  0.0400,  ..., -0.0102,  0.0223,  0.0362],\n",
       "                      [ 0.0032,  0.0384,  0.0073,  ..., -0.0353, -0.0181,  0.0197],\n",
       "                      ...,\n",
       "                      [-0.0483,  0.0160,  0.0510,  ...,  0.0076,  0.0324,  0.0546],\n",
       "                      [ 0.0203, -0.0151,  0.0170,  ..., -0.0248,  0.0136, -0.0293],\n",
       "                      [ 0.0528, -0.0537, -0.0462,  ..., -0.0462,  0.0328,  0.0241]])),\n",
       "             ('decoder.fc.0.bias',\n",
       "              tensor([-4.6357e-03, -3.4677e-02, -4.7752e-02,  2.3575e-02, -5.3536e-03,\n",
       "                      -4.3462e-02, -3.8303e-02,  3.2580e-02,  4.2759e-02,  5.7163e-02,\n",
       "                      -5.7635e-02, -1.5600e-03, -4.5288e-02,  7.1941e-03, -5.1848e-02,\n",
       "                       3.6586e-02, -3.5138e-02,  4.4849e-02, -4.9084e-02,  3.4180e-02,\n",
       "                      -5.4413e-02, -5.7798e-03,  9.0359e-03,  5.5346e-02, -2.0345e-02,\n",
       "                       1.6980e-03,  4.4317e-02, -1.6636e-02,  3.4916e-02, -4.0473e-02,\n",
       "                       4.8353e-02,  5.5499e-02,  3.3347e-02,  3.1383e-02, -5.2967e-02,\n",
       "                       5.3475e-02,  3.3629e-02,  1.7193e-03,  4.3714e-03, -5.2263e-02,\n",
       "                      -2.1235e-02, -3.7362e-02,  4.0510e-02, -3.0925e-02, -5.6977e-02,\n",
       "                       4.4674e-02,  3.6592e-02, -1.3801e-02, -2.1730e-02, -3.3971e-02,\n",
       "                       6.3509e-03,  5.8489e-02,  2.3413e-02,  4.3734e-02,  4.1226e-02,\n",
       "                       4.5247e-02,  5.3659e-02,  1.4534e-02,  4.5895e-02, -4.6763e-02,\n",
       "                       1.2137e-02,  4.9317e-02,  8.4548e-03,  3.1948e-02, -5.6447e-02,\n",
       "                       4.2764e-02,  9.4630e-04,  2.0819e-02,  1.6733e-02,  4.3551e-02,\n",
       "                       2.6484e-02,  4.9530e-02,  5.5258e-02, -5.5421e-02, -1.2336e-04,\n",
       "                       4.1690e-02, -1.8212e-02,  1.7312e-02,  3.0509e-02, -5.3861e-02,\n",
       "                      -1.0192e-02, -4.7598e-02, -1.3528e-02,  4.4573e-02,  7.5172e-03,\n",
       "                      -5.4219e-02,  3.2393e-02, -3.8865e-02, -1.9301e-02, -1.8604e-02,\n",
       "                       5.7010e-03,  1.9214e-02, -1.5228e-02,  4.5669e-02,  2.4501e-02,\n",
       "                       9.1808e-03,  5.4193e-02,  5.3011e-03,  3.4960e-02, -4.2978e-02,\n",
       "                       1.2611e-02,  4.2442e-02,  3.9958e-02, -5.0273e-02,  3.1151e-02,\n",
       "                       4.7577e-02, -3.9424e-02, -3.3991e-03, -2.8804e-02,  5.8798e-02,\n",
       "                       5.5473e-02, -2.3575e-02,  5.4672e-02,  3.8587e-02,  5.4343e-02,\n",
       "                      -4.0034e-03, -1.7748e-02, -4.1026e-02,  1.0294e-02, -8.3365e-03,\n",
       "                       2.3376e-02, -2.4475e-02,  1.9489e-02,  4.8698e-02, -4.6062e-02,\n",
       "                      -1.7406e-02, -4.0960e-05, -4.3240e-02, -5.5687e-02,  4.5189e-02,\n",
       "                      -3.4237e-02,  1.3422e-02, -4.6798e-02, -5.4332e-02, -3.9683e-02,\n",
       "                      -2.1828e-02, -5.1311e-02, -5.2057e-02, -5.7555e-02, -1.6899e-02,\n",
       "                       4.4879e-02,  6.1010e-03, -1.9785e-02, -2.7344e-02])),\n",
       "             ('decoder.fc.3.weight',\n",
       "              tensor([[ 0.0570,  0.0680,  0.0070,  ..., -0.0184,  0.0018, -0.0741],\n",
       "                      [-0.0583,  0.0698,  0.0690,  ..., -0.0666, -0.0321, -0.0286],\n",
       "                      [ 0.0492,  0.0266,  0.0242,  ..., -0.0120,  0.0176,  0.0688],\n",
       "                      ...,\n",
       "                      [ 0.0364,  0.0793, -0.0624,  ...,  0.0534,  0.0493,  0.0563],\n",
       "                      [-0.0736, -0.0048,  0.0551,  ...,  0.0375,  0.0582,  0.0007],\n",
       "                      [-0.0498, -0.0200,  0.0287,  ...,  0.0078,  0.0226,  0.0343]])),\n",
       "             ('decoder.fc.3.bias',\n",
       "              tensor([-0.0263,  0.0595,  0.0317,  0.0739,  0.0119,  0.0171, -0.0550, -0.0640,\n",
       "                      -0.0494,  0.0501,  0.0060,  0.0663,  0.0657, -0.0069, -0.0557,  0.0175,\n",
       "                      -0.0377,  0.0226, -0.0747, -0.0192, -0.0379, -0.0447, -0.0498, -0.0763,\n",
       "                       0.0612, -0.0304, -0.0788,  0.0202, -0.0438,  0.0373, -0.0079, -0.0392,\n",
       "                      -0.0070, -0.0485,  0.0226, -0.0323,  0.0281, -0.0823, -0.0688, -0.0525,\n",
       "                      -0.0043, -0.0186,  0.0087,  0.0069,  0.0163,  0.0617, -0.0198,  0.0723,\n",
       "                       0.0096, -0.0735, -0.0611,  0.0049,  0.0108, -0.0831,  0.0282,  0.0424,\n",
       "                       0.0502,  0.0356,  0.0566,  0.0701, -0.0419, -0.0332,  0.0375, -0.0696,\n",
       "                       0.0688,  0.0634,  0.0746,  0.0638, -0.0370,  0.0282,  0.0383, -0.0817,\n",
       "                       0.0081,  0.0568,  0.0249, -0.0491, -0.0504,  0.0202, -0.0302,  0.0376,\n",
       "                       0.0083, -0.0044, -0.0142,  0.0328,  0.0208,  0.0401, -0.0468, -0.0465,\n",
       "                       0.0311, -0.0162, -0.0159, -0.0079,  0.0166,  0.0392, -0.0027, -0.0791,\n",
       "                      -0.0473,  0.0434, -0.0441, -0.0160,  0.0372,  0.0272, -0.0362, -0.0788,\n",
       "                       0.0210, -0.0815, -0.0204,  0.0700, -0.0706,  0.0109,  0.0580, -0.0581,\n",
       "                      -0.0439, -0.0813, -0.0296,  0.0352,  0.0517, -0.0607, -0.0219,  0.0558,\n",
       "                      -0.0610,  0.0832,  0.0678,  0.0781,  0.0237,  0.0512, -0.0348, -0.0599,\n",
       "                       0.0250, -0.0546,  0.0567,  0.0473, -0.0239,  0.0123, -0.0487, -0.0730,\n",
       "                      -0.0212,  0.0386,  0.0274,  0.0730,  0.0184,  0.0167,  0.0125,  0.0719,\n",
       "                       0.0243,  0.0155, -0.0687, -0.0165, -0.0392, -0.0186, -0.0638, -0.0022,\n",
       "                       0.0015,  0.0488,  0.0564])),\n",
       "             ('word_decoder.embedding.weight',\n",
       "              tensor([[-2.0321e+00, -1.4777e+00,  2.0667e+00,  ...,  1.1315e+00,\n",
       "                        5.8852e-01, -1.8351e-01],\n",
       "                      [-2.5310e+00,  3.1841e-01,  4.9048e-01,  ..., -1.6756e-03,\n",
       "                       -8.8429e-01, -1.4258e+00],\n",
       "                      [ 2.8851e-01, -1.6594e-01, -2.2028e+00,  ..., -1.2382e+00,\n",
       "                        3.8790e-01, -1.4833e+00],\n",
       "                      ...,\n",
       "                      [ 2.1546e-01,  1.7356e+00, -2.1240e-01,  ..., -1.0949e-02,\n",
       "                        6.7297e-01, -2.9762e+00],\n",
       "                      [-1.3758e-01,  5.3706e-01, -1.4208e-01,  ...,  7.9714e-01,\n",
       "                        1.8580e+00,  4.9381e-01],\n",
       "                      [ 1.4678e-01, -1.8021e-01,  1.4630e+00,  ..., -1.4145e+00,\n",
       "                        1.2209e+00,  1.2609e+00]])),\n",
       "             ('word_decoder.attention.query_proj.linear.weight',\n",
       "              tensor([[-0.1185, -0.0176,  0.0586,  ...,  0.0893, -0.0118,  0.1293],\n",
       "                      [-0.1003, -0.0903,  0.1340,  ...,  0.1083, -0.0230,  0.0017],\n",
       "                      [-0.0221, -0.1236, -0.0319,  ...,  0.0441, -0.0417,  0.0277],\n",
       "                      ...,\n",
       "                      [ 0.0645, -0.1189,  0.0088,  ..., -0.0728, -0.1340, -0.1367],\n",
       "                      [ 0.1088,  0.0588, -0.0788,  ..., -0.0032, -0.0108, -0.1354],\n",
       "                      [-0.1414, -0.1095,  0.0846,  ...,  0.1255,  0.1233, -0.1151]])),\n",
       "             ('word_decoder.attention.query_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('word_decoder.attention.key_proj.linear.weight',\n",
       "              tensor([[ 0.0458,  0.0068,  0.0020,  ..., -0.0276,  0.1264, -0.0204],\n",
       "                      [ 0.1162,  0.0608, -0.0374,  ..., -0.0125,  0.0769,  0.0597],\n",
       "                      [-0.0341, -0.1246, -0.1247,  ...,  0.0043, -0.0643,  0.0380],\n",
       "                      ...,\n",
       "                      [-0.0463,  0.0624,  0.1138,  ...,  0.1336, -0.0318,  0.0383],\n",
       "                      [-0.1311,  0.1209, -0.1198,  ..., -0.1360, -0.1254, -0.0728],\n",
       "                      [-0.0877, -0.0213,  0.0898,  ..., -0.1012, -0.0035, -0.0417]])),\n",
       "             ('word_decoder.attention.key_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('word_decoder.attention.value_proj.linear.weight',\n",
       "              tensor([[ 0.0027,  0.0711,  0.1177,  ..., -0.0326,  0.0465,  0.0779],\n",
       "                      [-0.0191,  0.0585,  0.0186,  ..., -0.1394, -0.0990, -0.0829],\n",
       "                      [-0.1298,  0.0880,  0.1093,  ...,  0.1209, -0.1265, -0.0722],\n",
       "                      ...,\n",
       "                      [ 0.0383,  0.0753, -0.0941,  ..., -0.0657, -0.1234, -0.0908],\n",
       "                      [ 0.1443,  0.0421, -0.0652,  ..., -0.0992,  0.0271,  0.0901],\n",
       "                      [ 0.0421,  0.0582, -0.0880,  ...,  0.0803,  0.0491, -0.0555]])),\n",
       "             ('word_decoder.attention.value_proj.linear.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('word_decoder.ff.weight',\n",
       "              tensor([[ 0.0231, -0.0568,  0.0498,  ..., -0.0392,  0.0067,  0.0091],\n",
       "                      [-0.0099, -0.0503, -0.0367,  ..., -0.0532,  0.0529,  0.0382],\n",
       "                      [ 0.0237, -0.0391,  0.0316,  ...,  0.0242,  0.0475, -0.0106],\n",
       "                      ...,\n",
       "                      [ 0.0176, -0.0577, -0.0147,  ...,  0.0208, -0.0493, -0.0023],\n",
       "                      [-0.0135,  0.0536, -0.0281,  ...,  0.0054,  0.0038, -0.0040],\n",
       "                      [ 0.0226,  0.0214, -0.0129,  ...,  0.0189, -0.0255,  0.0120]])),\n",
       "             ('word_decoder.ff.bias',\n",
       "              tensor([-0.0235, -0.0297, -0.0427,  0.0291, -0.0143,  0.0424, -0.0346,  0.0211,\n",
       "                      -0.0155, -0.0538, -0.0478, -0.0470, -0.0004,  0.0274, -0.0540,  0.0328,\n",
       "                      -0.0284, -0.0211, -0.0283, -0.0564, -0.0180,  0.0282,  0.0216, -0.0553,\n",
       "                       0.0032,  0.0290,  0.0111,  0.0354, -0.0487, -0.0295, -0.0318,  0.0277,\n",
       "                      -0.0203, -0.0464,  0.0476, -0.0421, -0.0105, -0.0572,  0.0245, -0.0072,\n",
       "                       0.0384,  0.0566, -0.0458, -0.0397, -0.0271,  0.0120, -0.0029, -0.0336,\n",
       "                      -0.0100,  0.0074, -0.0102,  0.0122, -0.0221, -0.0427, -0.0413,  0.0049,\n",
       "                       0.0305,  0.0576, -0.0271,  0.0567,  0.0258, -0.0442, -0.0148, -0.0333,\n",
       "                       0.0532, -0.0025,  0.0467,  0.0394,  0.0464, -0.0177, -0.0322, -0.0537,\n",
       "                      -0.0360, -0.0436,  0.0263,  0.0028, -0.0469,  0.0078, -0.0552, -0.0275,\n",
       "                       0.0137,  0.0255, -0.0527,  0.0165,  0.0193,  0.0212, -0.0379,  0.0345,\n",
       "                       0.0241,  0.0096,  0.0054, -0.0571,  0.0401, -0.0146, -0.0111, -0.0119,\n",
       "                      -0.0449,  0.0511, -0.0293, -0.0260, -0.0294, -0.0116,  0.0519,  0.0384,\n",
       "                      -0.0169,  0.0292, -0.0332,  0.0378, -0.0060,  0.0513,  0.0037, -0.0085,\n",
       "                      -0.0247,  0.0334,  0.0520, -0.0238, -0.0298, -0.0206,  0.0200,  0.0505,\n",
       "                      -0.0490, -0.0543,  0.0391, -0.0351, -0.0103, -0.0036, -0.0255, -0.0489,\n",
       "                       0.0348,  0.0035, -0.0507, -0.0527,  0.0061,  0.0221,  0.0133,  0.0455,\n",
       "                      -0.0157,  0.0003, -0.0365, -0.0220,  0.0584,  0.0496, -0.0446, -0.0133])),\n",
       "             ('word_decoder.fc.0.weight',\n",
       "              tensor([[-0.0323, -0.0025,  0.0461,  ...,  0.0248,  0.0228, -0.0040],\n",
       "                      [ 0.0132,  0.0582,  0.0101,  ...,  0.0034, -0.0102,  0.0191],\n",
       "                      [-0.0336,  0.0078,  0.0273,  ...,  0.0501,  0.0269, -0.0495],\n",
       "                      ...,\n",
       "                      [ 0.0383,  0.0150, -0.0356,  ...,  0.0260, -0.0566,  0.0421],\n",
       "                      [ 0.0585, -0.0533,  0.0457,  ..., -0.0083, -0.0341,  0.0306],\n",
       "                      [ 0.0336,  0.0482,  0.0337,  ...,  0.0160, -0.0418, -0.0091]])),\n",
       "             ('word_decoder.fc.0.bias',\n",
       "              tensor([-0.0373, -0.0326, -0.0228,  0.0330,  0.0157,  0.0227,  0.0548,  0.0039,\n",
       "                      -0.0343, -0.0063,  0.0441, -0.0056,  0.0287,  0.0501,  0.0187, -0.0342,\n",
       "                      -0.0468, -0.0580,  0.0562,  0.0519,  0.0022, -0.0361,  0.0248, -0.0434,\n",
       "                      -0.0467,  0.0144,  0.0246,  0.0062,  0.0226, -0.0403, -0.0052,  0.0109,\n",
       "                      -0.0521, -0.0471,  0.0464,  0.0489,  0.0486,  0.0577,  0.0161,  0.0519,\n",
       "                      -0.0315,  0.0379,  0.0430, -0.0299,  0.0316, -0.0192,  0.0064, -0.0025,\n",
       "                      -0.0457,  0.0262,  0.0522,  0.0261, -0.0345, -0.0419,  0.0037,  0.0493,\n",
       "                      -0.0119, -0.0340, -0.0459, -0.0217,  0.0100,  0.0101, -0.0074,  0.0293,\n",
       "                       0.0145, -0.0023,  0.0198, -0.0230,  0.0557, -0.0455, -0.0067, -0.0337,\n",
       "                      -0.0224, -0.0173, -0.0091, -0.0562, -0.0219,  0.0135,  0.0130, -0.0215,\n",
       "                       0.0574, -0.0356, -0.0200,  0.0457,  0.0274,  0.0462, -0.0386,  0.0173,\n",
       "                       0.0368, -0.0299,  0.0522, -0.0507, -0.0427,  0.0452, -0.0505, -0.0085,\n",
       "                       0.0486,  0.0060, -0.0024, -0.0193,  0.0252,  0.0003, -0.0407,  0.0387,\n",
       "                       0.0131, -0.0342,  0.0378, -0.0420, -0.0265, -0.0575, -0.0414, -0.0240,\n",
       "                       0.0567,  0.0065,  0.0053,  0.0481, -0.0087,  0.0245, -0.0105, -0.0125,\n",
       "                       0.0492,  0.0098, -0.0027, -0.0037,  0.0323, -0.0257, -0.0209,  0.0169,\n",
       "                       0.0563,  0.0449,  0.0476,  0.0398,  0.0274,  0.0312,  0.0399,  0.0525,\n",
       "                       0.0424, -0.0129,  0.0440,  0.0191, -0.0520,  0.0270, -0.0125, -0.0184])),\n",
       "             ('word_decoder.fc.3.weight',\n",
       "              tensor([[-0.0345,  0.0355,  0.0479,  0.0026, -0.0064, -0.0461,  0.0165,  0.0403,\n",
       "                       -0.0270,  0.0564, -0.0746,  0.0497, -0.0812, -0.0679, -0.0613,  0.0654,\n",
       "                        0.0118, -0.0118,  0.0105, -0.0790, -0.0453,  0.0464, -0.0693, -0.0394,\n",
       "                       -0.0619,  0.0776, -0.0260, -0.0799,  0.0547,  0.0465,  0.0746,  0.0744,\n",
       "                       -0.0153, -0.0056,  0.0133, -0.0354,  0.0410, -0.0648,  0.0137, -0.0732,\n",
       "                       -0.0583, -0.0757, -0.0762, -0.0804,  0.0791, -0.0378, -0.0693, -0.0234,\n",
       "                        0.0553, -0.0221, -0.0113, -0.0762, -0.0002, -0.0410,  0.0063,  0.0445,\n",
       "                       -0.0002, -0.0705, -0.0806, -0.0719, -0.0709,  0.0644,  0.0403, -0.0515,\n",
       "                        0.0794, -0.0419,  0.0050,  0.0828,  0.0544,  0.0608, -0.0097,  0.0366,\n",
       "                       -0.0412, -0.0506, -0.0228,  0.0349,  0.0065, -0.0301,  0.0496,  0.0098,\n",
       "                        0.0189, -0.0411,  0.0399, -0.0829, -0.0170, -0.0224,  0.0063,  0.0398,\n",
       "                        0.0305,  0.0239, -0.0590, -0.0590, -0.0467,  0.0534, -0.0597, -0.0644,\n",
       "                        0.0102, -0.0657, -0.0187,  0.0367, -0.0044,  0.0160, -0.0057,  0.0580,\n",
       "                        0.0281, -0.0307,  0.0563, -0.0108,  0.0571, -0.0756,  0.0538,  0.0080,\n",
       "                        0.0390, -0.0826, -0.0765, -0.0491, -0.0779, -0.0635, -0.0233, -0.0781,\n",
       "                        0.0451,  0.0469,  0.0243,  0.0346, -0.0425, -0.0740,  0.0128, -0.0623,\n",
       "                       -0.0344,  0.0378,  0.0145, -0.0790,  0.0154,  0.0675, -0.0541,  0.0695,\n",
       "                       -0.0355, -0.0122,  0.0771, -0.0005, -0.0283, -0.0309, -0.0610, -0.0346],\n",
       "                      [-0.0685, -0.0283, -0.0084,  0.0523, -0.0664, -0.0596, -0.0691,  0.0123,\n",
       "                        0.0186, -0.0813,  0.0661,  0.0644, -0.0436, -0.0140,  0.0192, -0.0761,\n",
       "                       -0.0673, -0.0222,  0.0451, -0.0150, -0.0429,  0.0162,  0.0640,  0.0079,\n",
       "                       -0.0368,  0.0055,  0.0674, -0.0637, -0.0420,  0.0685, -0.0675,  0.0349,\n",
       "                        0.0045,  0.0384,  0.0570, -0.0214, -0.0807, -0.0771, -0.0107, -0.0155,\n",
       "                       -0.0171, -0.0511,  0.0457, -0.0452,  0.0132,  0.0269, -0.0095, -0.0368,\n",
       "                        0.0164,  0.0392, -0.0200,  0.0480, -0.0486,  0.0631, -0.0671,  0.0034,\n",
       "                        0.0532, -0.0406, -0.0763,  0.0593,  0.0781,  0.0361,  0.0717,  0.0249,\n",
       "                        0.0646, -0.0007, -0.0132,  0.0738, -0.0120,  0.0527,  0.0151,  0.0355,\n",
       "                       -0.0655,  0.0543,  0.0549,  0.0730,  0.0496,  0.0831, -0.0366,  0.0149,\n",
       "                       -0.0026,  0.0283,  0.0594,  0.0438, -0.0074, -0.0137, -0.0195, -0.0317,\n",
       "                       -0.0079,  0.0778,  0.0342,  0.0361,  0.0479, -0.0642, -0.0761,  0.0404,\n",
       "                       -0.0275,  0.0221, -0.0734, -0.0122, -0.0464, -0.0223,  0.0028,  0.0816,\n",
       "                        0.0472,  0.0328, -0.0362,  0.0008,  0.0718, -0.0565, -0.0112,  0.0710,\n",
       "                       -0.0138, -0.0354,  0.0059,  0.0792, -0.0154,  0.0103,  0.0366,  0.0823,\n",
       "                       -0.0440,  0.0674,  0.0019, -0.0506,  0.0168, -0.0785, -0.0372,  0.0210,\n",
       "                        0.0778, -0.0311,  0.0208,  0.0594,  0.0678,  0.0172,  0.0785,  0.0052,\n",
       "                        0.0472,  0.0740, -0.0251, -0.0005,  0.0684,  0.0767, -0.0556,  0.0237]])),\n",
       "             ('word_decoder.fc.3.bias', tensor([-0.0055,  0.0300]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = model.state_dict()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 0/760 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mvalidate(model, data_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:774\u001b[0m, in \u001b[0;36mTrainer.validate\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mPerform one evaluation epoch over the validation set.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39m    The length of the list corresponds to the number of validation dataloaders used.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> 774\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:821\u001b[0m, in \u001b[0;36mTrainer._validate_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validated_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[39m# run validate\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    823\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    824\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1249\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mdispatch(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_evaluate()\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1295\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_evaluation\u001b[39m\u001b[39m\"\u001b[39m), _evaluation_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator):\n\u001b[0;32m-> 1295\u001b[0m     eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1297\u001b[0m \u001b[39m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    142\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 240\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1704\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1706\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:370\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    369\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/wicii/DDH/class/graduation_project/mpvn/mpvn/model/model.py:136\u001b[0m, in \u001b[0;36mConformerRNNModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m per \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mper_metric(targets[:, \u001b[39m1\u001b[39m:], y_hats)\n\u001b[1;32m    135\u001b[0m \u001b[39m# self._log_states('valid', per, loss, cross_entropy_loss, ctc_loss)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmed_criterion(score, MED_outputs)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m1 sample result\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/grad/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/wicii/DDH/class/graduation_project/mpvn/mpvn/criterion/criterion.py:40\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, decoder_log_probs, targets)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     36\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m         decoder_log_probs: Tensor,\n\u001b[1;32m     38\u001b[0m         targets: Tensor,\n\u001b[1;32m     39\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor, Tensor]:\n\u001b[0;32m---> 40\u001b[0m     cross_entropy_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_entropy_loss(decoder_log_probs, targets\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_entropy_loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'contiguous'"
     ]
    }
   ],
   "source": [
    "trainer.validate(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log.log', 'w') as f:\n",
    "    for i, attn in enumerate(model.save_attn):\n",
    "        print(i, attn.softmax(dim=-1), file=f)\n",
    "        print(i, torch.sum(model.save_context.squeeze(), dim=-1)[i], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "def accuracy(y: Tensor, y_hat: Tensor, length: Tensor) -> float:\n",
    "    accs = list()\n",
    "    for y_, yhat_, l in zip(y, y_hat, length.to(torch.long)):\n",
    "        accs.append(torch.mean((y_ == yhat_)[:l].to(torch.float)))\n",
    "    return float(torch.sum(torch.stack(accs)))/length.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "[tensor(0.), tensor(1.), tensor(1.)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(\n",
    "    torch.Tensor([[2,2,1,2],[1,2,1,2],[1,2,1,2]]),\n",
    "    torch.Tensor([[1,2,1,2],[1,2,3,2],[1,2,1,2]]),\n",
    "    torch.Tensor([1,2,3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('grad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b1a50cc58c14d82ff378b4491fa094aec266868d23c7ca7976fe8f9a4aaf20a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
